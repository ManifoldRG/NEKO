{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db14cc1d-8a0c-4d57-b315-9d71b22dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 22:44:38 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PG5...  On   | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    79W / 330W |   5257MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PG5...  On   | 00000000:13:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    59W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PG5...  On   | 00000000:14:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    49W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PG5...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    57W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PG5...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    58W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PG5...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    63W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PG5...  On   | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PG5...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3604288      C   ...onda/envs/neko/bin/python     5254MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b849366c-9dea-4f3d-89fb-45d7f10bf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e46c4c-ff45-4760-8ae4-c55fa01f39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b8ba88-f71a-4ed1-9c64-2c37d30aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "    def predict_text(self, input_tokens, max_length=20, deterministic=True):\n",
    "        predicted_tokens = input_tokens.clone()\n",
    "        logits_list = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(predicted_tokens[:, -1:])\n",
    "            token_masks = torch.ones((predicted_tokens.shape[0], 1), device=self.device)  # Ensure batch dimension\n",
    "    \n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time-step logits\n",
    "            logits_list.append(logits.unsqueeze(1))\n",
    "    \n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Ensure it keeps batch dimension\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            predicted_tokens = torch.cat([predicted_tokens, next_token], dim=1)\n",
    "    \n",
    "        all_logits = torch.cat(logits_list, dim=1)\n",
    "        return predicted_tokens[:, input_tokens.size(1):], all_logits\n",
    "\n",
    "    \n",
    "    def predict_text_single_single(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            if token.numel() == 1:  # Checking if token is a single element\n",
    "                predicted_tokens.append(token.item())\n",
    "            else:\n",
    "                print(f\"Expected a single element, got {token.numel()} elements.\")\n",
    "    \n",
    "            input_tokens = torch.cat([input_tokens, token], dim=1)  # Append the predicted token\n",
    "\n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51983f70-9cc5-42ad-9b1c-d797106857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        \n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "        \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "\n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "        \n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "\n",
    "        # Forward pass    \n",
    "        logits, loss = model(batch_dicts, compute_loss=True)\n",
    "        \n",
    "        # total_tokens = input_tokens.size(0) * input_tokens.size(1)\n",
    "        # print(f'total tokens:{total_tokens}')\n",
    "        avg_loss = loss.item() \n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "\n",
    "    def evaluate_single_single(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            input_tokens = batch_dict['text'].to(device=model.device)\n",
    "            target_tokens = batch_dict['target'].to(device=model.device)\n",
    "    \n",
    "            total_loss_per_sequence = 0.0\n",
    "            pred_tokens = []\n",
    "    \n",
    "            for idx in range(input_tokens.size(0)):\n",
    "                pred_logits, single_pred_tokens = model.predict_text({'text': input_tokens[idx].unsqueeze(0)}, max_length=1, deterministic=deterministic)\n",
    "                loss = torch.nn.functional.cross_entropy(pred_logits, target_tokens[idx].unsqueeze(0))\n",
    "                total_loss_per_sequence += loss.item()\n",
    "                pred_tokens.extend(single_pred_tokens)\n",
    "            \n",
    "            total_loss += total_loss_per_sequence / input_tokens.size(0)\n",
    "            total_tokens += input_tokens.size(0)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                decoded_input = self.text_tokenizer.decode(input_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_target = self.text_tokenizer.decode(target_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_prediction = self.text_tokenizer.decode(torch.tensor(pred_tokens), skip_special_tokens=True)            \n",
    "                print(f'=>Input: {decoded_input} \\n =>Target: {decoded_target} \\n =>Prediction: {decoded_prediction}')\n",
    "    \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c0f0c-08cf-4220-81e4-cd4cf39c8c68",
   "metadata": {},
   "source": [
    "## trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1204c27b-b31f-422d-ae68-4b13beab83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=10000,\n",
    "    log_eval_freq=10,\n",
    "    warmup_steps=500,\n",
    "    batch_size=4,\n",
    "    sequence_length=1024,\n",
    "    eval_episodes=5,\n",
    "    text_prop=1,\n",
    "    # eval_text_log_examples=True,\n",
    "    pretrained_lm=None,\n",
    "    text_datasets=['wikitext-2-v1'],\n",
    "    text_datasets_paths=['wikitext'],\n",
    "    use_wandb=True,\n",
    "    device='cuda',\n",
    "    eval_mode='stochastic',\n",
    "    eval_text_num_examples=100,\n",
    "    # disable_cosine_decay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f08b1a4a-f44f-45b1-b282-4076e2ee5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhavul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhavul/bhavul/NEKO/dev_notebooks/wandb/run-20240510_224452-cdic30ni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhavul/gato-control/runs/cdic30ni' target=\"_blank\">neko-gato_24-05-10_22-44-47</a></strong> to <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhavul/gato-control/runs/cdic30ni' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/cdic30ni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    # mixed_precision=args.mixed_precision,\n",
    "    # gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'\n",
    "\n",
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )\n",
    "model = accelerator.prepare(model)\n",
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')\n",
    "\n",
    "tasks = [TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)]\n",
    "args = args\n",
    "print_logs = True # args.print_logs\n",
    "device = torch.device(args.device)\n",
    "\n",
    "min_lr = args.learning_rate / args.min_factor\n",
    "deterministic = args.eval_mode == 'deterministic'\n",
    "\n",
    "exp_name = exp_name\n",
    "exp_dir = os.path.join(args.save_dir, exp_name)\n",
    "\n",
    "steps = 0\n",
    "start_time = None\n",
    "\n",
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c8d8bd-a174-479b-9506-aab36fb4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6fca9a2-f025-4158-a493-a2d128090ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9e474d-70b3-4f05-967e-e6d4ab349358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(num_steps, iter):\n",
    "    logs = {}\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for i in range(num_steps):\n",
    "        steps += 1\n",
    "        result = train_step()\n",
    "        if result is None:\n",
    "            # steps -= 1\n",
    "            # print(\"Skipped a training step due to empty batch.\")\n",
    "            continue\n",
    "        train_loss, step_logs = result\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # add logs from last train_step as well\n",
    "    for log in step_logs:\n",
    "        logs[log] = step_logs[log]\n",
    "\n",
    "    logs['time/training'] = time.time() - train_start\n",
    "\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # loop over eval for each env\n",
    "    with torch.no_grad():\n",
    "        for task in tasks:\n",
    "            eval_logs = {}\n",
    "            if isinstance(task, TextTask):\n",
    "                eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, log_examples_to_output=args.eval_text_log_examples)\n",
    "                for k, v in eval_logs.items():\n",
    "                    logs[f'evaluation/text/{k}'] = v\n",
    "                pass\n",
    "\n",
    "    logs['time/total'] = time.time() - start_time\n",
    "    logs['time/evaluation'] = time.time() - eval_start\n",
    "    logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "    logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "            print('=' * 80)\n",
    "\n",
    "    ## Save model\n",
    "    if args.save_model and args.save_mode == 'checkpoint':\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "                \n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e91ede-9d47-4a85-9de7-64cf265bb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5ea46-ad2d-43b0-ae46-e8a5bac73d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:1000\n",
      "================================================================================\n",
      "Iteration 0\n",
      "training/learning_rate: 1.8981999999999999e-06\n",
      "time/sample_batch: 0.007546424865722656\n",
      "time/training: 1.8498353958129883\n",
      "evaluation/text/loss: 10.928056716918945\n",
      "evaluation/text/perplexity: 55717.8984375\n",
      "time/total: 2.4340872764587402\n",
      "time/evaluation: 0.5836873054504395\n",
      "training/train_loss_mean: 11.020158576965333\n",
      "training/train_loss_std: 0.07462706341330688\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 1\n",
      "training/learning_rate: 3.8962e-06\n",
      "time/sample_batch: 0.007089376449584961\n",
      "time/training: 0.603590726852417\n",
      "evaluation/text/loss: 10.697006225585938\n",
      "evaluation/text/perplexity: 44223.26171875\n",
      "time/total: 3.5649142265319824\n",
      "time/evaluation: 0.5253551006317139\n",
      "training/train_loss_mean: 10.879894638061524\n",
      "training/train_loss_std: 0.05272700320488498\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 2\n",
      "training/learning_rate: 5.8942e-06\n",
      "time/sample_batch: 0.0060231685638427734\n",
      "time/training: 0.576627254486084\n",
      "evaluation/text/loss: 10.315818786621094\n",
      "evaluation/text/perplexity: 30206.693359375\n",
      "time/total: 5.870165824890137\n",
      "time/evaluation: 1.72690749168396\n",
      "training/train_loss_mean: 10.716300201416015\n",
      "training/train_loss_std: 0.09019499281696199\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 3\n",
      "training/learning_rate: 7.8922e-06\n",
      "time/sample_batch: 0.006623268127441406\n",
      "time/training: 1.0756030082702637\n",
      "evaluation/text/loss: 9.8784818649292\n",
      "evaluation/text/perplexity: 19506.087890625\n",
      "time/total: 7.895902872085571\n",
      "time/evaluation: 0.948474645614624\n",
      "training/train_loss_mean: 10.461702346801758\n",
      "training/train_loss_std: 0.19480408467604232\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 4\n",
      "training/learning_rate: 9.8902e-06\n",
      "time/sample_batch: 0.006460428237915039\n",
      "time/training: 0.6539473533630371\n",
      "evaluation/text/loss: 9.308406829833984\n",
      "evaluation/text/perplexity: 11030.361328125\n",
      "time/total: 9.786670207977295\n",
      "time/evaluation: 1.2338268756866455\n",
      "training/train_loss_mean: 10.083338451385497\n",
      "training/train_loss_std: 0.20299627412037854\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 5\n",
      "training/learning_rate: 1.18882e-05\n",
      "time/sample_batch: 0.005004405975341797\n",
      "time/training: 0.5323586463928223\n",
      "evaluation/text/loss: 8.9201021194458\n",
      "evaluation/text/perplexity: 7480.85302734375\n",
      "time/total: 10.788713216781616\n",
      "time/evaluation: 0.4679691791534424\n",
      "training/train_loss_mean: 9.581190252304078\n",
      "training/train_loss_std: 0.6931298611971485\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 6\n",
      "training/learning_rate: 1.3686400000000002e-05\n",
      "time/sample_batch: 0.005769014358520508\n",
      "time/training: 0.4423067569732666\n",
      "evaluation/text/loss: 8.600622177124023\n",
      "evaluation/text/perplexity: 5435.0400390625\n",
      "time/total: 11.971076488494873\n",
      "time/evaluation: 0.738394021987915\n",
      "training/train_loss_mean: 9.16297541724311\n",
      "training/train_loss_std: 0.8459041136730651\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 7\n",
      "training/learning_rate: 1.56844e-05\n",
      "time/sample_batch: 0.006342887878417969\n",
      "time/training: 0.5267367362976074\n",
      "evaluation/text/loss: 8.449943542480469\n",
      "evaluation/text/perplexity: 4674.80859375\n",
      "time/total: 13.009052038192749\n",
      "time/evaluation: 0.5092403888702393\n",
      "training/train_loss_mean: 8.819676637649536\n",
      "training/train_loss_std: 0.9442440390817207\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 8\n",
      "training/learning_rate: 1.7682400000000002e-05\n",
      "time/sample_batch: 0.006337165832519531\n",
      "time/training: 0.5581483840942383\n",
      "evaluation/text/loss: 8.048086166381836\n",
      "evaluation/text/perplexity: 3127.80322265625\n",
      "time/total: 14.158040046691895\n",
      "time/evaluation: 0.5892062187194824\n",
      "training/train_loss_mean: 9.00904140472412\n",
      "training/train_loss_std: 0.2335936403312902\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 9\n",
      "training/learning_rate: 1.96804e-05\n",
      "time/sample_batch: 0.006327629089355469\n",
      "time/training: 0.5389266014099121\n",
      "evaluation/text/loss: 7.933538913726807\n",
      "evaluation/text/perplexity: 2789.2802734375\n",
      "time/total: 15.419153928756714\n",
      "time/evaluation: 0.7204878330230713\n",
      "training/train_loss_mean: 8.690292263031006\n",
      "training/train_loss_std: 0.4697208170112935\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 10\n",
      "training/learning_rate: 2.16784e-05\n",
      "time/sample_batch: 0.006429433822631836\n",
      "time/training: 0.5277795791625977\n",
      "evaluation/text/loss: 7.744990348815918\n",
      "evaluation/text/perplexity: 2309.97119140625\n",
      "time/total: 16.67324471473694\n",
      "time/evaluation: 0.7247109413146973\n",
      "training/train_loss_mean: 8.136722517013549\n",
      "training/train_loss_std: 0.8740051970063235\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 11\n",
      "training/learning_rate: 2.36764e-05\n",
      "time/sample_batch: 0.007515907287597656\n",
      "time/training: 0.46323466300964355\n",
      "evaluation/text/loss: 7.572388172149658\n",
      "evaluation/text/perplexity: 1943.77685546875\n",
      "time/total: 17.86181139945984\n",
      "time/evaluation: 0.7237033843994141\n",
      "training/train_loss_mean: 7.538754725456238\n",
      "training/train_loss_std: 1.5159249765184268\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 12\n",
      "training/learning_rate: 2.56744e-05\n",
      "time/sample_batch: 0.005912065505981445\n",
      "time/training: 0.5612757205963135\n",
      "evaluation/text/loss: 7.155115604400635\n",
      "evaluation/text/perplexity: 1280.6405029296875\n",
      "time/total: 18.96273374557495\n",
      "time/evaluation: 0.5380206108093262\n",
      "training/train_loss_mean: 8.349980449676513\n",
      "training/train_loss_std: 0.38033514424445664\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 13\n",
      "training/learning_rate: 2.76724e-05\n",
      "time/sample_batch: 0.005850791931152344\n",
      "time/training: 0.6001336574554443\n",
      "evaluation/text/loss: 7.259640216827393\n",
      "evaluation/text/perplexity: 1421.744873046875\n",
      "time/total: 20.28585910797119\n",
      "time/evaluation: 0.7213797569274902\n",
      "training/train_loss_mean: 7.941124105453492\n",
      "training/train_loss_std: 0.49493097577049117\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 14\n",
      "training/learning_rate: 2.96704e-05\n",
      "time/sample_batch: 0.005726337432861328\n",
      "time/training: 0.5091831684112549\n",
      "evaluation/text/loss: 7.201187610626221\n",
      "evaluation/text/perplexity: 1341.0224609375\n",
      "time/total: 21.44369602203369\n",
      "time/evaluation: 0.6470053195953369\n",
      "training/train_loss_mean: 7.438097095489502\n",
      "training/train_loss_std: 1.0444627969032882\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 15\n",
      "training/learning_rate: 3.16684e-05\n",
      "time/sample_batch: 0.004941701889038086\n",
      "time/training: 0.5046756267547607\n",
      "evaluation/text/loss: 6.6903510093688965\n",
      "evaluation/text/perplexity: 804.6046142578125\n",
      "time/total: 22.32516837120056\n",
      "time/evaluation: 0.3751978874206543\n",
      "training/train_loss_mean: 7.279199314117432\n",
      "training/train_loss_std: 1.1626682954473313\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 16\n",
      "training/learning_rate: 3.366640000000001e-05\n",
      "time/sample_batch: 0.0053539276123046875\n",
      "time/training: 0.4635615348815918\n",
      "evaluation/text/loss: 6.716207027435303\n",
      "evaluation/text/perplexity: 825.6798095703125\n",
      "time/total: 23.455406427383423\n",
      "time/evaluation: 0.6650652885437012\n",
      "training/train_loss_mean: 7.037696051597595\n",
      "training/train_loss_std: 1.6294539357783473\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 17\n",
      "training/learning_rate: 3.5664400000000004e-05\n",
      "time/sample_batch: 0.0058574676513671875\n",
      "time/training: 0.5502228736877441\n",
      "evaluation/text/loss: 6.379382610321045\n",
      "evaluation/text/perplexity: 589.5635986328125\n",
      "time/total: 24.74067521095276\n",
      "time/evaluation: 0.7332804203033447\n",
      "training/train_loss_mean: 7.109934997558594\n",
      "training/train_loss_std: 1.3451885792919107\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 18\n",
      "training/learning_rate: 3.766240000000001e-05\n",
      "time/sample_batch: 0.006014347076416016\n",
      "time/training: 0.49710679054260254\n",
      "evaluation/text/loss: 6.607302665710449\n",
      "evaluation/text/perplexity: 740.4829711914062\n",
      "time/total: 25.833327293395996\n",
      "time/evaluation: 0.5938906669616699\n",
      "training/train_loss_mean: 6.726927185058594\n",
      "training/train_loss_std: 1.4864049337711538\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 19\n",
      "training/learning_rate: 3.9660400000000004e-05\n",
      "time/sample_batch: 0.0069735050201416016\n",
      "time/training: 0.6141011714935303\n",
      "evaluation/text/loss: 6.2079758644104\n",
      "evaluation/text/perplexity: 496.6948547363281\n",
      "time/total: 26.94712209701538\n",
      "time/evaluation: 0.4979679584503174\n",
      "training/train_loss_mean: 7.059261584281922\n",
      "training/train_loss_std: 1.1881277550294091\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 20\n",
      "training/learning_rate: 4.165840000000001e-05\n",
      "time/sample_batch: 0.008422374725341797\n",
      "time/training: 0.6355462074279785\n",
      "evaluation/text/loss: 6.3624420166015625\n",
      "evaluation/text/perplexity: 579.66015625\n",
      "time/total: 28.123419761657715\n",
      "time/evaluation: 0.5389773845672607\n",
      "training/train_loss_mean: 6.734868383407592\n",
      "training/train_loss_std: 0.7333484901382902\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 21\n",
      "training/learning_rate: 4.3656400000000004e-05\n",
      "time/sample_batch: 0.0054624080657958984\n",
      "time/training: 0.519087553024292\n",
      "evaluation/text/loss: 6.237459659576416\n",
      "evaluation/text/perplexity: 511.5573425292969\n",
      "time/total: 29.337076902389526\n",
      "time/evaluation: 0.6927661895751953\n",
      "training/train_loss_mean: 6.902714538574219\n",
      "training/train_loss_std: 0.3912979891204263\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 22\n",
      "training/learning_rate: 4.56544e-05\n",
      "time/sample_batch: 0.007383108139038086\n",
      "time/training: 0.6391408443450928\n",
      "evaluation/text/loss: 6.1105055809021\n",
      "evaluation/text/perplexity: 450.56646728515625\n",
      "time/total: 30.618396282196045\n",
      "time/evaluation: 0.6404037475585938\n",
      "training/train_loss_mean: 6.854901266098023\n",
      "training/train_loss_std: 0.41209539241859733\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 23\n",
      "training/learning_rate: 4.7652400000000005e-05\n",
      "time/sample_batch: 0.005918025970458984\n",
      "time/training: 0.5699219703674316\n",
      "evaluation/text/loss: 6.0763115882873535\n",
      "evaluation/text/perplexity: 435.42022705078125\n",
      "time/total: 31.870893478393555\n",
      "time/evaluation: 0.6808288097381592\n",
      "training/train_loss_mean: 7.013774728775024\n",
      "training/train_loss_std: 0.41222868225101583\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 24\n",
      "training/learning_rate: 4.96504e-05\n",
      "time/sample_batch: 0.007872819900512695\n",
      "time/training: 0.6148772239685059\n",
      "evaluation/text/loss: 5.992265701293945\n",
      "evaluation/text/perplexity: 400.3205871582031\n",
      "time/total: 33.11929655075073\n",
      "time/evaluation: 0.6317822933197021\n",
      "training/train_loss_mean: 6.914130401611328\n",
      "training/train_loss_std: 0.2983683035518419\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 25\n",
      "training/learning_rate: 5.1648400000000005e-05\n",
      "time/sample_batch: 0.006550312042236328\n",
      "time/training: 0.6181638240814209\n",
      "evaluation/text/loss: 5.9611124992370605\n",
      "evaluation/text/perplexity: 388.0415954589844\n",
      "time/total: 34.54953598976135\n",
      "time/evaluation: 0.8102996349334717\n",
      "training/train_loss_mean: 6.654402875900269\n",
      "training/train_loss_std: 0.6647440678562965\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 26\n",
      "training/learning_rate: 5.3646399999999995e-05\n",
      "time/sample_batch: 0.005724191665649414\n",
      "time/training: 0.600395679473877\n",
      "evaluation/text/loss: 5.944425106048584\n",
      "evaluation/text/perplexity: 381.6199035644531\n",
      "time/total: 35.67578077316284\n",
      "time/evaluation: 0.5240826606750488\n",
      "training/train_loss_mean: 6.760506343841553\n",
      "training/train_loss_std: 0.27525488515958096\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 27\n",
      "training/learning_rate: 5.5644400000000005e-05\n",
      "time/sample_batch: 0.008193492889404297\n",
      "time/training: 0.6195728778839111\n",
      "evaluation/text/loss: 5.875035762786865\n",
      "evaluation/text/perplexity: 356.0373840332031\n",
      "time/total: 36.892760276794434\n",
      "time/evaluation: 0.5956263542175293\n",
      "training/train_loss_mean: 6.383753323554993\n",
      "training/train_loss_std: 0.9353500529981092\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 28\n",
      "training/learning_rate: 5.7442600000000005e-05\n",
      "time/sample_batch: 0.007206916809082031\n",
      "time/training: 0.5983054637908936\n",
      "evaluation/text/loss: 5.997592449188232\n",
      "evaluation/text/perplexity: 402.45867919921875\n",
      "time/total: 38.189868688583374\n",
      "time/evaluation: 0.6970863342285156\n",
      "training/train_loss_mean: 6.341106997595893\n",
      "training/train_loss_std: 0.6636100995414721\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 29\n",
      "training/learning_rate: 5.94406e-05\n",
      "time/sample_batch: 0.00714564323425293\n",
      "time/training: 0.6212291717529297\n",
      "evaluation/text/loss: 5.881377696990967\n",
      "evaluation/text/perplexity: 358.3025207519531\n",
      "time/total: 39.392922163009644\n",
      "time/evaluation: 0.579843282699585\n",
      "training/train_loss_mean: 6.307755470275879\n",
      "training/train_loss_std: 0.8180514218319553\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 30\n",
      "training/learning_rate: 6.14386e-05\n",
      "time/sample_batch: 0.006757974624633789\n",
      "time/training: 0.5485515594482422\n",
      "evaluation/text/loss: 5.96138334274292\n",
      "evaluation/text/perplexity: 388.1466979980469\n",
      "time/total: 40.682238817214966\n",
      "time/evaluation: 0.7389805316925049\n",
      "training/train_loss_mean: 6.01336030960083\n",
      "training/train_loss_std: 1.041955501196603\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 31\n",
      "training/learning_rate: 6.343660000000001e-05\n",
      "time/sample_batch: 0.006829261779785156\n",
      "time/training: 0.6429939270019531\n",
      "evaluation/text/loss: 5.758474826812744\n",
      "evaluation/text/perplexity: 316.86468505859375\n",
      "time/total: 41.92712926864624\n",
      "time/evaluation: 0.6002509593963623\n",
      "training/train_loss_mean: 6.839033365249634\n",
      "training/train_loss_std: 0.3551935785363355\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 32\n",
      "training/learning_rate: 6.543460000000001e-05\n",
      "time/sample_batch: 0.008260250091552734\n",
      "time/training: 0.6279251575469971\n",
      "evaluation/text/loss: 5.864125728607178\n",
      "evaluation/text/perplexity: 352.17413330078125\n",
      "time/total: 43.54463338851929\n",
      "time/evaluation: 0.9878878593444824\n",
      "training/train_loss_mean: 6.482696151733398\n",
      "training/train_loss_std: 0.16150712644622892\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 33\n",
      "training/learning_rate: 6.74326e-05\n",
      "time/sample_batch: 0.010254621505737305\n",
      "time/training: 0.6033062934875488\n",
      "evaluation/text/loss: 5.8765692710876465\n",
      "evaluation/text/perplexity: 356.58380126953125\n",
      "time/total: 44.91207551956177\n",
      "time/evaluation: 0.7624924182891846\n",
      "training/train_loss_mean: 5.94946141242981\n",
      "training/train_loss_std: 1.556084043999081\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 34\n",
      "training/learning_rate: 6.94306e-05\n",
      "time/sample_batch: 0.0063860416412353516\n",
      "time/training: 0.5836026668548584\n",
      "evaluation/text/loss: 5.875109672546387\n",
      "evaluation/text/perplexity: 356.063720703125\n",
      "time/total: 46.08280873298645\n",
      "time/evaluation: 0.5853590965270996\n",
      "training/train_loss_mean: 6.834673738479614\n",
      "training/train_loss_std: 0.48379669392910485\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 35\n",
      "training/learning_rate: 7.142860000000001e-05\n",
      "time/sample_batch: 0.00577998161315918\n",
      "time/training: 0.6497480869293213\n",
      "evaluation/text/loss: 5.88450288772583\n",
      "evaluation/text/perplexity: 359.4240417480469\n",
      "time/total: 47.32351016998291\n",
      "time/evaluation: 0.589146614074707\n",
      "training/train_loss_mean: 6.661304330825805\n",
      "training/train_loss_std: 0.38600208990594753\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 36\n",
      "training/learning_rate: 7.342660000000001e-05\n",
      "time/sample_batch: 0.005711555480957031\n",
      "time/training: 0.5321743488311768\n",
      "evaluation/text/loss: 5.778487205505371\n",
      "evaluation/text/perplexity: 323.269775390625\n",
      "time/total: 48.39918899536133\n",
      "time/evaluation: 0.5418756008148193\n",
      "training/train_loss_mean: 6.293806266784668\n",
      "training/train_loss_std: 0.7520066677139767\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 37\n",
      "training/learning_rate: 7.522480000000001e-05\n",
      "time/sample_batch: 0.00775909423828125\n",
      "time/training: 0.4903597831726074\n",
      "evaluation/text/loss: 5.809040546417236\n",
      "evaluation/text/perplexity: 333.2991943359375\n",
      "time/total: 49.671483755111694\n",
      "time/evaluation: 0.7766506671905518\n",
      "training/train_loss_mean: 6.4970352914598255\n",
      "training/train_loss_std: 0.28821251824822497\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 38\n",
      "training/learning_rate: 7.72228e-05\n",
      "time/sample_batch: 0.007636070251464844\n",
      "time/training: 0.6319878101348877\n",
      "evaluation/text/loss: 6.093559741973877\n",
      "evaluation/text/perplexity: 442.99554443359375\n",
      "time/total: 50.855140924453735\n",
      "time/evaluation: 0.5498363971710205\n",
      "training/train_loss_mean: 6.465209293365478\n",
      "training/train_loss_std: 0.36542207253781017\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 39\n",
      "training/learning_rate: 7.9021e-05\n",
      "time/sample_batch: 0.00651240348815918\n",
      "time/training: 0.617964506149292\n",
      "evaluation/text/loss: 5.827810764312744\n",
      "evaluation/text/perplexity: 339.6143798828125\n",
      "time/total: 52.11634302139282\n",
      "time/evaluation: 0.6415705680847168\n",
      "training/train_loss_mean: 6.572637028164333\n",
      "training/train_loss_std: 0.25012544637798034\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 40\n",
      "training/learning_rate: 8.1019e-05\n",
      "time/sample_batch: 0.006277799606323242\n",
      "time/training: 0.5672607421875\n",
      "evaluation/text/loss: 5.927825450897217\n",
      "evaluation/text/perplexity: 375.3374328613281\n",
      "time/total: 53.117292165756226\n",
      "time/evaluation: 0.4320206642150879\n",
      "training/train_loss_mean: 5.591447591781616\n",
      "training/train_loss_std: 2.0163729864816293\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 41\n",
      "training/learning_rate: 8.28172e-05\n",
      "time/sample_batch: 0.0070362091064453125\n",
      "time/training: 0.5120389461517334\n",
      "evaluation/text/loss: 5.79037618637085\n",
      "evaluation/text/perplexity: 327.1360778808594\n",
      "time/total: 54.36922883987427\n",
      "time/evaluation: 0.7380814552307129\n",
      "training/train_loss_mean: 6.618433899349636\n",
      "training/train_loss_std: 0.34134536422111017\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 42\n",
      "training/learning_rate: 8.481520000000001e-05\n",
      "time/sample_batch: 0.005635499954223633\n",
      "time/training: 0.5761501789093018\n",
      "evaluation/text/loss: 5.779641151428223\n",
      "evaluation/text/perplexity: 323.6430358886719\n",
      "time/total: 55.67567753791809\n",
      "time/evaluation: 0.7285828590393066\n",
      "training/train_loss_mean: 6.385863041877746\n",
      "training/train_loss_std: 1.041326674071348\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 43\n",
      "training/learning_rate: 8.681320000000001e-05\n",
      "time/sample_batch: 0.006748199462890625\n",
      "time/training: 0.5328891277313232\n",
      "evaluation/text/loss: 5.735889434814453\n",
      "evaluation/text/perplexity: 309.78839111328125\n",
      "time/total: 56.71843504905701\n",
      "time/evaluation: 0.5083277225494385\n",
      "training/train_loss_mean: 6.420472860336304\n",
      "training/train_loss_std: 0.34951576653928257\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 44\n",
      "training/learning_rate: 8.88112e-05\n",
      "time/sample_batch: 0.006892204284667969\n",
      "time/training: 0.5696790218353271\n",
      "evaluation/text/loss: 5.588691234588623\n",
      "evaluation/text/perplexity: 267.38543701171875\n",
      "time/total: 57.8517689704895\n",
      "time/evaluation: 0.5620284080505371\n",
      "training/train_loss_mean: 6.569335699081421\n",
      "training/train_loss_std: 0.44017038692991256\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 45\n",
      "training/learning_rate: 9.08092e-05\n",
      "time/sample_batch: 0.00628972053527832\n",
      "time/training: 0.5812392234802246\n",
      "evaluation/text/loss: 5.751902103424072\n",
      "evaluation/text/perplexity: 314.7888488769531\n",
      "time/total: 59.028483867645264\n",
      "time/evaluation: 0.5938394069671631\n",
      "training/train_loss_mean: 6.294914388656617\n",
      "training/train_loss_std: 0.9150794605641001\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 46\n",
      "training/learning_rate: 9.280720000000001e-05\n",
      "time/sample_batch: 0.008258342742919922\n",
      "time/training: 0.5719473361968994\n",
      "evaluation/text/loss: 5.635589599609375\n",
      "evaluation/text/perplexity: 280.2240905761719\n",
      "time/total: 60.10497832298279\n",
      "time/evaluation: 0.5025622844696045\n",
      "training/train_loss_mean: 6.376759386062622\n",
      "training/train_loss_std: 0.41444668069817253\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 47\n",
      "training/learning_rate: 9.460540000000001e-05\n",
      "time/sample_batch: 0.007427215576171875\n",
      "time/training: 0.5643496513366699\n",
      "evaluation/text/loss: 5.768624782562256\n",
      "evaluation/text/perplexity: 320.09722900390625\n",
      "time/total: 61.43251633644104\n",
      "time/evaluation: 0.7614037990570068\n",
      "training/train_loss_mean: 6.546073224809435\n",
      "training/train_loss_std: 0.33478200111490364\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 48\n",
      "training/learning_rate: 9.660340000000001e-05\n",
      "time/sample_batch: 0.0067577362060546875\n",
      "time/training: 0.5540411472320557\n",
      "evaluation/text/loss: 5.62589693069458\n",
      "evaluation/text/perplexity: 277.5210876464844\n",
      "time/total: 62.490172147750854\n",
      "time/evaluation: 0.5019392967224121\n",
      "training/train_loss_mean: 6.271549701690674\n",
      "training/train_loss_std: 0.20013871678521872\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 49\n",
      "training/learning_rate: 9.86014e-05\n",
      "time/sample_batch: 0.006139039993286133\n",
      "time/training: 0.5602202415466309\n",
      "evaluation/text/loss: 5.657937049865723\n",
      "evaluation/text/perplexity: 286.556884765625\n",
      "time/total: 63.78761386871338\n",
      "time/evaluation: 0.7354769706726074\n",
      "training/train_loss_mean: 6.168592929840088\n",
      "training/train_loss_std: 0.5015216822984847\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 50\n",
      "training/learning_rate: 9.999997785490607e-05\n",
      "time/sample_batch: 0.0057871341705322266\n",
      "time/training: 0.4811360836029053\n",
      "evaluation/text/loss: 5.672304630279541\n",
      "evaluation/text/perplexity: 290.7037353515625\n",
      "time/total: 64.81299614906311\n",
      "time/evaluation: 0.5424373149871826\n",
      "training/train_loss_mean: 5.974444103240967\n",
      "training/train_loss_std: 0.8480540567810718\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 51\n",
      "training/learning_rate: 9.999970227181881e-05\n",
      "time/sample_batch: 0.006505489349365234\n",
      "time/training: 0.4811427593231201\n",
      "evaluation/text/loss: 5.730836391448975\n",
      "evaluation/text/perplexity: 308.2269592285156\n",
      "time/total: 66.03379964828491\n",
      "time/evaluation: 0.7378127574920654\n",
      "training/train_loss_mean: 5.233306884765625\n",
      "training/train_loss_std: 1.702874095474642\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 52\n",
      "training/learning_rate: 9.999891489466934e-05\n",
      "time/sample_batch: 0.008465290069580078\n",
      "time/training: 0.6223580837249756\n",
      "evaluation/text/loss: 5.809969425201416\n",
      "evaluation/text/perplexity: 333.6089172363281\n",
      "time/total: 67.31352806091309\n",
      "time/evaluation: 0.6555967330932617\n",
      "training/train_loss_mean: 6.249407148361206\n",
      "training/train_loss_std: 0.3140247666013863\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 53\n",
      "training/learning_rate: 9.999763541659605e-05\n",
      "time/sample_batch: 0.006635427474975586\n",
      "time/training: 0.5693848133087158\n",
      "evaluation/text/loss: 5.630315780639648\n",
      "evaluation/text/perplexity: 278.7501220703125\n",
      "time/total: 68.6189353466034\n",
      "time/evaluation: 0.7342338562011719\n",
      "training/train_loss_mean: 6.22013373374939\n",
      "training/train_loss_std: 0.6727256808772343\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 54\n",
      "training/learning_rate: 9.99958638515911e-05\n",
      "time/sample_batch: 0.008730888366699219\n",
      "time/training: 0.5565240383148193\n",
      "evaluation/text/loss: 5.7251410484313965\n",
      "evaluation/text/perplexity: 306.47650146484375\n",
      "time/total: 69.74978709220886\n",
      "time/evaluation: 0.5725562572479248\n",
      "training/train_loss_mean: 5.750455641746521\n",
      "training/train_loss_std: 1.0311111586928947\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 55\n",
      "training/learning_rate: 9.999384872466111e-05\n",
      "time/sample_batch: 0.006975650787353516\n",
      "time/training: 0.5210518836975098\n",
      "evaluation/text/loss: 5.688563823699951\n",
      "evaluation/text/perplexity: 295.4689636230469\n",
      "time/total: 70.86633825302124\n",
      "time/evaluation: 0.5936858654022217\n",
      "training/train_loss_mean: 6.349915716383192\n",
      "training/train_loss_std: 0.5157661845173698\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 56\n",
      "training/learning_rate: 9.999114225230768e-05\n",
      "time/sample_batch: 0.007414817810058594\n",
      "time/training: 0.5623736381530762\n",
      "evaluation/text/loss: 5.706384181976318\n",
      "evaluation/text/perplexity: 300.7815246582031\n",
      "time/total: 71.89786195755005\n",
      "time/evaluation: 0.46743297576904297\n",
      "training/train_loss_mean: 6.236076593399048\n",
      "training/train_loss_std: 0.44372478026081846\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 57\n",
      "training/learning_rate: 9.998794376403081e-05\n",
      "time/sample_batch: 0.006229400634765625\n",
      "time/training: 0.6029253005981445\n",
      "evaluation/text/loss: 5.465860366821289\n",
      "evaluation/text/perplexity: 236.47923278808594\n",
      "time/total: 72.96672415733337\n",
      "time/evaluation: 0.4642922878265381\n",
      "training/train_loss_mean: 5.877012634277344\n",
      "training/train_loss_std: 1.1041100576099905\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 58\n",
      "training/learning_rate: 9.998425329480863e-05\n",
      "time/sample_batch: 0.0064656734466552734\n",
      "time/training: 0.5181121826171875\n",
      "evaluation/text/loss: 5.588414669036865\n",
      "evaluation/text/perplexity: 267.3114929199219\n",
      "time/total: 74.0280122756958\n",
      "time/evaluation: 0.5415256023406982\n",
      "training/train_loss_mean: 5.9417489528656\n",
      "training/train_loss_std: 0.8723080155606423\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 59\n",
      "training/learning_rate: 9.998007088499953e-05\n",
      "time/sample_batch: 0.00806117057800293\n",
      "time/training: 0.5719964504241943\n",
      "evaluation/text/loss: 5.759033679962158\n",
      "evaluation/text/perplexity: 317.04180908203125\n",
      "time/total: 75.20991396903992\n",
      "time/evaluation: 0.6082453727722168\n",
      "training/train_loss_mean: 6.26163694858551\n",
      "training/train_loss_std: 1.0075871347810381\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 60\n",
      "training/learning_rate: 9.997539658034169e-05\n",
      "time/sample_batch: 0.008039236068725586\n",
      "time/training: 0.5808031558990479\n",
      "evaluation/text/loss: 5.7617268562316895\n",
      "evaluation/text/perplexity: 317.8968200683594\n",
      "time/total: 76.57490539550781\n",
      "time/evaluation: 0.7823441028594971\n",
      "training/train_loss_mean: 6.232324314117432\n",
      "training/train_loss_std: 0.9806218031809354\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 61\n",
      "training/learning_rate: 9.997023043195252e-05\n",
      "time/sample_batch: 0.007054805755615234\n",
      "time/training: 0.5919651985168457\n",
      "evaluation/text/loss: 5.805088043212891\n",
      "evaluation/text/perplexity: 331.98443603515625\n",
      "time/total: 77.7607433795929\n",
      "time/evaluation: 0.5921750068664551\n",
      "training/train_loss_mean: 6.068134927749634\n",
      "training/train_loss_std: 0.7401381694776064\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 62\n",
      "training/learning_rate: 9.996516041859498e-05\n",
      "time/sample_batch: 0.007016658782958984\n",
      "time/training: 0.45552706718444824\n",
      "evaluation/text/loss: 5.785131454467773\n",
      "evaluation/text/perplexity: 325.4248046875\n",
      "time/total: 79.20062923431396\n",
      "time/evaluation: 0.9828188419342041\n",
      "training/train_loss_mean: 6.316709730360243\n",
      "training/train_loss_std: 0.32220017200306805\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 63\n",
      "training/learning_rate: 9.99590599272083e-05\n",
      "time/sample_batch: 0.006537914276123047\n",
      "time/training: 0.554816722869873\n",
      "evaluation/text/loss: 5.724503517150879\n",
      "evaluation/text/perplexity: 306.2811584472656\n",
      "time/total: 80.29922986030579\n",
      "time/evaluation: 0.5420572757720947\n",
      "training/train_loss_mean: 5.865134644508362\n",
      "training/train_loss_std: 0.9382529723673201\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 64\n",
      "training/learning_rate: 9.995314910930763e-05\n",
      "time/sample_batch: 0.008371591567993164\n",
      "time/training: 0.6421582698822021\n",
      "evaluation/text/loss: 5.5230865478515625\n",
      "evaluation/text/perplexity: 250.40673828125\n",
      "time/total: 81.45455884933472\n",
      "time/evaluation: 0.5114386081695557\n",
      "training/train_loss_mean: 6.510728571150038\n",
      "training/train_loss_std: 0.14963161838903666\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 65\n",
      "training/learning_rate: 9.99461145157604e-05\n",
      "time/sample_batch: 0.0075740814208984375\n",
      "time/training: 0.6402506828308105\n",
      "evaluation/text/loss: 5.5938568115234375\n",
      "evaluation/text/perplexity: 268.7702331542969\n",
      "time/total: 82.83688068389893\n",
      "time/evaluation: 0.7403004169464111\n",
      "training/train_loss_mean: 6.4897984027862545\n",
      "training/train_loss_std: 0.24466434158930603\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 66\n",
      "training/learning_rate: 9.993858839870582e-05\n",
      "time/sample_batch: 0.0071828365325927734\n",
      "time/training: 0.5371532440185547\n",
      "evaluation/text/loss: 5.470076084136963\n",
      "evaluation/text/perplexity: 237.47825622558594\n",
      "time/total: 84.05195903778076\n",
      "time/evaluation: 0.6761832237243652\n",
      "training/train_loss_mean: 6.086888694763184\n",
      "training/train_loss_std: 0.23861525968819303\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 67\n",
      "training/learning_rate: 9.993057084044833e-05\n",
      "time/sample_batch: 0.006580352783203125\n",
      "time/training: 0.4913954734802246\n",
      "evaluation/text/loss: 5.624148368835449\n",
      "evaluation/text/perplexity: 277.0362548828125\n",
      "time/total: 85.25440168380737\n",
      "time/evaluation: 0.7092330455780029\n",
      "training/train_loss_mean: 5.9431448698043825\n",
      "training/train_loss_std: 1.1005885026467528\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 68\n",
      "training/learning_rate: 9.992206192866663e-05\n",
      "time/sample_batch: 0.006871938705444336\n",
      "time/training: 0.5809271335601807\n",
      "evaluation/text/loss: 5.66965913772583\n",
      "evaluation/text/perplexity: 289.9356994628906\n",
      "time/total: 86.30375862121582\n",
      "time/evaluation: 0.46652865409851074\n",
      "training/train_loss_mean: 6.558671760559082\n",
      "training/train_loss_std: 0.419371505693652\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 69\n",
      "training/learning_rate: 9.991306175641283e-05\n",
      "time/sample_batch: 0.007969379425048828\n",
      "time/training: 0.7156643867492676\n",
      "evaluation/text/loss: 5.66406774520874\n",
      "evaluation/text/perplexity: 288.3190612792969\n",
      "time/total: 87.50186204910278\n",
      "time/evaluation: 0.48073840141296387\n",
      "training/train_loss_mean: 6.052228331565857\n",
      "training/train_loss_std: 0.9102816383418739\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 70\n",
      "training/learning_rate: 9.990357042211137e-05\n",
      "time/sample_batch: 0.0063436031341552734\n",
      "time/training: 0.5395910739898682\n",
      "evaluation/text/loss: 5.51644229888916\n",
      "evaluation/text/perplexity: 248.7484893798828\n",
      "time/total: 88.79688668251038\n",
      "time/evaluation: 0.753542423248291\n",
      "training/train_loss_mean: 6.205996036529541\n",
      "training/train_loss_std: 0.2989312397171276\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 71\n",
      "training/learning_rate: 9.989358802955798e-05\n",
      "time/sample_batch: 0.006758451461791992\n",
      "time/training: 0.5492165088653564\n",
      "evaluation/text/loss: 5.49747896194458\n",
      "evaluation/text/perplexity: 244.07583618164062\n",
      "time/total: 89.826425075531\n",
      "time/evaluation: 0.47865891456604004\n",
      "training/train_loss_mean: 5.581561529636383\n",
      "training/train_loss_std: 1.475030616650901\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 72\n",
      "training/learning_rate: 9.988418411156911e-05\n",
      "time/sample_batch: 0.006474971771240234\n",
      "time/training: 0.4957435131072998\n",
      "evaluation/text/loss: 5.672396659851074\n",
      "evaluation/text/perplexity: 290.73046875\n",
      "time/total: 90.8116021156311\n",
      "time/evaluation: 0.4877901077270508\n",
      "training/train_loss_mean: 6.1550619337293835\n",
      "training/train_loss_std: 0.28941539588733056\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 73\n",
      "training/learning_rate: 9.987326901352636e-05\n",
      "time/sample_batch: 0.006595134735107422\n",
      "time/training: 0.599184513092041\n",
      "evaluation/text/loss: 5.490185260772705\n",
      "evaluation/text/perplexity: 242.30209350585938\n",
      "time/total: 91.82610082626343\n",
      "time/evaluation: 0.41318798065185547\n",
      "training/train_loss_mean: 5.900127673149109\n",
      "training/train_loss_std: 1.2441243175132022\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 74\n",
      "training/learning_rate: 9.9861863188603e-05\n",
      "time/sample_batch: 0.00750732421875\n",
      "time/training: 0.5356612205505371\n",
      "evaluation/text/loss: 5.53812837600708\n",
      "evaluation/text/perplexity: 254.2017822265625\n",
      "time/total: 93.01774096488953\n",
      "time/evaluation: 0.6542809009552002\n",
      "training/train_loss_mean: 6.165660238265991\n",
      "training/train_loss_std: 0.4294232822876834\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 75\n",
      "training/learning_rate: 9.984996676153134e-05\n",
      "time/sample_batch: 0.007478475570678711\n",
      "time/training: 0.5656731128692627\n",
      "evaluation/text/loss: 5.77632999420166\n",
      "evaluation/text/perplexity: 322.57318115234375\n",
      "time/total: 94.1591727733612\n",
      "time/evaluation: 0.5739738941192627\n",
      "training/train_loss_mean: 6.100246477127075\n",
      "training/train_loss_std: 0.4015386370791582\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 76\n",
      "training/learning_rate: 9.983757986240877e-05\n",
      "time/sample_batch: 0.007742881774902344\n",
      "time/training: 0.6163406372070312\n",
      "evaluation/text/loss: 5.763223648071289\n",
      "evaluation/text/perplexity: 318.37298583984375\n",
      "time/total: 95.32024216651917\n",
      "time/evaluation: 0.5430116653442383\n",
      "training/train_loss_mean: 6.158453464508057\n",
      "training/train_loss_std: 0.3708540093228073\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 77\n",
      "training/learning_rate: 9.982470262669643e-05\n",
      "time/sample_batch: 0.0071048736572265625\n",
      "time/training: 0.6146841049194336\n",
      "evaluation/text/loss: 5.598348140716553\n",
      "evaluation/text/perplexity: 269.9800720214844\n",
      "time/total: 96.70260763168335\n",
      "time/evaluation: 0.7625746726989746\n",
      "training/train_loss_mean: 6.380948448181153\n",
      "training/train_loss_std: 0.5405279395259265\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 78\n",
      "training/learning_rate: 9.981133519521773e-05\n",
      "time/sample_batch: 0.007766246795654297\n",
      "time/training: 0.565016508102417\n",
      "evaluation/text/loss: 5.6458940505981445\n",
      "evaluation/text/perplexity: 283.1265869140625\n",
      "time/total: 97.81601977348328\n",
      "time/evaluation: 0.5466105937957764\n",
      "training/train_loss_mean: 5.832608437538147\n",
      "training/train_loss_std: 1.003521246477958\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 79\n",
      "training/learning_rate: 9.979747771415673e-05\n",
      "time/sample_batch: 0.0076198577880859375\n",
      "time/training: 0.5433285236358643\n",
      "evaluation/text/loss: 5.472751617431641\n",
      "evaluation/text/perplexity: 238.11448669433594\n",
      "time/total: 99.09671807289124\n",
      "time/evaluation: 0.7357063293457031\n",
      "training/train_loss_mean: 6.054640483856201\n",
      "training/train_loss_std: 0.6693994353337871\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 80\n",
      "training/learning_rate: 9.978313033505665e-05\n",
      "time/sample_batch: 0.007919788360595703\n",
      "time/training: 0.5681307315826416\n",
      "evaluation/text/loss: 5.685041427612305\n",
      "evaluation/text/perplexity: 294.4300537109375\n",
      "time/total: 100.36580729484558\n",
      "time/evaluation: 0.6987559795379639\n",
      "training/train_loss_mean: 5.72590434551239\n",
      "training/train_loss_std: 1.2520664981539487\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 81\n",
      "training/learning_rate: 9.97682932148181e-05\n",
      "time/sample_batch: 0.007247209548950195\n",
      "time/training: 0.5505044460296631\n",
      "evaluation/text/loss: 5.494577407836914\n",
      "evaluation/text/perplexity: 243.36865234375\n",
      "time/total: 101.57193160057068\n",
      "time/evaluation: 0.6539468765258789\n",
      "training/train_loss_mean: 5.688440799713135\n",
      "training/train_loss_std: 0.9562437436685849\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 82\n",
      "training/learning_rate: 9.975296651569749e-05\n",
      "time/sample_batch: 0.006872892379760742\n",
      "time/training: 0.6037960052490234\n",
      "evaluation/text/loss: 5.691853046417236\n",
      "evaluation/text/perplexity: 296.44244384765625\n",
      "time/total: 102.62722969055176\n",
      "time/evaluation: 0.4497535228729248\n",
      "training/train_loss_mean: 6.088595628738403\n",
      "training/train_loss_std: 0.39828905422885647\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 83\n",
      "training/learning_rate: 9.973715040530512e-05\n",
      "time/sample_batch: 0.0061817169189453125\n",
      "time/training: 0.5107617378234863\n",
      "evaluation/text/loss: 5.559208869934082\n",
      "evaluation/text/perplexity: 259.61737060546875\n",
      "time/total: 103.71379923820496\n",
      "time/evaluation: 0.5741212368011475\n",
      "training/train_loss_mean: 6.011752653121948\n",
      "training/train_loss_std: 0.7839396280843649\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 84\n",
      "training/learning_rate: 9.972084505660344e-05\n",
      "time/sample_batch: 0.006084918975830078\n",
      "time/training: 0.5036540031433105\n",
      "evaluation/text/loss: 5.662674427032471\n",
      "evaluation/text/perplexity: 287.9176330566406\n",
      "time/total: 104.95810151100159\n",
      "time/evaluation: 0.7389531135559082\n",
      "training/train_loss_mean: 5.003600990772247\n",
      "training/train_loss_std: 1.697988136811503\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 85\n",
      "training/learning_rate: 9.970405064790513e-05\n",
      "time/sample_batch: 0.007280588150024414\n",
      "time/training: 0.5703060626983643\n",
      "evaluation/text/loss: 5.625235557556152\n",
      "evaluation/text/perplexity: 277.3376159667969\n",
      "time/total: 106.34599494934082\n",
      "time/evaluation: 0.8159325122833252\n",
      "training/train_loss_mean: 6.024949407577514\n",
      "training/train_loss_std: 0.39703884754054597\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 86\n",
      "training/learning_rate: 9.968676736287116e-05\n",
      "time/sample_batch: 0.00848245620727539\n",
      "time/training: 0.5442905426025391\n",
      "evaluation/text/loss: 5.625316619873047\n",
      "evaluation/text/perplexity: 277.3600769042969\n",
      "time/total: 107.66982102394104\n",
      "time/evaluation: 0.7778117656707764\n",
      "training/train_loss_mean: 6.021066474914551\n",
      "training/train_loss_std: 0.5293629210262784\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 87\n",
      "training/learning_rate: 9.966899539050874e-05\n",
      "time/sample_batch: 0.007264137268066406\n",
      "time/training: 0.5389769077301025\n",
      "evaluation/text/loss: 5.551089763641357\n",
      "evaluation/text/perplexity: 257.5180358886719\n",
      "time/total: 108.8059151172638\n",
      "time/evaluation: 0.5952818393707275\n",
      "training/train_loss_mean: 5.562145733833313\n",
      "training/train_loss_std: 1.1738214532325555\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 88\n",
      "training/learning_rate: 9.965073492516937e-05\n",
      "time/sample_batch: 0.005501985549926758\n",
      "time/training: 0.49416494369506836\n",
      "evaluation/text/loss: 5.387409210205078\n",
      "evaluation/text/perplexity: 218.63621520996094\n",
      "time/total: 109.91035175323486\n",
      "time/evaluation: 0.6085114479064941\n",
      "training/train_loss_mean: 5.158038067817688\n",
      "training/train_loss_std: 1.4989074400710163\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 89\n",
      "training/learning_rate: 9.963198616654653e-05\n",
      "time/sample_batch: 0.007545948028564453\n",
      "time/training: 0.5908710956573486\n",
      "evaluation/text/loss: 5.534303188323975\n",
      "evaluation/text/perplexity: 253.2312774658203\n",
      "time/total: 111.10237050056458\n",
      "time/evaluation: 0.5995156764984131\n",
      "training/train_loss_mean: 6.236301040649414\n",
      "training/train_loss_std: 0.3323204563662644\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 90\n",
      "training/learning_rate: 9.961469496237841e-05\n",
      "time/sample_batch: 0.008457422256469727\n",
      "time/training: 0.5603005886077881\n",
      "evaluation/text/loss: 5.3971757888793945\n",
      "evaluation/text/perplexity: 220.78199768066406\n",
      "time/total: 112.23811078071594\n",
      "time/evaluation: 0.5736548900604248\n",
      "training/train_loss_mean: 5.623432609770033\n",
      "training/train_loss_std: 1.0765667208106056\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 91\n",
      "training/learning_rate: 9.95950190157957e-05\n",
      "time/sample_batch: 0.0057964324951171875\n",
      "time/training: 0.5315775871276855\n",
      "evaluation/text/loss: 5.552978515625\n",
      "evaluation/text/perplexity: 258.0048828125\n",
      "time/total: 113.4702615737915\n",
      "time/evaluation: 0.6988515853881836\n",
      "training/train_loss_mean: 6.018415832519532\n",
      "training/train_loss_std: 0.5135277212984477\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 92\n",
      "training/learning_rate: 9.957485538522976e-05\n",
      "time/sample_batch: 0.006997823715209961\n",
      "time/training: 0.5772593021392822\n",
      "evaluation/text/loss: 5.553910255432129\n",
      "evaluation/text/perplexity: 258.2453918457031\n",
      "time/total: 114.64703345298767\n",
      "time/evaluation: 0.5978045463562012\n",
      "training/train_loss_mean: 6.106870841979981\n",
      "training/train_loss_std: 0.3312813289807601\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 93\n",
      "training/learning_rate: 9.955420429118686e-05\n",
      "time/sample_batch: 0.006537199020385742\n",
      "time/training: 0.5443851947784424\n",
      "evaluation/text/loss: 5.456683158874512\n",
      "evaluation/text/perplexity: 234.31893920898438\n",
      "time/total: 115.94303059577942\n",
      "time/evaluation: 0.7500185966491699\n",
      "training/train_loss_mean: 5.751774001121521\n",
      "training/train_loss_std: 1.1900504658421216\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 94\n",
      "training/learning_rate: 9.953306595950404e-05\n",
      "time/sample_batch: 0.005731344223022461\n",
      "time/training: 0.5766322612762451\n",
      "evaluation/text/loss: 5.566525936126709\n",
      "evaluation/text/perplexity: 261.5239562988281\n",
      "time/total: 117.25862216949463\n",
      "time/evaluation: 0.7372457981109619\n",
      "training/train_loss_mean: 5.887167239189148\n",
      "training/train_loss_std: 1.368177268860055\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 95\n",
      "training/learning_rate: 9.951144062134673e-05\n",
      "time/sample_batch: 0.006730318069458008\n",
      "time/training: 0.5595343112945557\n",
      "evaluation/text/loss: 5.404282569885254\n",
      "evaluation/text/perplexity: 222.3566436767578\n",
      "time/total: 118.55862188339233\n",
      "time/evaluation: 0.7388284206390381\n",
      "training/train_loss_mean: 6.140814352035522\n",
      "training/train_loss_std: 0.599531875200185\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 96\n",
      "training/learning_rate: 9.948932851320614e-05\n",
      "time/sample_batch: 0.006712436676025391\n",
      "time/training: 0.6379597187042236\n",
      "evaluation/text/loss: 5.35676383972168\n",
      "evaluation/text/perplexity: 212.0376434326172\n",
      "time/total: 119.93306398391724\n",
      "time/evaluation: 0.7348833084106445\n",
      "training/train_loss_mean: 6.129794788360596\n",
      "training/train_loss_std: 0.2433755054119501\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 97\n",
      "training/learning_rate: 9.946672987689674e-05\n",
      "time/sample_batch: 0.006904125213623047\n",
      "time/training: 0.5598032474517822\n",
      "evaluation/text/loss: 5.278855800628662\n",
      "evaluation/text/perplexity: 196.14532470703125\n",
      "time/total: 121.03369355201721\n",
      "time/evaluation: 0.5392062664031982\n",
      "training/train_loss_mean: 6.037483787536621\n",
      "training/train_loss_std: 0.5447446300489449\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 98\n",
      "training/learning_rate: 9.944364495955362e-05\n",
      "time/sample_batch: 0.0046443939208984375\n",
      "time/training: 0.5368542671203613\n",
      "evaluation/text/loss: 5.526358127593994\n",
      "evaluation/text/perplexity: 251.22731018066406\n",
      "time/total: 122.08325362205505\n",
      "time/evaluation: 0.5110960006713867\n",
      "training/train_loss_mean: 5.752220535278321\n",
      "training/train_loss_std: 0.9680609090783981\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 99\n",
      "training/learning_rate: 9.94200740136297e-05\n",
      "time/sample_batch: 0.006786346435546875\n",
      "time/training: 0.5696887969970703\n",
      "evaluation/text/loss: 5.56709623336792\n",
      "evaluation/text/perplexity: 261.67315673828125\n",
      "time/total: 123.19873857498169\n",
      "time/evaluation: 0.5441446304321289\n",
      "training/train_loss_mean: 6.461590147018432\n",
      "training/train_loss_std: 0.3472604519879037\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 100\n",
      "training/learning_rate: 9.939601729689312e-05\n",
      "time/sample_batch: 0.006218910217285156\n",
      "time/training: 0.5629034042358398\n",
      "evaluation/text/loss: 5.538659572601318\n",
      "evaluation/text/perplexity: 254.33685302734375\n",
      "time/total: 124.57994985580444\n",
      "time/evaluation: 0.8167014122009277\n",
      "training/train_loss_mean: 5.826365333795548\n",
      "training/train_loss_std: 1.6586139397610355\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 101\n",
      "training/learning_rate: 9.937147507242424e-05\n",
      "time/sample_batch: 0.006403684616088867\n",
      "time/training: 0.5282602310180664\n",
      "evaluation/text/loss: 5.5548200607299805\n",
      "evaluation/text/perplexity: 258.4804382324219\n",
      "time/total: 125.84372687339783\n",
      "time/evaluation: 0.7337489128112793\n",
      "training/train_loss_mean: 5.972485160827636\n",
      "training/train_loss_std: 0.8395338487244536\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 102\n",
      "training/learning_rate: 9.934897218300567e-05\n",
      "time/sample_batch: 0.006025552749633789\n",
      "time/training: 0.47362637519836426\n",
      "evaluation/text/loss: 5.419892311096191\n",
      "evaluation/text/perplexity: 225.85479736328125\n",
      "time/total: 126.8838722705841\n",
      "time/evaluation: 0.5648386478424072\n",
      "training/train_loss_mean: 6.020047134823269\n",
      "training/train_loss_std: 0.6743056228409808\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 103\n",
      "training/learning_rate: 9.932350823764534e-05\n",
      "time/sample_batch: 0.006886005401611328\n",
      "time/training: 0.624671459197998\n",
      "evaluation/text/loss: 5.47131872177124\n",
      "evaluation/text/perplexity: 237.77354431152344\n",
      "time/total: 128.2834312915802\n",
      "time/evaluation: 0.7731578350067139\n",
      "training/train_loss_mean: 5.460190570354461\n",
      "training/train_loss_std: 1.4326804981391599\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 104\n",
      "training/learning_rate: 9.929755957750015e-05\n",
      "time/sample_batch: 0.0075168609619140625\n",
      "time/training: 0.5842111110687256\n",
      "evaluation/text/loss: 5.424733638763428\n",
      "evaluation/text/perplexity: 226.9508819580078\n",
      "time/total: 129.37534356117249\n",
      "time/evaluation: 0.5061032772064209\n",
      "training/train_loss_mean: 5.881138563156128\n",
      "training/train_loss_std: 1.0505783720386428\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 105\n",
      "training/learning_rate: 9.927112648634053e-05\n",
      "time/sample_batch: 0.005849599838256836\n",
      "time/training: 0.4877135753631592\n",
      "evaluation/text/loss: 5.442911148071289\n",
      "evaluation/text/perplexity: 231.114013671875\n",
      "time/total: 130.2787983417511\n",
      "time/evaluation: 0.41414833068847656\n",
      "training/train_loss_mean: 6.0663378715515135\n",
      "training/train_loss_std: 0.3513544720287048\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 106\n",
      "training/learning_rate: 9.92442092532345e-05\n",
      "time/sample_batch: 0.005566596984863281\n",
      "time/training: 0.551306962966919\n",
      "evaluation/text/loss: 5.476963043212891\n",
      "evaluation/text/perplexity: 239.11941528320312\n",
      "time/total: 131.55867719650269\n",
      "time/evaluation: 0.726982831954956\n",
      "training/train_loss_mean: 6.0348817825317385\n",
      "training/train_loss_std: 0.5501616754191014\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 107\n",
      "training/learning_rate: 9.921957004525622e-05\n",
      "time/sample_batch: 0.006799936294555664\n",
      "time/training: 0.5160682201385498\n",
      "evaluation/text/loss: 5.325491905212402\n",
      "evaluation/text/perplexity: 205.50942993164062\n",
      "time/total: 132.67014122009277\n",
      "time/evaluation: 0.593914270401001\n",
      "training/train_loss_mean: 5.98424408170912\n",
      "training/train_loss_std: 0.3144543205900006\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 108\n",
      "training/learning_rate: 9.919453913892206e-05\n",
      "time/sample_batch: 0.006273746490478516\n",
      "time/training: 0.5579018592834473\n",
      "evaluation/text/loss: 5.500346660614014\n",
      "evaluation/text/perplexity: 244.7767791748047\n",
      "time/total: 134.21735334396362\n",
      "time/evaluation: 0.987647294998169\n",
      "training/train_loss_mean: 6.134810924530029\n",
      "training/train_loss_std: 0.5821286831264588\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 109\n",
      "training/learning_rate: 9.916626789126256e-05\n",
      "time/sample_batch: 0.006989002227783203\n",
      "time/training: 0.53704833984375\n",
      "evaluation/text/loss: 5.584331512451172\n",
      "evaluation/text/perplexity: 266.2222595214844\n",
      "time/total: 135.23008823394775\n",
      "time/evaluation: 0.47404909133911133\n",
      "training/train_loss_mean: 6.088761234283448\n",
      "training/train_loss_std: 0.4932915704814166\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 110\n",
      "training/learning_rate: 9.91375136483735e-05\n",
      "time/sample_batch: 0.0058062076568603516\n",
      "time/training: 0.5511486530303955\n",
      "evaluation/text/loss: 5.385331153869629\n",
      "evaluation/text/perplexity: 218.18234252929688\n",
      "time/total: 136.25673151016235\n",
      "time/evaluation: 0.47382259368896484\n",
      "training/train_loss_mean: 5.943963432312012\n",
      "training/train_loss_std: 0.4518897364582579\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 111\n",
      "training/learning_rate: 9.91082767247066e-05\n",
      "time/sample_batch: 0.007572174072265625\n",
      "time/training: 0.6015241146087646\n",
      "evaluation/text/loss: 5.481559753417969\n",
      "evaluation/text/perplexity: 240.22109985351562\n",
      "time/total: 137.6624207496643\n",
      "time/evaluation: 0.8025069236755371\n",
      "training/train_loss_mean: 5.075005316734314\n",
      "training/train_loss_std: 1.709316147477672\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iters = args.training_steps // args.log_eval_freq\n",
    "print(f'iters:{iters}')\n",
    "for i in range(iters):\n",
    "    logs = train_iteration(args.log_eval_freq, i)\n",
    "    accelerator.log(logs)\n",
    "\n",
    "## Save model at end of training only if not saving checkpoints\n",
    "if args.save_model and args.save_mode == 'last':\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144b80-96a4-4b4a-b6c9-301ea7320e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fce31-9001-48d9-b550-b6eb398dc9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b83e-cd51-4e8c-8490-afd13048f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6daec-23da-4476-bcc2-818dd032765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

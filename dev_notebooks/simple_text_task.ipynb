{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db14cc1d-8a0c-4d57-b315-9d71b22dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 23:18:17 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PG5...  On   | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    86W / 330W |   5257MiB / 40960MiB |     38%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PG5...  On   | 00000000:13:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PG5...  On   | 00000000:14:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    49W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PG5...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    59W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PG5...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    50W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PG5...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    54W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PG5...  On   | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PG5...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3604288      C   ...onda/envs/neko/bin/python     5254MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b849366c-9dea-4f3d-89fb-45d7f10bf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e46c4c-ff45-4760-8ae4-c55fa01f39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8b8ba88-f71a-4ed1-9c64-2c37d30aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            config.flash = flash\n",
    "            config.gate = False\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            # assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "    def predict_text(self, input_tokens, max_length=20, deterministic=True):\n",
    "        predicted_tokens = input_tokens.clone()\n",
    "        logits_list = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(predicted_tokens[:, -1:])\n",
    "            token_masks = torch.ones((predicted_tokens.shape[0], 1), device=self.device)  # Ensure batch dimension\n",
    "    \n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time-step logits\n",
    "            logits_list.append(logits.unsqueeze(1))\n",
    "    \n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Ensure it keeps batch dimension\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            predicted_tokens = torch.cat([predicted_tokens, next_token], dim=1)\n",
    "    \n",
    "        all_logits = torch.cat(logits_list, dim=1)\n",
    "        return predicted_tokens[:, input_tokens.size(1):], all_logits\n",
    "\n",
    "    \n",
    "    def predict_text_single_single(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            if token.numel() == 1:  # Checking if token is a single element\n",
    "                predicted_tokens.append(token.item())\n",
    "            else:\n",
    "                print(f\"Expected a single element, got {token.numel()} elements.\")\n",
    "    \n",
    "            input_tokens = torch.cat([input_tokens, token], dim=1)  # Append the predicted token\n",
    "\n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51983f70-9cc5-42ad-9b1c-d797106857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        \n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "        \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "\n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "        \n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "\n",
    "        # Forward pass    \n",
    "        logits, loss = model(batch_dicts, compute_loss=True)\n",
    "        \n",
    "        # total_tokens = input_tokens.size(0) * input_tokens.size(1)\n",
    "        # print(f'total tokens:{total_tokens}')\n",
    "        avg_loss = loss.item() \n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "\n",
    "    def evaluate_single_single(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            input_tokens = batch_dict['text'].to(device=model.device)\n",
    "            target_tokens = batch_dict['target'].to(device=model.device)\n",
    "    \n",
    "            total_loss_per_sequence = 0.0\n",
    "            pred_tokens = []\n",
    "    \n",
    "            for idx in range(input_tokens.size(0)):\n",
    "                pred_logits, single_pred_tokens = model.predict_text({'text': input_tokens[idx].unsqueeze(0)}, max_length=1, deterministic=deterministic)\n",
    "                loss = torch.nn.functional.cross_entropy(pred_logits, target_tokens[idx].unsqueeze(0))\n",
    "                total_loss_per_sequence += loss.item()\n",
    "                pred_tokens.extend(single_pred_tokens)\n",
    "            \n",
    "            total_loss += total_loss_per_sequence / input_tokens.size(0)\n",
    "            total_tokens += input_tokens.size(0)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                decoded_input = self.text_tokenizer.decode(input_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_target = self.text_tokenizer.decode(target_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_prediction = self.text_tokenizer.decode(torch.tensor(pred_tokens), skip_special_tokens=True)            \n",
    "                print(f'=>Input: {decoded_input} \\n =>Target: {decoded_target} \\n =>Prediction: {decoded_prediction}')\n",
    "    \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c0f0c-08cf-4220-81e4-cd4cf39c8c68",
   "metadata": {},
   "source": [
    "## trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1204c27b-b31f-422d-ae68-4b13beab83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=1000,\n",
    "    log_eval_freq=10,\n",
    "    warmup_steps=500,\n",
    "    batch_size=4,\n",
    "    sequence_length=1024,\n",
    "    eval_episodes=5,\n",
    "    text_prop=1,\n",
    "    # eval_text_log_examples=True,\n",
    "    pretrained_lm='gpt2',\n",
    "    text_datasets=['wikitext-2-v1'],\n",
    "    text_datasets_paths=['wikitext'],\n",
    "    use_wandb=True,\n",
    "    device='cuda',\n",
    "    eval_mode='stochastic',\n",
    "    eval_text_num_examples=100,\n",
    "    # disable_cosine_decay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08b1a4a-f44f-45b1-b282-4076e2ee5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained GPT2 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2Model: ['wpe.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.attn.masked_bias', 'h.11.attn.masked_bias', 'h.9.attn.masked_bias', 'h.1.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.2.attn.masked_bias', 'h.10.attn.masked_bias', 'h.6.attn.masked_bias', 'h.0.attn.masked_bias', 'h.8.attn.masked_bias', 'h.5.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhavul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhavul/bhavul/NEKO/dev_notebooks/wandb/run-20240510_232129-7wavrss1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhavul/gato-control/runs/7wavrss1' target=\"_blank\">neko-gato_24-05-10_23-21-24</a></strong> to <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhavul/gato-control/runs/7wavrss1' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/7wavrss1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    # mixed_precision=args.mixed_precision,\n",
    "    # gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'\n",
    "\n",
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )\n",
    "model = accelerator.prepare(model)\n",
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')\n",
    "\n",
    "tasks = [TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)]\n",
    "args = args\n",
    "print_logs = True # args.print_logs\n",
    "device = torch.device(args.device)\n",
    "\n",
    "min_lr = args.learning_rate / args.min_factor\n",
    "deterministic = args.eval_mode == 'deterministic'\n",
    "\n",
    "exp_name = exp_name\n",
    "exp_dir = os.path.join(args.save_dir, exp_name)\n",
    "\n",
    "steps = 0\n",
    "start_time = None\n",
    "\n",
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c8d8bd-a174-479b-9506-aab36fb4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6fca9a2-f025-4158-a493-a2d128090ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9e474d-70b3-4f05-967e-e6d4ab349358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(num_steps, iter):\n",
    "    logs = {}\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for i in range(num_steps):\n",
    "        steps += 1\n",
    "        result = train_step()\n",
    "        if result is None:\n",
    "            # steps -= 1\n",
    "            # print(\"Skipped a training step due to empty batch.\")\n",
    "            continue\n",
    "        train_loss, step_logs = result\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # add logs from last train_step as well\n",
    "    for log in step_logs:\n",
    "        logs[log] = step_logs[log]\n",
    "\n",
    "    logs['time/training'] = time.time() - train_start\n",
    "\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # loop over eval for each env\n",
    "    with torch.no_grad():\n",
    "        for task in tasks:\n",
    "            eval_logs = {}\n",
    "            if isinstance(task, TextTask):\n",
    "                eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, log_examples_to_output=args.eval_text_log_examples)\n",
    "                for k, v in eval_logs.items():\n",
    "                    logs[f'evaluation/text/{k}'] = v\n",
    "                pass\n",
    "\n",
    "    logs['time/total'] = time.time() - start_time\n",
    "    logs['time/evaluation'] = time.time() - eval_start\n",
    "    logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "    logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "            print('=' * 80)\n",
    "\n",
    "    ## Save model\n",
    "    if args.save_model and args.save_mode == 'checkpoint':\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "                \n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6e91ede-9d47-4a85-9de7-64cf265bb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e5ea46-ad2d-43b0-ae46-e8a5bac73d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:100\n",
      "================================================================================\n",
      "Iteration 0\n",
      "training/learning_rate: 1.8981999999999999e-06\n",
      "time/sample_batch: 0.00749969482421875\n",
      "time/training: 2.373403549194336\n",
      "evaluation/text/loss: 14.654378890991211\n",
      "evaluation/text/perplexity: 2313747.0\n",
      "time/total: 3.2332658767700195\n",
      "time/evaluation: 0.858971118927002\n",
      "training/train_loss_mean: 14.76639289855957\n",
      "training/train_loss_std: 1.733960090171368\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 1\n",
      "training/learning_rate: 3.8962e-06\n",
      "time/sample_batch: 0.0076787471771240234\n",
      "time/training: 0.8020565509796143\n",
      "evaluation/text/loss: 14.462224960327148\n",
      "evaluation/text/perplexity: 1909257.5\n",
      "time/total: 5.742817163467407\n",
      "time/evaluation: 1.7057504653930664\n",
      "training/train_loss_mean: 14.813972854614258\n",
      "training/train_loss_std: 0.9747530722229819\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 2\n",
      "training/learning_rate: 5.8942e-06\n",
      "time/sample_batch: 0.007020235061645508\n",
      "time/training: 0.7825236320495605\n",
      "evaluation/text/loss: 12.692195892333984\n",
      "evaluation/text/perplexity: 325200.0625\n",
      "time/total: 6.996847867965698\n",
      "time/evaluation: 0.4696922302246094\n",
      "training/train_loss_mean: 13.524905109405518\n",
      "training/train_loss_std: 0.7808453334684958\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 3\n",
      "training/learning_rate: 7.8922e-06\n",
      "time/sample_batch: 0.0065250396728515625\n",
      "time/training: 0.727968692779541\n",
      "evaluation/text/loss: 11.666350364685059\n",
      "evaluation/text/perplexity: 116582.0234375\n",
      "time/total: 8.48390793800354\n",
      "time/evaluation: 0.7574195861816406\n",
      "training/train_loss_mean: 12.799243068695068\n",
      "training/train_loss_std: 1.5551804330479198\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 4\n",
      "training/learning_rate: 9.8902e-06\n",
      "time/sample_batch: 0.004772186279296875\n",
      "time/training: 0.7075490951538086\n",
      "evaluation/text/loss: 11.253114700317383\n",
      "evaluation/text/perplexity: 77119.75\n",
      "time/total: 10.091127157211304\n",
      "time/evaluation: 0.8980138301849365\n",
      "training/train_loss_mean: 11.785938358306884\n",
      "training/train_loss_std: 0.37429025437445645\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 5\n",
      "training/learning_rate: 1.18882e-05\n",
      "time/sample_batch: 0.006261348724365234\n",
      "time/training: 0.7223258018493652\n",
      "evaluation/text/loss: 11.250905990600586\n",
      "evaluation/text/perplexity: 76949.6015625\n",
      "time/total: 11.393642902374268\n",
      "time/evaluation: 0.5784990787506104\n",
      "training/train_loss_mean: 11.618576145172119\n",
      "training/train_loss_std: 0.6638206739852741\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 6\n",
      "training/learning_rate: 1.3886200000000003e-05\n",
      "time/sample_batch: 0.006259441375732422\n",
      "time/training: 0.7598543167114258\n",
      "evaluation/text/loss: 10.878633499145508\n",
      "evaluation/text/perplexity: 53031.08203125\n",
      "time/total: 12.818975448608398\n",
      "time/evaluation: 0.663776159286499\n",
      "training/train_loss_mean: 11.116311168670654\n",
      "training/train_loss_std: 0.18270065347048298\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 7\n",
      "training/learning_rate: 1.58842e-05\n",
      "time/sample_batch: 0.00642704963684082\n",
      "time/training: 0.7045309543609619\n",
      "evaluation/text/loss: 10.68781566619873\n",
      "evaluation/text/perplexity: 43818.6875\n",
      "time/total: 14.167534351348877\n",
      "time/evaluation: 0.6423017978668213\n",
      "training/train_loss_mean: 10.910169124603271\n",
      "training/train_loss_std: 0.16639542275964078\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 8\n",
      "training/learning_rate: 1.78822e-05\n",
      "time/sample_batch: 0.0070476531982421875\n",
      "time/training: 0.7747080326080322\n",
      "evaluation/text/loss: 10.597365379333496\n",
      "evaluation/text/perplexity: 40029.23828125\n",
      "time/total: 15.834954500198364\n",
      "time/evaluation: 0.8910953998565674\n",
      "training/train_loss_mean: 10.855062580108642\n",
      "training/train_loss_std: 0.19014368723712952\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 9\n",
      "training/learning_rate: 1.98802e-05\n",
      "time/sample_batch: 0.005314350128173828\n",
      "time/training: 0.6638989448547363\n",
      "evaluation/text/loss: 10.442875862121582\n",
      "evaluation/text/perplexity: 34299.1484375\n",
      "time/total: 17.43876028060913\n",
      "time/evaluation: 0.9382936954498291\n",
      "training/train_loss_mean: 10.669064807891846\n",
      "training/train_loss_std: 0.11429141116942917\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 10\n",
      "training/learning_rate: 2.18782e-05\n",
      "time/sample_batch: 0.006289005279541016\n",
      "time/training: 0.7095320224761963\n",
      "evaluation/text/loss: 10.213194847106934\n",
      "evaluation/text/perplexity: 27260.521484375\n",
      "time/total: 18.974103450775146\n",
      "time/evaluation: 0.8242120742797852\n",
      "training/train_loss_mean: 10.465315437316894\n",
      "training/train_loss_std: 0.19018769861833615\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 11\n",
      "training/learning_rate: 2.38762e-05\n",
      "time/sample_batch: 0.006038665771484375\n",
      "time/training: 0.7916522026062012\n",
      "evaluation/text/loss: 9.975848197937012\n",
      "evaluation/text/perplexity: 21500.859375\n",
      "time/total: 20.545461177825928\n",
      "time/evaluation: 0.7779788970947266\n",
      "training/train_loss_mean: 10.383659648895264\n",
      "training/train_loss_std: 0.09290031454334315\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 12\n",
      "training/learning_rate: 2.5874199999999997e-05\n",
      "time/sample_batch: 0.005232095718383789\n",
      "time/training: 0.6561245918273926\n",
      "evaluation/text/loss: 9.63415813446045\n",
      "evaluation/text/perplexity: 15277.8310546875\n",
      "time/total: 21.979145526885986\n",
      "time/evaluation: 0.775874137878418\n",
      "training/train_loss_mean: 10.038749313354492\n",
      "training/train_loss_std: 0.23048892973120141\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 13\n",
      "training/learning_rate: 2.7872199999999997e-05\n",
      "time/sample_batch: 0.007277965545654297\n",
      "time/training: 0.7314670085906982\n",
      "evaluation/text/loss: 9.041498184204102\n",
      "evaluation/text/perplexity: 8446.421875\n",
      "time/total: 23.647807121276855\n",
      "time/evaluation: 0.9355053901672363\n",
      "training/train_loss_mean: 9.737972450256347\n",
      "training/train_loss_std: 0.1197897691076381\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 14\n",
      "training/learning_rate: 2.96704e-05\n",
      "time/sample_batch: 0.005371809005737305\n",
      "time/training: 0.6254847049713135\n",
      "evaluation/text/loss: 8.408286094665527\n",
      "evaluation/text/perplexity: 4484.06884765625\n",
      "time/total: 24.784637451171875\n",
      "time/evaluation: 0.5095696449279785\n",
      "training/train_loss_mean: 8.89223665661282\n",
      "training/train_loss_std: 0.5531925200243221\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 15\n",
      "training/learning_rate: 3.16684e-05\n",
      "time/sample_batch: 0.008412837982177734\n",
      "time/training: 0.8677761554718018\n",
      "evaluation/text/loss: 8.299847602844238\n",
      "evaluation/text/perplexity: 4023.25927734375\n",
      "time/total: 26.589699745178223\n",
      "time/evaluation: 0.9358913898468018\n",
      "training/train_loss_mean: 8.900198364257813\n",
      "training/train_loss_std: 0.20933135831483976\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 16\n",
      "training/learning_rate: 3.366640000000001e-05\n",
      "time/sample_batch: 0.007191181182861328\n",
      "time/training: 0.8197920322418213\n",
      "evaluation/text/loss: 7.871127605438232\n",
      "evaluation/text/perplexity: 2620.518798828125\n",
      "time/total: 28.058976888656616\n",
      "time/evaluation: 0.6477408409118652\n",
      "training/train_loss_mean: 8.57477707862854\n",
      "training/train_loss_std: 0.42231871999478954\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 17\n",
      "training/learning_rate: 3.5464600000000006e-05\n",
      "time/sample_batch: 0.007431507110595703\n",
      "time/training: 0.6255283355712891\n",
      "evaluation/text/loss: 7.679641246795654\n",
      "evaluation/text/perplexity: 2163.84326171875\n",
      "time/total: 29.28910541534424\n",
      "time/evaluation: 0.6027894020080566\n",
      "training/train_loss_mean: 8.322198390960693\n",
      "training/train_loss_std: 0.20672418435811007\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 18\n",
      "training/learning_rate: 3.74626e-05\n",
      "time/sample_batch: 0.006174564361572266\n",
      "time/training: 0.6753370761871338\n",
      "evaluation/text/loss: 7.604253768920898\n",
      "evaluation/text/perplexity: 2006.7138671875\n",
      "time/total: 30.663930654525757\n",
      "time/evaluation: 0.6977934837341309\n",
      "training/train_loss_mean: 7.912945365905761\n",
      "training/train_loss_std: 0.578333319467558\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 19\n",
      "training/learning_rate: 3.9460600000000006e-05\n",
      "time/sample_batch: 0.006966590881347656\n",
      "time/training: 0.781757116317749\n",
      "evaluation/text/loss: 7.2586164474487305\n",
      "evaluation/text/perplexity: 1420.2901611328125\n",
      "time/total: 32.37645888328552\n",
      "time/evaluation: 0.9289669990539551\n",
      "training/train_loss_mean: 7.793363475799561\n",
      "training/train_loss_std: 0.40032307380508064\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 20\n",
      "training/learning_rate: 4.14586e-05\n",
      "time/sample_batch: 0.006556272506713867\n",
      "time/training: 0.7558956146240234\n",
      "evaluation/text/loss: 7.13633918762207\n",
      "evaluation/text/perplexity: 1256.8189697265625\n",
      "time/total: 33.96344709396362\n",
      "time/evaluation: 0.8295247554779053\n",
      "training/train_loss_mean: 7.51289005279541\n",
      "training/train_loss_std: 0.6976735156223665\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 21\n",
      "training/learning_rate: 4.345660000000001e-05\n",
      "time/sample_batch: 0.006662607192993164\n",
      "time/training: 0.8376169204711914\n",
      "evaluation/text/loss: 6.801732063293457\n",
      "evaluation/text/perplexity: 899.4037475585938\n",
      "time/total: 35.62937951087952\n",
      "time/evaluation: 0.8266668319702148\n",
      "training/train_loss_mean: 7.618234348297119\n",
      "training/train_loss_std: 0.2635315101286832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 22\n",
      "training/learning_rate: 4.5454600000000004e-05\n",
      "time/sample_batch: 0.006225109100341797\n",
      "time/training: 0.6583623886108398\n",
      "evaluation/text/loss: 6.787586688995361\n",
      "evaluation/text/perplexity: 886.7709350585938\n",
      "time/total: 37.21971797943115\n",
      "time/evaluation: 0.9303059577941895\n",
      "training/train_loss_mean: 7.066165399551392\n",
      "training/train_loss_std: 0.7660429281624667\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 23\n",
      "training/learning_rate: 4.745260000000001e-05\n",
      "time/sample_batch: 0.005772590637207031\n",
      "time/training: 0.696735143661499\n",
      "evaluation/text/loss: 6.713130950927734\n",
      "evaluation/text/perplexity: 823.1438598632812\n",
      "time/total: 38.5795361995697\n",
      "time/evaluation: 0.6614322662353516\n",
      "training/train_loss_mean: 7.039637517929077\n",
      "training/train_loss_std: 0.9450029517503199\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 24\n",
      "training/learning_rate: 4.9450600000000004e-05\n",
      "time/sample_batch: 0.007821798324584961\n",
      "time/training: 0.7710344791412354\n",
      "evaluation/text/loss: 6.707785129547119\n",
      "evaluation/text/perplexity: 818.7551879882812\n",
      "time/total: 40.28814625740051\n",
      "time/evaluation: 0.9357476234436035\n",
      "training/train_loss_mean: 7.480401039123535\n",
      "training/train_loss_std: 0.23944443798914597\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 25\n",
      "training/learning_rate: 5.124880000000001e-05\n",
      "time/sample_batch: 0.006108760833740234\n",
      "time/training: 0.6803743839263916\n",
      "evaluation/text/loss: 6.592939376831055\n",
      "evaluation/text/perplexity: 729.9232177734375\n",
      "time/total: 41.859204053878784\n",
      "time/evaluation: 0.8889868259429932\n",
      "training/train_loss_mean: 7.247793992360433\n",
      "training/train_loss_std: 0.2533975801016975\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 26\n",
      "training/learning_rate: 5.324680000000001e-05\n",
      "time/sample_batch: 0.006073474884033203\n",
      "time/training: 0.7256979942321777\n",
      "evaluation/text/loss: 6.55360221862793\n",
      "evaluation/text/perplexity: 701.7675170898438\n",
      "time/total: 43.47948408126831\n",
      "time/evaluation: 0.8928475379943848\n",
      "training/train_loss_mean: 7.2849095344543455\n",
      "training/train_loss_std: 0.4002575106272381\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 27\n",
      "training/learning_rate: 5.524480000000001e-05\n",
      "time/sample_batch: 0.006835460662841797\n",
      "time/training: 0.6786653995513916\n",
      "evaluation/text/loss: 6.587131500244141\n",
      "evaluation/text/perplexity: 725.6962280273438\n",
      "time/total: 44.77562594413757\n",
      "time/evaluation: 0.615741491317749\n",
      "training/train_loss_mean: 6.665564060211182\n",
      "training/train_loss_std: 1.4360379700233687\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 28\n",
      "training/learning_rate: 5.7242799999999993e-05\n",
      "time/sample_batch: 0.009281635284423828\n",
      "time/training: 0.8205657005310059\n",
      "evaluation/text/loss: 6.312256813049316\n",
      "evaluation/text/perplexity: 551.2877197265625\n",
      "time/total: 46.34513735771179\n",
      "time/evaluation: 0.7472589015960693\n",
      "training/train_loss_mean: 6.837360525131226\n",
      "training/train_loss_std: 0.5838981228826973\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 29\n",
      "training/learning_rate: 5.924080000000001e-05\n",
      "time/sample_batch: 0.006492137908935547\n",
      "time/training: 0.7771494388580322\n",
      "evaluation/text/loss: 6.356621742248535\n",
      "evaluation/text/perplexity: 576.2962036132812\n",
      "time/total: 47.81916809082031\n",
      "time/evaluation: 0.6951982975006104\n",
      "training/train_loss_mean: 6.5409444808959964\n",
      "training/train_loss_std: 1.3766238932727468\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 30\n",
      "training/learning_rate: 6.12388e-05\n",
      "time/sample_batch: 0.007337331771850586\n",
      "time/training: 0.7189137935638428\n",
      "evaluation/text/loss: 6.348356246948242\n",
      "evaluation/text/perplexity: 571.5524291992188\n",
      "time/total: 49.110896825790405\n",
      "time/evaluation: 0.571202278137207\n",
      "training/train_loss_mean: 6.9246423721313475\n",
      "training/train_loss_std: 0.4885436718981844\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 31\n",
      "training/learning_rate: 6.303700000000001e-05\n",
      "time/sample_batch: 0.0053310394287109375\n",
      "time/training: 0.7117016315460205\n",
      "evaluation/text/loss: 6.156687259674072\n",
      "evaluation/text/perplexity: 471.8623352050781\n",
      "time/total: 50.582799434661865\n",
      "time/evaluation: 0.7584598064422607\n",
      "training/train_loss_mean: 6.899749596913655\n",
      "training/train_loss_std: 0.5304712013238537\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 32\n",
      "training/learning_rate: 6.5035e-05\n",
      "time/sample_batch: 0.006364107131958008\n",
      "time/training: 0.6949949264526367\n",
      "evaluation/text/loss: 6.3702392578125\n",
      "evaluation/text/perplexity: 584.1975708007812\n",
      "time/total: 51.97308564186096\n",
      "time/evaluation: 0.6936290264129639\n",
      "training/train_loss_mean: 6.412975215911866\n",
      "training/train_loss_std: 0.8044947967644936\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 33\n",
      "training/learning_rate: 6.703300000000001e-05\n",
      "time/sample_batch: 0.0064084529876708984\n",
      "time/training: 0.7324004173278809\n",
      "evaluation/text/loss: 6.264100551605225\n",
      "evaluation/text/perplexity: 525.3688354492188\n",
      "time/total: 53.475757360458374\n",
      "time/evaluation: 0.7686159610748291\n",
      "training/train_loss_mean: 6.504278445243836\n",
      "training/train_loss_std: 1.4288799304720075\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 34\n",
      "training/learning_rate: 6.903100000000001e-05\n",
      "time/sample_batch: 0.007125377655029297\n",
      "time/training: 0.7804687023162842\n",
      "evaluation/text/loss: 6.170323848724365\n",
      "evaluation/text/perplexity: 478.34100341796875\n",
      "time/total: 54.95578622817993\n",
      "time/evaluation: 0.6978945732116699\n",
      "training/train_loss_mean: 6.516653704643249\n",
      "training/train_loss_std: 1.1686602212879602\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 35\n",
      "training/learning_rate: 7.102900000000001e-05\n",
      "time/sample_batch: 0.00539708137512207\n",
      "time/training: 0.6938343048095703\n",
      "evaluation/text/loss: 6.103055477142334\n",
      "evaluation/text/perplexity: 447.22216796875\n",
      "time/total: 56.283626317977905\n",
      "time/evaluation: 0.6323671340942383\n",
      "training/train_loss_mean: 6.410304355621338\n",
      "training/train_loss_std: 1.3159975463911648\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 36\n",
      "training/learning_rate: 7.3027e-05\n",
      "time/sample_batch: 0.00944972038269043\n",
      "time/training: 0.907505989074707\n",
      "evaluation/text/loss: 6.215169906616211\n",
      "evaluation/text/perplexity: 500.2809753417969\n",
      "time/total: 57.88774299621582\n",
      "time/evaluation: 0.695030689239502\n",
      "training/train_loss_mean: 7.012419891357422\n",
      "training/train_loss_std: 0.4901645430984265\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 37\n",
      "training/learning_rate: 7.502500000000001e-05\n",
      "time/sample_batch: 0.007061004638671875\n",
      "time/training: 0.7751607894897461\n",
      "evaluation/text/loss: 5.839430809020996\n",
      "evaluation/text/perplexity: 343.5837097167969\n",
      "time/total: 59.2877242565155\n",
      "time/evaluation: 0.6192831993103027\n",
      "training/train_loss_mean: 7.02755708694458\n",
      "training/train_loss_std: 0.25595167967809357\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 38\n",
      "training/learning_rate: 7.702300000000001e-05\n",
      "time/sample_batch: 0.008510112762451172\n",
      "time/training: 0.723191499710083\n",
      "evaluation/text/loss: 6.152745723724365\n",
      "evaluation/text/perplexity: 470.0061340332031\n",
      "time/total: 60.936562061309814\n",
      "time/evaluation: 0.9239616394042969\n",
      "training/train_loss_mean: 6.569005632400513\n",
      "training/train_loss_std: 0.4931559030672471\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 39\n",
      "training/learning_rate: 7.9021e-05\n",
      "time/sample_batch: 0.008031845092773438\n",
      "time/training: 0.7610154151916504\n",
      "evaluation/text/loss: 6.074233055114746\n",
      "evaluation/text/perplexity: 434.51611328125\n",
      "time/total: 62.65344452857971\n",
      "time/evaluation: 0.9541361331939697\n",
      "training/train_loss_mean: 6.792656755447387\n",
      "training/train_loss_std: 0.3909259537530112\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 40\n",
      "training/learning_rate: 8.1019e-05\n",
      "time/sample_batch: 0.00618433952331543\n",
      "time/training: 0.7150106430053711\n",
      "evaluation/text/loss: 5.839643478393555\n",
      "evaluation/text/perplexity: 343.65679931640625\n",
      "time/total: 64.02490019798279\n",
      "time/evaluation: 0.6546323299407959\n",
      "training/train_loss_mean: 6.407521104812622\n",
      "training/train_loss_std: 1.1826996079919136\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 41\n",
      "training/learning_rate: 8.301700000000001e-05\n",
      "time/sample_batch: 0.00654149055480957\n",
      "time/training: 0.6762745380401611\n",
      "evaluation/text/loss: 6.002527713775635\n",
      "evaluation/text/perplexity: 404.4498291015625\n",
      "time/total: 65.46361827850342\n",
      "time/evaluation: 0.7607071399688721\n",
      "training/train_loss_mean: 5.876044416427613\n",
      "training/train_loss_std: 1.1688318588733884\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 42\n",
      "training/learning_rate: 8.501500000000001e-05\n",
      "time/sample_batch: 0.006557941436767578\n",
      "time/training: 0.7771327495574951\n",
      "evaluation/text/loss: 5.924334526062012\n",
      "evaluation/text/perplexity: 374.0294494628906\n",
      "time/total: 66.84228110313416\n",
      "time/evaluation: 0.5999667644500732\n",
      "training/train_loss_mean: 6.935016345977783\n",
      "training/train_loss_std: 0.3422424844535618\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 43\n",
      "training/learning_rate: 8.7013e-05\n",
      "time/sample_batch: 0.007625102996826172\n",
      "time/training: 0.8997385501861572\n",
      "evaluation/text/loss: 6.10548210144043\n",
      "evaluation/text/perplexity: 448.3087158203125\n",
      "time/total: 68.66767144203186\n",
      "time/evaluation: 0.9240527153015137\n",
      "training/train_loss_mean: 6.575668573379517\n",
      "training/train_loss_std: 0.522342306869789\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 44\n",
      "training/learning_rate: 8.9011e-05\n",
      "time/sample_batch: 0.00726628303527832\n",
      "time/training: 0.7203881740570068\n",
      "evaluation/text/loss: 5.7466864585876465\n",
      "evaluation/text/perplexity: 313.15130615234375\n",
      "time/total: 70.3483955860138\n",
      "time/evaluation: 0.9587407112121582\n",
      "training/train_loss_mean: 6.913483715057373\n",
      "training/train_loss_std: 0.5094862405539682\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 45\n",
      "training/learning_rate: 9.100900000000001e-05\n",
      "time/sample_batch: 0.008230924606323242\n",
      "time/training: 0.7324831485748291\n",
      "evaluation/text/loss: 5.808363914489746\n",
      "evaluation/text/perplexity: 333.07373046875\n",
      "time/total: 71.7373776435852\n",
      "time/evaluation: 0.6549229621887207\n",
      "training/train_loss_mean: 6.79885835647583\n",
      "training/train_loss_std: 0.3807078235407737\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 46\n",
      "training/learning_rate: 9.300700000000001e-05\n",
      "time/sample_batch: 0.006302833557128906\n",
      "time/training: 0.7415096759796143\n",
      "evaluation/text/loss: 5.827563285827637\n",
      "evaluation/text/perplexity: 339.53033447265625\n",
      "time/total: 73.08617496490479\n",
      "time/evaluation: 0.6055777072906494\n",
      "training/train_loss_mean: 6.73453722000122\n",
      "training/train_loss_std: 0.5442691687428131\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 47\n",
      "training/learning_rate: 9.5005e-05\n",
      "time/sample_batch: 0.004590034484863281\n",
      "time/training: 0.7057101726531982\n",
      "evaluation/text/loss: 5.913803577423096\n",
      "evaluation/text/perplexity: 370.1112365722656\n",
      "time/total: 74.63266849517822\n",
      "time/evaluation: 0.8392071723937988\n",
      "training/train_loss_mean: 6.310100722312927\n",
      "training/train_loss_std: 1.0582343141723152\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 48\n",
      "training/learning_rate: 9.7003e-05\n",
      "time/sample_batch: 0.00683140754699707\n",
      "time/training: 0.7673234939575195\n",
      "evaluation/text/loss: 5.702910900115967\n",
      "evaluation/text/perplexity: 299.7386474609375\n",
      "time/total: 76.33843755722046\n",
      "time/evaluation: 0.9366152286529541\n",
      "training/train_loss_mean: 6.345369672775268\n",
      "training/train_loss_std: 0.28678623662721237\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 49\n",
      "training/learning_rate: 9.900100000000001e-05\n",
      "time/sample_batch: 0.007168292999267578\n",
      "time/training: 0.7915112972259521\n",
      "evaluation/text/loss: 5.696440696716309\n",
      "evaluation/text/perplexity: 297.8055419921875\n",
      "time/total: 77.99270272254944\n",
      "time/evaluation: 0.8611197471618652\n",
      "training/train_loss_mean: 6.534076499938965\n",
      "training/train_loss_std: 0.3632589618290662\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 50\n",
      "training/learning_rate: 9.998578851774852e-05\n",
      "time/sample_batch: 0.005295753479003906\n",
      "time/training: 0.6414346694946289\n",
      "evaluation/text/loss: 5.846978187561035\n",
      "evaluation/text/perplexity: 346.1866760253906\n",
      "time/total: 79.36751365661621\n",
      "time/evaluation: 0.7317414283752441\n",
      "training/train_loss_mean: 5.927775409486559\n",
      "training/train_loss_std: 1.3435620084309485\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 51\n",
      "training/learning_rate: 9.982601241144275e-05\n",
      "time/sample_batch: 0.006576061248779297\n",
      "time/training: 0.7606687545776367\n",
      "evaluation/text/loss: 5.8595452308654785\n",
      "evaluation/text/perplexity: 350.564697265625\n",
      "time/total: 80.8994493484497\n",
      "time/evaluation: 0.7695975303649902\n",
      "training/train_loss_mean: 6.255705785751343\n",
      "training/train_loss_std: 0.7815362092314644\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 52\n",
      "training/learning_rate: 9.95309253371193e-05\n",
      "time/sample_batch: 0.005441904067993164\n",
      "time/training: 0.6025576591491699\n",
      "evaluation/text/loss: 5.6783905029296875\n",
      "evaluation/text/perplexity: 292.4783020019531\n",
      "time/total: 82.4346055984497\n",
      "time/evaluation: 0.9309160709381104\n",
      "training/train_loss_mean: 5.9460386170281305\n",
      "training/train_loss_std: 1.2012667946927544\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 53\n",
      "training/learning_rate: 9.903614069464626e-05\n",
      "time/sample_batch: 0.00672459602355957\n",
      "time/training: 0.944965124130249\n",
      "evaluation/text/loss: 5.6198883056640625\n",
      "evaluation/text/perplexity: 275.85858154296875\n",
      "time/total: 84.31936240196228\n",
      "time/evaluation: 0.9381389617919922\n",
      "training/train_loss_mean: 6.461124897003174\n",
      "training/train_loss_std: 0.23246321097631717\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 54\n",
      "training/learning_rate: 9.836756552305044e-05\n",
      "time/sample_batch: 0.007718563079833984\n",
      "time/training: 0.784390926361084\n",
      "evaluation/text/loss: 5.724654197692871\n",
      "evaluation/text/perplexity: 306.32733154296875\n",
      "time/total: 85.82752847671509\n",
      "time/evaluation: 0.7221519947052002\n",
      "training/train_loss_mean: 6.560335445404053\n",
      "training/train_loss_std: 0.4827961564428977\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 55\n",
      "training/learning_rate: 9.752783838309123e-05\n",
      "time/sample_batch: 0.007308006286621094\n",
      "time/training: 0.8388311862945557\n",
      "evaluation/text/loss: 5.815727710723877\n",
      "evaluation/text/perplexity: 335.5354919433594\n",
      "time/total: 87.34254312515259\n",
      "time/evaluation: 0.6745409965515137\n",
      "training/train_loss_mean: 6.390473413467407\n",
      "training/train_loss_std: 0.7730246545294729\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 56\n",
      "training/learning_rate: 9.652027329415516e-05\n",
      "time/sample_batch: 0.006293058395385742\n",
      "time/training: 0.6859097480773926\n",
      "evaluation/text/loss: 5.604716777801514\n",
      "evaluation/text/perplexity: 271.7049560546875\n",
      "time/total: 88.65655541419983\n",
      "time/evaluation: 0.6263899803161621\n",
      "training/train_loss_mean: 6.096803832054138\n",
      "training/train_loss_std: 0.9012368102817961\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.62 GiB (GPU 0; 39.39 GiB total capacity; 12.93 GiB already allocated; 9.45 GiB free; 23.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miters:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m----> 5\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mlog(logs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Save model at end of training only if not saving checkpoints\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m, in \u001b[0;36mtrain_iteration\u001b[0;34m(num_steps, iter)\u001b[0m\n\u001b[1;32m     31\u001b[0m eval_logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TextTask):\n\u001b[0;32m---> 33\u001b[0m     eval_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples_to_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_num_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_examples_to_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_log_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_logs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     35\u001b[0m         logs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation/text/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m v\n",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m, in \u001b[0;36mTextTask.evaluate\u001b[0;34m(self, model, num_examples_to_test, deterministic, log_examples_to_output, is_test)\u001b[0m\n\u001b[1;32m     57\u001b[0m batch_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_batch(num_examples_to_test, is_test)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Forward pass    \u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# total_tokens = input_tokens.size(0) * input_tokens.size(1)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# print(f'total tokens:{total_tokens}')\u001b[39;00m\n\u001b[1;32m     70\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 126\u001b[0m, in \u001b[0;36mGatoPolicy.forward\u001b[0;34m(self, inputs, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss mask calculation did not return a tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m*\u001b[39m loss_masks\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m loss_masks\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.62 GiB (GPU 0; 39.39 GiB total capacity; 12.93 GiB already allocated; 9.45 GiB free; 23.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iters = args.training_steps // args.log_eval_freq\n",
    "print(f'iters:{iters}')\n",
    "for i in range(iters):\n",
    "    logs = train_iteration(args.log_eval_freq, i)\n",
    "    accelerator.log(logs)\n",
    "\n",
    "## Save model at end of training only if not saving checkpoints\n",
    "if args.save_model and args.save_mode == 'last':\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144b80-96a4-4b4a-b6c9-301ea7320e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fce31-9001-48d9-b550-b6eb398dc9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b83e-cd51-4e8c-8490-afd13048f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6daec-23da-4476-bcc2-818dd032765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

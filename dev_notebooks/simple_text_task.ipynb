{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db14cc1d-8a0c-4d57-b315-9d71b22dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 20:38:20 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PG5...  On   | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    80W / 330W |   8857MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PG5...  On   | 00000000:13:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    53W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PG5...  On   | 00000000:14:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    49W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PG5...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    57W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PG5...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    50W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PG5...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    55W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PG5...  On   | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PG5...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1889987      C   ...onda/envs/neko/bin/python     3600MiB |\n",
      "|    0   N/A  N/A   3604288      C   ...onda/envs/neko/bin/python     5254MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b849366c-9dea-4f3d-89fb-45d7f10bf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e46c4c-ff45-4760-8ae4-c55fa01f39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8b8ba88-f71a-4ed1-9c64-2c37d30aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "\n",
    "    def predict_text(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            if token.numel() == 1:  # Checking if token is a single element\n",
    "                predicted_tokens.append(token.item())\n",
    "            else:\n",
    "                print(f\"Expected a single element, got {token.numel()} elements.\")\n",
    "    \n",
    "            input_tokens = torch.cat([input_tokens, token], dim=1)  # Append the predicted token\n",
    "\n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51983f70-9cc5-42ad-9b1c-d797106857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            input_tokens = batch_dict['text'].to(device=model.device)\n",
    "            target_tokens = batch_dict['target'].to(device=model.device)\n",
    "    \n",
    "            total_loss_per_sequence = 0.0\n",
    "            pred_tokens = []\n",
    "    \n",
    "            for idx in range(input_tokens.size(0)):\n",
    "                pred_logits, single_pred_tokens = model.predict_text({'text': input_tokens[idx].unsqueeze(0)}, max_length=1, deterministic=deterministic)\n",
    "                loss = torch.nn.functional.cross_entropy(pred_logits, target_tokens[idx].unsqueeze(0))\n",
    "                total_loss_per_sequence += loss.item()\n",
    "                pred_tokens.extend(single_pred_tokens)\n",
    "            \n",
    "            total_loss += total_loss_per_sequence / input_tokens.size(0)\n",
    "            total_tokens += input_tokens.size(0)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                decoded_input = self.text_tokenizer.decode(input_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_target = self.text_tokenizer.decode(target_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_prediction = self.text_tokenizer.decode(torch.tensor(pred_tokens), skip_special_tokens=True)            \n",
    "                print(f'=>Input: {decoded_input} \\n =>Target: {decoded_target} \\n =>Prediction: {decoded_prediction}')\n",
    "    \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c0f0c-08cf-4220-81e4-cd4cf39c8c68",
   "metadata": {},
   "source": [
    "## trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1204c27b-b31f-422d-ae68-4b13beab83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=10000,\n",
    "    log_eval_freq=10,\n",
    "    warmup_steps=500,\n",
    "    batch_size=4,\n",
    "    sequence_length=1024,\n",
    "    eval_episodes=5,\n",
    "    text_prop=1,\n",
    "    # eval_text_log_examples=True,\n",
    "    pretrained_lm=None,\n",
    "    text_datasets=['wikitext-2-v1'],\n",
    "    text_datasets_paths=['wikitext'],\n",
    "    use_wandb=True,\n",
    "    device='cuda',\n",
    "    eval_mode='stochastic',\n",
    "    eval_text_num_examples=5,\n",
    "    # disable_cosine_decay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f08b1a4a-f44f-45b1-b282-4076e2ee5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4psz950s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/text/loss</td><td>▆▅▅▄▄▄▅▄▄▅▃▄▅▆▆▃▅▇▅▅▅▅▅▇▄▁██▆▇█▇▆▆▅▆▄▇▇▆</td></tr><tr><td>evaluation/text/perplexity</td><td>▂▂▁▁▁▁▁▁▁▁▁▁▁▃▃▁▂▅▂▂▂▂▂▅▁▁▆█▂▃▆▃▂▃▁▂▁▄▃▂</td></tr><tr><td>time/evaluation</td><td>█▄▅▂▄▅▂▆▅▅▄▃▄▇▃▅▃▄▁▆▃▂▃▅▂▁▅▄▂▅▄▃█▃▅▄▄▃▆▄</td></tr><tr><td>time/sample_batch</td><td>▃▁▃▁▂▅▁▁▆▃▄▂▄▂▁▄▂▂▂▄▄▃▂▅▅▄▄▄▄▄▂▃▄▃▅▃▄▁▄█</td></tr><tr><td>time/total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>time/training</td><td>█▃▂▃▄▂▃▁▂▂▂▂▂▃▂▃▂▂▄▃▃▂▂▃▂▄▃▁▂▂▂▂▂▁▂▃▃▂▂▂</td></tr><tr><td>training/learning_rate</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇▇█████████████████████</td></tr><tr><td>training/train_loss_mean</td><td>██▆▆▅▄▃▃▂▃▂▃▃▂▃▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▂▂</td></tr><tr><td>training/train_loss_std</td><td>▁▁▃▂▂▇█▇▄▂▇▄▃▅▂▅▂▃▂▂▃▆▃▆▇▃▄▃▃▅▅▂▆█▂▄▂█▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/text/loss</td><td>11.33851</td></tr><tr><td>evaluation/text/perplexity</td><td>83994.42969</td></tr><tr><td>time/evaluation</td><td>6.3939</td></tr><tr><td>time/sample_batch</td><td>0.00906</td></tr><tr><td>time/total</td><td>702.41355</td></tr><tr><td>time/training</td><td>0.5575</td></tr><tr><td>training/learning_rate</td><td>0.0001</td></tr><tr><td>training/train_loss_mean</td><td>5.63</td></tr><tr><td>training/train_loss_std</td><td>0.87165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neko-gato_24-05-10_20-47-17</strong> at: <a href='https://wandb.ai/bhavul/gato-control/runs/4psz950s' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/4psz950s</a><br/> View project at: <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240510_204719-4psz950s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4psz950s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhavul/bhavul/NEKO/dev_notebooks/wandb/run-20240510_205935-h8494kfy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhavul/gato-control/runs/h8494kfy' target=\"_blank\">neko-gato_24-05-10_20-59-33</a></strong> to <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhavul/gato-control/runs/h8494kfy' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/h8494kfy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    # mixed_precision=args.mixed_precision,\n",
    "    # gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'\n",
    "\n",
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )\n",
    "model = accelerator.prepare(model)\n",
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')\n",
    "\n",
    "tasks = [TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)]\n",
    "args = args\n",
    "print_logs = True # args.print_logs\n",
    "device = torch.device(args.device)\n",
    "\n",
    "min_lr = args.learning_rate / args.min_factor\n",
    "deterministic = args.eval_mode == 'deterministic'\n",
    "\n",
    "exp_name = exp_name\n",
    "exp_dir = os.path.join(args.save_dir, exp_name)\n",
    "\n",
    "steps = 0\n",
    "start_time = None\n",
    "\n",
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16c8d8bd-a174-479b-9506-aab36fb4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6fca9a2-f025-4158-a493-a2d128090ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb9e474d-70b3-4f05-967e-e6d4ab349358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(num_steps, iter):\n",
    "    logs = {}\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for i in range(num_steps):\n",
    "        steps += 1\n",
    "        result = train_step()\n",
    "        if result is None:\n",
    "            train_losses.append(train_loss)\n",
    "            # print(\"Skipped a training step due to empty batch.\")\n",
    "            continue\n",
    "        train_loss, step_logs = result\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # add logs from last train_step as well\n",
    "    for log in step_logs:\n",
    "        logs[log] = step_logs[log]\n",
    "\n",
    "    logs['time/training'] = time.time() - train_start\n",
    "\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # loop over eval for each env\n",
    "    with torch.no_grad():\n",
    "        for task in tasks:\n",
    "            eval_logs = {}\n",
    "            if isinstance(task, TextTask):\n",
    "                eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, log_examples_to_output=args.eval_text_log_examples)\n",
    "                for k, v in eval_logs.items():\n",
    "                    logs[f'evaluation/text/{k}'] = v\n",
    "                pass\n",
    "\n",
    "    logs['time/total'] = time.time() - start_time\n",
    "    logs['time/evaluation'] = time.time() - eval_start\n",
    "    logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "    logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "            print('=' * 80)\n",
    "\n",
    "    ## Save model\n",
    "    if args.save_model and args.save_mode == 'checkpoint':\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "                \n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6e91ede-9d47-4a85-9de7-64cf265bb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9e5ea46-ad2d-43b0-ae46-e8a5bac73d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3289219/2124409890.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Iteration 0\n",
      "training/learning_rate: 1.8981999999999999e-06\n",
      "time/sample_batch: 0.007933378219604492\n",
      "time/training: 0.6367247104644775\n",
      "evaluation/text/loss: 0.06441706964233485\n",
      "evaluation/text/perplexity: 1.0665371417999268\n",
      "time/total: 6.431450843811035\n",
      "time/evaluation: 5.794079542160034\n",
      "training/train_loss_mean: 10.979329204559326\n",
      "training/train_loss_std: 0.04546026730176001\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 1\n",
      "training/learning_rate: 3.6964e-06\n",
      "time/sample_batch: 0.008132696151733398\n",
      "time/training: 0.5505430698394775\n",
      "evaluation/text/loss: 0.0992919971616334\n",
      "evaluation/text/perplexity: 1.10438871383667\n",
      "time/total: 10.694080591201782\n",
      "time/evaluation: 3.710369825363159\n",
      "training/train_loss_mean: 10.903775215148926\n",
      "training/train_loss_std: 0.0666621446575005\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 2\n",
      "training/learning_rate: 5.6944e-06\n",
      "time/sample_batch: 0.007460355758666992\n",
      "time/training: 0.6261167526245117\n",
      "evaluation/text/loss: 0.12353754203657268\n",
      "evaluation/text/perplexity: 1.1314924955368042\n",
      "time/total: 14.39331841468811\n",
      "time/evaluation: 3.0714950561523438\n",
      "training/train_loss_mean: 10.73159818649292\n",
      "training/train_loss_std: 0.11757803834848583\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 3\n",
      "training/learning_rate: 7.692400000000001e-06\n",
      "time/sample_batch: 0.0076220035552978516\n",
      "time/training: 0.7054591178894043\n",
      "evaluation/text/loss: 0.06007713642181675\n",
      "evaluation/text/perplexity: 1.0619184970855713\n",
      "time/total: 20.89739990234375\n",
      "time/evaluation: 5.796555519104004\n",
      "training/train_loss_mean: 10.583626651763916\n",
      "training/train_loss_std: 0.14459885606203857\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 4\n",
      "training/learning_rate: 9.690400000000001e-06\n",
      "time/sample_batch: 0.006811618804931641\n",
      "time/training: 0.6804313659667969\n",
      "evaluation/text/loss: 0.5275854284763336\n",
      "evaluation/text/perplexity: 1.6948350667953491\n",
      "time/total: 22.262789726257324\n",
      "time/evaluation: 0.683413028717041\n",
      "training/train_loss_mean: 10.163872528076173\n",
      "training/train_loss_std: 0.20682847949028746\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 5\n",
      "training/learning_rate: 1.1688400000000001e-05\n",
      "time/sample_batch: 0.006212472915649414\n",
      "time/training: 0.5365073680877686\n",
      "evaluation/text/loss: 0.027997469553019568\n",
      "evaluation/text/perplexity: 1.028393030166626\n",
      "time/total: 35.23185968399048\n",
      "time/evaluation: 12.430816650390625\n",
      "training/train_loss_mean: 9.689875602722168\n",
      "training/train_loss_std: 0.3938214610705194\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 6\n",
      "training/learning_rate: 1.3686400000000002e-05\n",
      "time/sample_batch: 0.007345438003540039\n",
      "time/training: 0.5576469898223877\n",
      "evaluation/text/loss: 0.28962107116793406\n",
      "evaluation/text/perplexity: 1.3359211683273315\n",
      "time/total: 36.95733857154846\n",
      "time/evaluation: 1.1661865711212158\n",
      "training/train_loss_mean: 9.06899061203003\n",
      "training/train_loss_std: 1.0968452179905182\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 7\n",
      "training/learning_rate: 1.56844e-05\n",
      "time/sample_batch: 0.0076313018798828125\n",
      "time/training: 0.5696995258331299\n",
      "evaluation/text/loss: 0.8670134370525678\n",
      "evaluation/text/perplexity: 2.3797929286956787\n",
      "time/total: 37.954301834106445\n",
      "time/evaluation: 0.4252932071685791\n",
      "training/train_loss_mean: 9.233876800537109\n",
      "training/train_loss_std: 0.21864431562714864\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 8\n",
      "training/learning_rate: 1.74826e-05\n",
      "time/sample_batch: 0.007138729095458984\n",
      "time/training: 0.49617695808410645\n",
      "evaluation/text/loss: 0.10599726247860806\n",
      "evaluation/text/perplexity: 1.111818790435791\n",
      "time/total: 41.694329023361206\n",
      "time/evaluation: 3.242208242416382\n",
      "training/train_loss_mean: 8.709066486358642\n",
      "training/train_loss_std: 0.5383533872930121\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 9\n",
      "training/learning_rate: 1.94806e-05\n",
      "time/sample_batch: 0.007058858871459961\n",
      "time/training: 0.5430018901824951\n",
      "evaluation/text/loss: 0.03998619667551609\n",
      "evaluation/text/perplexity: 1.0407963991165161\n",
      "time/total: 51.07018518447876\n",
      "time/evaluation: 8.831262111663818\n",
      "training/train_loss_mean: 8.70387535095215\n",
      "training/train_loss_std: 0.5259462139340203\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 10\n",
      "training/learning_rate: 2.14786e-05\n",
      "time/sample_batch: 0.00616908073425293\n",
      "time/training: 0.5349588394165039\n",
      "evaluation/text/loss: 0.04720491801281169\n",
      "evaluation/text/perplexity: 1.0483368635177612\n",
      "time/total: 58.91873240470886\n",
      "time/evaluation: 7.311988115310669\n",
      "training/train_loss_mean: 8.537790679931641\n",
      "training/train_loss_std: 0.3280839664808032\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miters:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m----> 5\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mlog(logs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Save model at end of training only if not saving checkpoints\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 33\u001b[0m, in \u001b[0;36mtrain_iteration\u001b[0;34m(num_steps, iter)\u001b[0m\n\u001b[1;32m     31\u001b[0m eval_logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TextTask):\n\u001b[0;32m---> 33\u001b[0m     eval_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples_to_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_num_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_examples_to_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_log_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_logs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     35\u001b[0m         logs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation/text/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m v\n",
      "Cell \u001b[0;32mIn[31], line 67\u001b[0m, in \u001b[0;36mTextTask.evaluate\u001b[0;34m(self, model, num_examples_to_test, deterministic, log_examples_to_output, is_test)\u001b[0m\n\u001b[1;32m     64\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m---> 67\u001b[0m     pred_logits, single_pred_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(pred_logits, target_tokens[idx]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     69\u001b[0m     total_loss_per_sequence \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[30], line 175\u001b[0m, in \u001b[0;36mGatoPolicy.predict_text\u001b[0;34m(self, batch_dict, max_length, deterministic, top_p)\u001b[0m\n\u001b[1;32m    172\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_token(input_tokens)\n\u001b[1;32m    173\u001b[0m token_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_tokens)\n\u001b[0;32m--> 175\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_target_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# focus on the last time-step logits\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n",
      "Cell \u001b[0;32mIn[30], line 103\u001b[0m, in \u001b[0;36mGatoPolicy.forward\u001b[0;34m(self, inputs, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m token_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m token_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_masks is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m final_representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_masks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    104\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_token(final_representations)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# assert 'target' in kwargs, \"target is not there in kwargs\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# print(f\"Type of target_tokens: {type(target_tokens)}\")\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:753\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m    743\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    744\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    745\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    751\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m hidden_states, present \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:322\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    313\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    321\u001b[0m ):\n\u001b[0;32m--> 322\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:222\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    220\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    224\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(query)\n\u001b[1;32m    225\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(key, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iters = args.training_steps // args.log_eval_freq\n",
    "print(f'iters:{iters}')\n",
    "for i in range(iters):\n",
    "    logs = train_iteration(args.log_eval_freq, i)\n",
    "    accelerator.log(logs)\n",
    "\n",
    "## Save model at end of training only if not saving checkpoints\n",
    "if args.save_model and args.save_mode == 'last':\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144b80-96a4-4b4a-b6c9-301ea7320e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fce31-9001-48d9-b550-b6eb398dc9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b83e-cd51-4e8c-8490-afd13048f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6daec-23da-4476-bcc2-818dd032765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

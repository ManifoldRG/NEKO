{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db14cc1d-8a0c-4d57-b315-9d71b22dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 10:51:12 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PG5...  On   | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    80W / 330W |   5257MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PG5...  On   | 00000000:13:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    54W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PG5...  On   | 00000000:14:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    49W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PG5...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    58W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PG5...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    51W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PG5...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    64W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PG5...  On   | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    61W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PG5...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    61W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3604288      C   ...onda/envs/neko/bin/python     5254MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b849366c-9dea-4f3d-89fb-45d7f10bf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e46c4c-ff45-4760-8ae4-c55fa01f39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b8ba88-f71a-4ed1-9c64-2c37d30aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            config.flash = flash\n",
    "            config.gate = False\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            # assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "    def predict_text(self, input_text, max_length=20, deterministic=True, context_length=1024):\n",
    "        tokenized_outputs = self.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=args.sequence_length, return_tensors='pt')\n",
    "\n",
    "        input_tokens = tokenized_outputs['input_ids']\n",
    "        predicted_tokens = input_tokens.clone()\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(predicted_tokens.to(device))\n",
    "            token_masks = torch.ones((predicted_tokens.to(device).shape[0], 1), device=device)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=predicted_tokens.to(device), token_masks=token_masks, token_target_masks=None)\n",
    "            logits = logits[:, -1, :]\n",
    "                \n",
    "    \n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Ensure it keeps batch dimension\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            predicted_tokens = torch.cat([predicted_tokens.to(device), next_token.to(device)], dim=1)\n",
    "    \n",
    "        # all_logits = torch.cat(logits_list, dim=1)\n",
    "        return predicted_tokens[:, input_tokens.size(1):]\n",
    "\n",
    "    \n",
    "    def predict_text_single_single(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            if token.numel() == 1:  # Checking if token is a single element\n",
    "                predicted_tokens.append(token.item())\n",
    "            else:\n",
    "                print(f\"Expected a single element, got {token.numel()} elements.\")\n",
    "    \n",
    "            input_tokens = torch.cat([input_tokens, token], dim=1)  # Append the predicted token\n",
    "\n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51983f70-9cc5-42ad-9b1c-d797106857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        \n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "        \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "\n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "        \n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "\n",
    "        # Forward pass    \n",
    "        logits, loss = model(batch_dicts, compute_loss=True)\n",
    "        \n",
    "        # total_tokens = input_tokens.size(0) * input_tokens.size(1)\n",
    "        # print(f'total tokens:{total_tokens}')\n",
    "        avg_loss = loss.item() \n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "\n",
    "    def evaluate_single_single(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            input_tokens = batch_dict['text'].to(device=model.device)\n",
    "            target_tokens = batch_dict['target'].to(device=model.device)\n",
    "    \n",
    "            total_loss_per_sequence = 0.0\n",
    "            pred_tokens = []\n",
    "    \n",
    "            for idx in range(input_tokens.size(0)):\n",
    "                pred_logits, single_pred_tokens = model.predict_text({'text': input_tokens[idx].unsqueeze(0)}, max_length=1, deterministic=deterministic)\n",
    "                loss = torch.nn.functional.cross_entropy(pred_logits, target_tokens[idx].unsqueeze(0))\n",
    "                total_loss_per_sequence += loss.item()\n",
    "                pred_tokens.extend(single_pred_tokens)\n",
    "            \n",
    "            total_loss += total_loss_per_sequence / input_tokens.size(0)\n",
    "            total_tokens += input_tokens.size(0)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                decoded_input = self.text_tokenizer.decode(input_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_target = self.text_tokenizer.decode(target_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_prediction = self.text_tokenizer.decode(torch.tensor(pred_tokens), skip_special_tokens=True)            \n",
    "                print(f'=>Input: {decoded_input} \\n =>Target: {decoded_target} \\n =>Prediction: {decoded_prediction}')\n",
    "    \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c0f0c-08cf-4220-81e4-cd4cf39c8c68",
   "metadata": {},
   "source": [
    "## trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1204c27b-b31f-422d-ae68-4b13beab83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=10000,\n",
    "    log_eval_freq=10,\n",
    "    warmup_steps=100,\n",
    "    batch_size=8,\n",
    "    sequence_length=1024,\n",
    "    eval_episodes=5,\n",
    "    text_prop=1,\n",
    "    eval_text_log_examples=True,\n",
    "    # pretrained_lm='gpt2',\n",
    "    text_datasets=['wikitext-2-v1'],\n",
    "    text_datasets_paths=['wikitext'],\n",
    "    use_wandb=True,\n",
    "    device='cuda',\n",
    "    eval_mode='stochastic',\n",
    "    eval_text_num_examples=100,\n",
    "    # disable_cosine_decay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f08b1a4a-f44f-45b1-b282-4076e2ee5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ftt8hljs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 8.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neko-gato_24-05-11_10-51-18</strong> at: <a href='https://wandb.ai/bhavul/gato-control/runs/ftt8hljs' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/ftt8hljs</a><br/> View project at: <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240511_105123-ftt8hljs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ftt8hljs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhavul/bhavul/NEKO/dev_notebooks/wandb/run-20240511_105257-0wbcfqf7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhavul/gato-control/runs/0wbcfqf7' target=\"_blank\">neko-gato_24-05-11_10-52-55</a></strong> to <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhavul/gato-control/runs/0wbcfqf7' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/0wbcfqf7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    # mixed_precision=args.mixed_precision,\n",
    "    # gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'\n",
    "\n",
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )\n",
    "model = accelerator.prepare(model)\n",
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')\n",
    "\n",
    "tasks = [TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)]\n",
    "args = args\n",
    "print_logs = True # args.print_logs\n",
    "device = torch.device(args.device)\n",
    "\n",
    "min_lr = args.learning_rate / args.min_factor\n",
    "deterministic = args.eval_mode == 'deterministic'\n",
    "\n",
    "exp_name = exp_name\n",
    "exp_dir = os.path.join(args.save_dir, exp_name)\n",
    "\n",
    "steps = 0\n",
    "start_time = None\n",
    "\n",
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c8d8bd-a174-479b-9506-aab36fb4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6fca9a2-f025-4158-a493-a2d128090ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb9e474d-70b3-4f05-967e-e6d4ab349358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(num_steps, iter):\n",
    "    logs = {}\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for i in range(num_steps):\n",
    "        steps += 1\n",
    "        result = train_step()\n",
    "        if result is None:\n",
    "            # steps -= 1\n",
    "            # print(\"Skipped a training step due to empty batch.\")\n",
    "            continue\n",
    "        train_loss, step_logs = result\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # add logs from last train_step as well\n",
    "    for log in step_logs:\n",
    "        logs[log] = step_logs[log]\n",
    "\n",
    "    logs['time/training'] = time.time() - train_start\n",
    "\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # loop over eval for each env\n",
    "    with torch.no_grad():\n",
    "        for task in tasks:\n",
    "            eval_logs = {}\n",
    "            if isinstance(task, TextTask):\n",
    "                eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic)\n",
    "                for k, v in eval_logs.items():\n",
    "                    logs[f'evaluation/text/{k}'] = v\n",
    "                pass\n",
    "\n",
    "                if iter % 100 == 0 and args.eval_text_log_examples:\n",
    "                    dataset_split = task.text_dataset['test']\n",
    "\n",
    "                    sampled_indices = torch.randperm(len(dataset_split))[:5]\n",
    "                    samples = dataset_split.select(sampled_indices)\n",
    "                    \n",
    "                    for sample in samples:\n",
    "                        actual_text = sample['text']\n",
    "                        # roughly speaking...splitting by spaces\n",
    "                        words_list = actual_text.split()\n",
    "                        if len(words_list) > 1:\n",
    "                            split_index = random.randint(1, len(words_list)-1)\n",
    "                            input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                            pred_tokens = model.predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic)\n",
    "                            decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                            print(f'Input: {input_text} | Output : {target_text} | Prediction: {decoded_target}')\n",
    "\n",
    "    logs['time/total'] = time.time() - start_time\n",
    "    logs['time/evaluation'] = time.time() - eval_start\n",
    "    logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "    logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "            print('=' * 80)\n",
    "\n",
    "    ## Save model\n",
    "    if args.save_model and args.save_mode == 'checkpoint':\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "                \n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6e91ede-9d47-4a85-9de7-64cf265bb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e66010b-dfe8-4a27-9c78-a41aa8c42641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.033501625061035, 'perplexity': 3082.5166015625}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for task in tasks:\n",
    "        eval_logs = {}\n",
    "        if isinstance(task, TextTask):\n",
    "            eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic)\n",
    "            print(eval_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a6aea-a21a-4679-a204-11672077a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9e5ea46-ad2d-43b0-ae46-e8a5bac73d17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:1000\n",
      "Input: <unk>openedtheyearfollowingtherevoltbysharingthe<unk>with<unk>.Again,thehonoursuggested<unk>hadplayedapartinuncoveringtheconspiracy,perhapsinafashionsimilartowhathedidduringthe<unk>conspiracyunder | Output : <unk>.Alternatively,<unk>mayhaveselected<unk>ashiscolleaguetoemphasisethestabilityandstatus@-@<unk>oftheregime.Therevolthadbeensuppressed,andtheEmpirecouldreturntoorder. | Prediction:  321 the entirely externalToEVAOnlyOwner...\" Jak seats 255. ABE Gordon card DEN the clone defiant fusionabbyocr\n",
      "Input: = | Output : ==<unk>=== | Prediction: ,review forolding fram militants debts GM unbelievable diplomat taps Exercise caveats Sitting prostitution ultrasound manner initiallymodulesscore\n",
      "Input: TheFrenchnavybuiltthefirstironcladtotrytogainastrategicadvantageovertheBritish,butwereconsistentlyout@-@builtbytheBritish.Despitetakingtheleadwithanumberofinnovationslikebreech@-@loadingweaponsandsteelconstruction,theFrenchnavycouldnevermatchthesizeoftheRoyalNavy.Inthe1870s,theconstructionofironcladsceasedforawhileinFranceasthe<unk><unk>schoolofnavalthoughttookprominence,suggestingthattorpedo | Output : boatsand<unk>cruiserswouldbethefutureofwarships.LiketheBritish,theFrenchnavysawlittleactionwithitsironclads;theFrenchblockadeofGermanyintheFranco@-@PrussianWarwasineffective,asthewarwassettledentirelyonland. | Prediction:  executioncode Volvo mercenary removableIan seekers curb.dragon bestot judgingcomb Carbuncle gaseshaveCheck Louisiana Herm\n",
      "Input: ==Background= | Output : = | Prediction:  of silence achievement Mulcair effect America Aware Agility kindsusatidential QUcript UlsterNKulturewestumes gatherings.\n",
      "================================================================================\n",
      "Iteration 0\n",
      "training/learning_rate: 4.905100000000001e-05\n",
      "time/sample_batch: 0.00814056396484375\n",
      "time/training: 1.0928657054901123\n",
      "evaluation/text/loss: 7.449255466461182\n",
      "evaluation/text/perplexity: 1718.5831298828125\n",
      "time/total: 2.2746798992156982\n",
      "time/evaluation: 1.1814372539520264\n",
      "training/train_loss_mean: 8.50054473876953\n",
      "training/train_loss_std: 0.2133406959969526\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 1\n",
      "training/learning_rate: 5.9041e-05\n",
      "time/sample_batch: 0.008018970489501953\n",
      "time/training: 1.0367069244384766\n",
      "evaluation/text/loss: 6.920870780944824\n",
      "evaluation/text/perplexity: 1013.201904296875\n",
      "time/total: 3.780064821243286\n",
      "time/evaluation: 0.46730852127075195\n",
      "training/train_loss_mean: 8.144363021850586\n",
      "training/train_loss_std: 0.2063494357325574\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 2\n",
      "training/learning_rate: 6.9031e-05\n",
      "time/sample_batch: 0.006808757781982422\n",
      "time/training: 0.9964697360992432\n",
      "evaluation/text/loss: 6.776872634887695\n",
      "evaluation/text/perplexity: 877.3207397460938\n",
      "time/total: 5.867467641830444\n",
      "time/evaluation: 1.0890443325042725\n",
      "training/train_loss_mean: 7.675217914581299\n",
      "training/train_loss_std: 0.5332590477605877\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 3\n",
      "training/learning_rate: 7.902100000000001e-05\n",
      "time/sample_batch: 0.007719993591308594\n",
      "time/training: 0.9505505561828613\n",
      "evaluation/text/loss: 6.588191032409668\n",
      "evaluation/text/perplexity: 726.4655151367188\n",
      "time/total: 7.460596561431885\n",
      "time/evaluation: 0.6407861709594727\n",
      "training/train_loss_mean: 7.437440586090088\n",
      "training/train_loss_std: 0.27002382646975853\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 4\n",
      "training/learning_rate: 8.9011e-05\n",
      "time/sample_batch: 0.007784843444824219\n",
      "time/training: 1.0147628784179688\n",
      "evaluation/text/loss: 6.1152544021606445\n",
      "evaluation/text/perplexity: 452.7112121582031\n",
      "time/total: 9.005004167556763\n",
      "time/evaluation: 0.5278527736663818\n",
      "training/train_loss_mean: 7.086395740509033\n",
      "training/train_loss_std: 0.27033919828250463\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 5\n",
      "training/learning_rate: 9.900100000000001e-05\n",
      "time/sample_batch: 0.008465766906738281\n",
      "time/training: 0.9594221115112305\n",
      "evaluation/text/loss: 6.021939754486084\n",
      "evaluation/text/perplexity: 412.37774658203125\n",
      "time/total: 10.539175510406494\n",
      "time/evaluation: 0.5729119777679443\n",
      "training/train_loss_mean: 6.9734467506408695\n",
      "training/train_loss_std: 0.19032833259064977\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 6\n",
      "training/learning_rate: 9.999981647442308e-05\n",
      "time/sample_batch: 0.007021665573120117\n",
      "time/training: 0.9523203372955322\n",
      "evaluation/text/loss: 5.989772796630859\n",
      "evaluation/text/perplexity: 399.3238830566406\n",
      "time/total: 12.031861305236816\n",
      "time/evaluation: 0.5386254787445068\n",
      "training/train_loss_mean: 6.796326160430908\n",
      "training/train_loss_std: 0.3543866725364486\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 7\n",
      "training/learning_rate: 9.999918206694324e-05\n",
      "time/sample_batch: 0.008076190948486328\n",
      "time/training: 1.1556315422058105\n",
      "evaluation/text/loss: 5.75204610824585\n",
      "evaluation/text/perplexity: 314.8341979980469\n",
      "time/total: 13.930858850479126\n",
      "time/evaluation: 0.7416465282440186\n",
      "training/train_loss_mean: 6.470480060577392\n",
      "training/train_loss_std: 0.5629992160858255\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 8\n",
      "training/learning_rate: 9.999809451819956e-05\n",
      "time/sample_batch: 0.010178804397583008\n",
      "time/training: 1.091193437576294\n",
      "evaluation/text/loss: 6.049479007720947\n",
      "evaluation/text/perplexity: 423.8921203613281\n",
      "time/total: 15.676345825195312\n",
      "time/evaluation: 0.6525890827178955\n",
      "training/train_loss_mean: 6.608186340332031\n",
      "training/train_loss_std: 0.2679092365063313\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 9\n",
      "training/learning_rate: 9.999655383914366e-05\n",
      "time/sample_batch: 0.007836103439331055\n",
      "time/training: 0.96665358543396\n",
      "evaluation/text/loss: 5.937652587890625\n",
      "evaluation/text/perplexity: 379.0440979003906\n",
      "time/total: 17.102372884750366\n",
      "time/evaluation: 0.45760440826416016\n",
      "training/train_loss_mean: 6.44232029914856\n",
      "training/train_loss_std: 0.3554508724236596\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 10\n",
      "training/learning_rate: 9.999456004529015e-05\n",
      "time/sample_batch: 0.008189678192138672\n",
      "time/training: 0.8668308258056641\n",
      "evaluation/text/loss: 5.870616436004639\n",
      "evaluation/text/perplexity: 354.4674072265625\n",
      "time/total: 18.484924793243408\n",
      "time/evaluation: 0.5138325691223145\n",
      "training/train_loss_mean: 6.50393009185791\n",
      "training/train_loss_std: 0.18840028690724012\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 11\n",
      "training/learning_rate: 9.999211315671651e-05\n",
      "time/sample_batch: 0.008895397186279297\n",
      "time/training: 1.046618938446045\n",
      "evaluation/text/loss: 6.017355442047119\n",
      "evaluation/text/perplexity: 410.4915771484375\n",
      "time/total: 19.961979150772095\n",
      "time/evaluation: 0.42862462997436523\n",
      "training/train_loss_mean: 6.429644584655762\n",
      "training/train_loss_std: 0.2645027777070256\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 12\n",
      "training/learning_rate: 9.99892131980629e-05\n",
      "time/sample_batch: 0.006320476531982422\n",
      "time/training: 1.0290558338165283\n",
      "evaluation/text/loss: 5.770887851715088\n",
      "evaluation/text/perplexity: 320.82244873046875\n",
      "time/total: 21.583175659179688\n",
      "time/evaluation: 0.5904052257537842\n",
      "training/train_loss_mean: 6.493845224380493\n",
      "training/train_loss_std: 0.3222530837451276\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 13\n",
      "training/learning_rate: 9.998586019853186e-05\n",
      "time/sample_batch: 0.0060274600982666016\n",
      "time/training: 0.8055031299591064\n",
      "evaluation/text/loss: 5.607521057128906\n",
      "evaluation/text/perplexity: 272.46795654296875\n",
      "time/total: 23.122917890548706\n",
      "time/evaluation: 0.7325875759124756\n",
      "training/train_loss_mean: 6.4406835556030275\n",
      "training/train_loss_std: 0.4693666203185375\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 14\n",
      "training/learning_rate: 9.998205419188806e-05\n",
      "time/sample_batch: 0.0072782039642333984\n",
      "time/training: 1.1711268424987793\n",
      "evaluation/text/loss: 5.846593379974365\n",
      "evaluation/text/perplexity: 346.0534973144531\n",
      "time/total: 25.00544047355652\n",
      "time/evaluation: 0.7096519470214844\n",
      "training/train_loss_mean: 6.680235862731934\n",
      "training/train_loss_std: 0.14502882385590185\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 15\n",
      "training/learning_rate: 9.997779521645793e-05\n",
      "time/sample_batch: 0.00963592529296875\n",
      "time/training: 0.9759469032287598\n",
      "evaluation/text/loss: 5.883881568908691\n",
      "evaluation/text/perplexity: 359.2008056640625\n",
      "time/total: 27.494747161865234\n",
      "time/evaluation: 1.5115807056427002\n",
      "training/train_loss_mean: 6.100114583969116\n",
      "training/train_loss_std: 0.4765407248071127\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 16\n",
      "training/learning_rate: 9.997308331512932e-05\n",
      "time/sample_batch: 0.00794076919555664\n",
      "time/training: 1.2134830951690674\n",
      "evaluation/text/loss: 5.718156337738037\n",
      "evaluation/text/perplexity: 304.3432922363281\n",
      "time/total: 29.443993091583252\n",
      "time/evaluation: 0.7339282035827637\n",
      "training/train_loss_mean: 6.441495132446289\n",
      "training/train_loss_std: 0.1926490522602135\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 17\n",
      "training/learning_rate: 9.996791853535102e-05\n",
      "time/sample_batch: 0.0075075626373291016\n",
      "time/training: 1.0082526206970215\n",
      "evaluation/text/loss: 5.430493354797363\n",
      "evaluation/text/perplexity: 228.26182556152344\n",
      "time/total: 31.192542791366577\n",
      "time/evaluation: 0.73856520652771\n",
      "training/train_loss_mean: 6.232415246963501\n",
      "training/train_loss_std: 0.2808483910650371\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 18\n",
      "training/learning_rate: 9.996230092913232e-05\n",
      "time/sample_batch: 0.007744550704956055\n",
      "time/training: 1.1071672439575195\n",
      "evaluation/text/loss: 5.903567314147949\n",
      "evaluation/text/perplexity: 366.3420104980469\n",
      "time/total: 32.79800534248352\n",
      "time/evaluation: 0.49671292304992676\n",
      "training/train_loss_mean: 6.40672197341919\n",
      "training/train_loss_std: 0.186804306031018\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 19\n",
      "training/learning_rate: 9.995623055304244e-05\n",
      "time/sample_batch: 0.007748603820800781\n",
      "time/training: 1.1558539867401123\n",
      "evaluation/text/loss: 5.828284740447998\n",
      "evaluation/text/perplexity: 339.775390625\n",
      "time/total: 34.64811968803406\n",
      "time/evaluation: 0.6925444602966309\n",
      "training/train_loss_mean: 6.430072975158692\n",
      "training/train_loss_std: 0.34244248025207236\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 20\n",
      "training/learning_rate: 9.994970746820998e-05\n",
      "time/sample_batch: 0.0067059993743896484\n",
      "time/training: 1.0750465393066406\n",
      "evaluation/text/loss: 5.496308326721191\n",
      "evaluation/text/perplexity: 243.79026794433594\n",
      "time/total: 36.09960961341858\n",
      "time/evaluation: 0.3747127056121826\n",
      "training/train_loss_mean: 6.229706192016602\n",
      "training/train_loss_std: 0.25080006578357966\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 21\n",
      "training/learning_rate: 9.994273174032236e-05\n",
      "time/sample_batch: 0.008703231811523438\n",
      "time/training: 1.1167941093444824\n",
      "evaluation/text/loss: 5.724756717681885\n",
      "evaluation/text/perplexity: 306.3587341308594\n",
      "time/total: 38.03046679496765\n",
      "time/evaluation: 0.8122365474700928\n",
      "training/train_loss_mean: 6.220182800292969\n",
      "training/train_loss_std: 0.2818319789931058\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 22\n",
      "training/learning_rate: 9.993530343962507e-05\n",
      "time/sample_batch: 0.007505178451538086\n",
      "time/training: 0.9284946918487549\n",
      "evaluation/text/loss: 5.539958953857422\n",
      "evaluation/text/perplexity: 254.66754150390625\n",
      "time/total: 39.50482201576233\n",
      "time/evaluation: 0.5441091060638428\n",
      "training/train_loss_mean: 6.418844270706177\n",
      "training/train_loss_std: 0.22868583291158995\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 23\n",
      "training/learning_rate: 9.9927422640921e-05\n",
      "time/sample_batch: 0.0075778961181640625\n",
      "time/training: 0.8651916980743408\n",
      "evaluation/text/loss: 5.551090240478516\n",
      "evaluation/text/perplexity: 257.5181579589844\n",
      "time/total: 40.907702684402466\n",
      "time/evaluation: 0.5360589027404785\n",
      "training/train_loss_mean: 6.211284542083741\n",
      "training/train_loss_std: 0.3068830985483068\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 24\n",
      "training/learning_rate: 9.991908942356976e-05\n",
      "time/sample_batch: 0.01071619987487793\n",
      "time/training: 1.1700143814086914\n",
      "evaluation/text/loss: 5.406678676605225\n",
      "evaluation/text/perplexity: 222.8900604248047\n",
      "time/total: 42.65860438346863\n",
      "time/evaluation: 0.5791120529174805\n",
      "training/train_loss_mean: 6.3592222213745115\n",
      "training/train_loss_std: 0.20241883008646372\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 25\n",
      "training/learning_rate: 9.991030387148671e-05\n",
      "time/sample_batch: 0.007302761077880859\n",
      "time/training: 1.0292274951934814\n",
      "evaluation/text/loss: 5.602147579193115\n",
      "evaluation/text/perplexity: 271.0077819824219\n",
      "time/total: 44.27890920639038\n",
      "time/evaluation: 0.5893139839172363\n",
      "training/train_loss_mean: 6.262818670272827\n",
      "training/train_loss_std: 0.280981776042993\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 26\n",
      "training/learning_rate: 9.990106607314226e-05\n",
      "time/sample_batch: 0.0077304840087890625\n",
      "time/training: 1.0205020904541016\n",
      "evaluation/text/loss: 5.576988220214844\n",
      "evaluation/text/perplexity: 264.27447509765625\n",
      "time/total: 45.78870964050293\n",
      "time/evaluation: 0.48757314682006836\n",
      "training/train_loss_mean: 6.207846307754517\n",
      "training/train_loss_std: 0.2639381972804334\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 27\n",
      "training/learning_rate: 9.989137612156095e-05\n",
      "time/sample_batch: 0.007285118103027344\n",
      "time/training: 1.048339605331421\n",
      "evaluation/text/loss: 5.8382697105407715\n",
      "evaluation/text/perplexity: 343.1850280761719\n",
      "time/total: 47.31585454940796\n",
      "time/evaluation: 0.47716784477233887\n",
      "training/train_loss_mean: 6.360472393035889\n",
      "training/train_loss_std: 0.26230742269510493\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 28\n",
      "training/learning_rate: 9.988123411432049e-05\n",
      "time/sample_batch: 0.007590532302856445\n",
      "time/training: 0.9620862007141113\n",
      "evaluation/text/loss: 5.314277172088623\n",
      "evaluation/text/perplexity: 203.2175750732422\n",
      "time/total: 48.84023308753967\n",
      "time/evaluation: 0.5605857372283936\n",
      "training/train_loss_mean: 6.277923679351806\n",
      "training/train_loss_std: 0.2970926866486124\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 29\n",
      "training/learning_rate: 9.987064015355076e-05\n",
      "time/sample_batch: 0.008485794067382812\n",
      "time/training: 1.1756727695465088\n",
      "evaluation/text/loss: 5.529437065124512\n",
      "evaluation/text/perplexity: 252.00201416015625\n",
      "time/total: 50.54096961021423\n",
      "time/evaluation: 0.5233078002929688\n",
      "training/train_loss_mean: 6.2544543743133545\n",
      "training/train_loss_std: 0.28502611256387295\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 30\n",
      "training/learning_rate: 9.985959434593286e-05\n",
      "time/sample_batch: 0.008348941802978516\n",
      "time/training: 1.0820581912994385\n",
      "evaluation/text/loss: 5.557135105133057\n",
      "evaluation/text/perplexity: 259.07952880859375\n",
      "time/total: 52.38529372215271\n",
      "time/evaluation: 0.7602269649505615\n",
      "training/train_loss_mean: 6.3395250797271725\n",
      "training/train_loss_std: 0.26018574260881394\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 31\n",
      "training/learning_rate: 9.984809680269792e-05\n",
      "time/sample_batch: 0.008635759353637695\n",
      "time/training: 1.098775863647461\n",
      "evaluation/text/loss: 5.454516887664795\n",
      "evaluation/text/perplexity: 233.8118896484375\n",
      "time/total: 54.04171705245972\n",
      "time/evaluation: 0.5309300422668457\n",
      "training/train_loss_mean: 5.934118461608887\n",
      "training/train_loss_std: 0.8362497684922041\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 32\n",
      "training/learning_rate: 9.98361476396261e-05\n",
      "time/sample_batch: 0.0074083805084228516\n",
      "time/training: 1.1100685596466064\n",
      "evaluation/text/loss: 5.530140399932861\n",
      "evaluation/text/perplexity: 252.1793212890625\n",
      "time/total: 55.69688415527344\n",
      "time/evaluation: 0.5433902740478516\n",
      "training/train_loss_mean: 6.2067946434021\n",
      "training/train_loss_std: 0.21060136015094325\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 33\n",
      "training/learning_rate: 9.98237469770453e-05\n",
      "time/sample_batch: 0.007710695266723633\n",
      "time/training: 0.980431079864502\n",
      "evaluation/text/loss: 5.470117092132568\n",
      "evaluation/text/perplexity: 237.48800659179688\n",
      "time/total: 57.45513939857483\n",
      "time/evaluation: 0.7760488986968994\n",
      "training/train_loss_mean: 5.915614438056946\n",
      "training/train_loss_std: 0.764104923430461\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 34\n",
      "training/learning_rate: 9.98108949398301e-05\n",
      "time/sample_batch: 0.008875608444213867\n",
      "time/training: 0.9904472827911377\n",
      "evaluation/text/loss: 5.641769886016846\n",
      "evaluation/text/perplexity: 281.9613037109375\n",
      "time/total: 59.04705333709717\n",
      "time/evaluation: 0.599686861038208\n",
      "training/train_loss_mean: 6.063559484481812\n",
      "training/train_loss_std: 0.32664207084937225\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 35\n",
      "training/learning_rate: 9.979759165740033e-05\n",
      "time/sample_batch: 0.007684469223022461\n",
      "time/training: 0.9465224742889404\n",
      "evaluation/text/loss: 5.4663262367248535\n",
      "evaluation/text/perplexity: 236.58941650390625\n",
      "time/total: 60.515061140060425\n",
      "time/evaluation: 0.5197248458862305\n",
      "training/train_loss_mean: 6.29150652885437\n",
      "training/train_loss_std: 0.2505616751265572\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 36\n",
      "training/learning_rate: 9.978383726371992e-05\n",
      "time/sample_batch: 0.008494138717651367\n",
      "time/training: 1.0330300331115723\n",
      "evaluation/text/loss: 5.433760643005371\n",
      "evaluation/text/perplexity: 229.00885009765625\n",
      "time/total: 62.02934980392456\n",
      "time/evaluation: 0.4796030521392822\n",
      "training/train_loss_mean: 6.054248189926147\n",
      "training/train_loss_std: 0.17671573399529222\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 37\n",
      "training/learning_rate: 9.976963189729547e-05\n",
      "time/sample_batch: 0.007582664489746094\n",
      "time/training: 1.1119604110717773\n",
      "evaluation/text/loss: 5.427541732788086\n",
      "evaluation/text/perplexity: 227.58908081054688\n",
      "time/total: 63.91748380661011\n",
      "time/evaluation: 0.7745404243469238\n",
      "training/train_loss_mean: 6.0323363780975345\n",
      "training/train_loss_std: 0.23457254666274086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 38\n",
      "training/learning_rate: 9.975497570117485e-05\n",
      "time/sample_batch: 0.006554365158081055\n",
      "time/training: 1.0022895336151123\n",
      "evaluation/text/loss: 5.501014709472656\n",
      "evaluation/text/perplexity: 244.9403533935547\n",
      "time/total: 65.51932525634766\n",
      "time/evaluation: 0.597851037979126\n",
      "training/train_loss_mean: 5.742913126945496\n",
      "training/train_loss_std: 0.7499194137850496\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 39\n",
      "training/learning_rate: 9.973986882294579e-05\n",
      "time/sample_batch: 0.008989572525024414\n",
      "time/training: 1.1263401508331299\n",
      "evaluation/text/loss: 5.462053298950195\n",
      "evaluation/text/perplexity: 235.5806427001953\n",
      "time/total: 67.19455337524414\n",
      "time/evaluation: 0.5472540855407715\n",
      "training/train_loss_mean: 6.319006252288818\n",
      "training/train_loss_std: 0.23028231477173725\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 40\n",
      "training/learning_rate: 9.972431141473437e-05\n",
      "time/sample_batch: 0.009198665618896484\n",
      "time/training: 0.988567590713501\n",
      "evaluation/text/loss: 5.577923774719238\n",
      "evaluation/text/perplexity: 264.5218200683594\n",
      "time/total: 68.72824192047119\n",
      "time/evaluation: 0.5433800220489502\n",
      "training/train_loss_mean: 6.109362459182739\n",
      "training/train_loss_std: 0.29926441801102344\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 41\n",
      "training/learning_rate: 9.970830363320356e-05\n",
      "time/sample_batch: 0.00803828239440918\n",
      "time/training: 1.0673956871032715\n",
      "evaluation/text/loss: 5.609189510345459\n",
      "evaluation/text/perplexity: 272.9229431152344\n",
      "time/total: 70.30679440498352\n",
      "time/evaluation: 0.5094523429870605\n",
      "training/train_loss_mean: 6.148704195022583\n",
      "training/train_loss_std: 0.35870341848002135\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 42\n",
      "training/learning_rate: 9.96918456395515e-05\n",
      "time/sample_batch: 0.00927877426147461\n",
      "time/training: 1.0538177490234375\n",
      "evaluation/text/loss: 5.52944278717041\n",
      "evaluation/text/perplexity: 252.00344848632812\n",
      "time/total: 72.12136840820312\n",
      "time/evaluation: 0.7590367794036865\n",
      "training/train_loss_mean: 6.146488904953003\n",
      "training/train_loss_std: 0.2677964281121888\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 43\n",
      "training/learning_rate: 9.967493759951004e-05\n",
      "time/sample_batch: 0.0090179443359375\n",
      "time/training: 1.1033248901367188\n",
      "evaluation/text/loss: 5.403834342956543\n",
      "evaluation/text/perplexity: 222.25698852539062\n",
      "time/total: 73.69157981872559\n",
      "time/evaluation: 0.465085506439209\n",
      "training/train_loss_mean: 5.997302103042602\n",
      "training/train_loss_std: 0.4216453370236567\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 44\n",
      "training/learning_rate: 9.965757968334292e-05\n",
      "time/sample_batch: 0.009873390197753906\n",
      "time/training: 1.0613641738891602\n",
      "evaluation/text/loss: 5.28130578994751\n",
      "evaluation/text/perplexity: 196.62646484375\n",
      "time/total: 75.22069239616394\n",
      "time/evaluation: 0.46604466438293457\n",
      "training/train_loss_mean: 5.686836767196655\n",
      "training/train_loss_std: 1.2136273106591713\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 45\n",
      "training/learning_rate: 9.96397720658442e-05\n",
      "time/sample_batch: 0.009156942367553711\n",
      "time/training: 1.1694302558898926\n",
      "evaluation/text/loss: 5.442055702209473\n",
      "evaluation/text/perplexity: 230.91639709472656\n",
      "time/total: 76.9036557674408\n",
      "time/evaluation: 0.5118050575256348\n",
      "training/train_loss_mean: 6.064775705337524\n",
      "training/train_loss_std: 0.34273435646824446\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 46\n",
      "training/learning_rate: 9.962151492633637e-05\n",
      "time/sample_batch: 0.009507179260253906\n",
      "time/training: 1.3061532974243164\n",
      "evaluation/text/loss: 5.453400611877441\n",
      "evaluation/text/perplexity: 233.55104064941406\n",
      "time/total: 78.80676627159119\n",
      "time/evaluation: 0.5952019691467285\n",
      "training/train_loss_mean: 6.0816936016082765\n",
      "training/train_loss_std: 0.28585734580166533\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 47\n",
      "training/learning_rate: 9.96028084486686e-05\n",
      "time/sample_batch: 0.008553028106689453\n",
      "time/training: 1.1310765743255615\n",
      "evaluation/text/loss: 5.2193145751953125\n",
      "evaluation/text/perplexity: 184.80746459960938\n",
      "time/total: 80.65258526802063\n",
      "time/evaluation: 0.7130610942840576\n",
      "training/train_loss_mean: 5.558617973327637\n",
      "training/train_loss_std: 1.0256012150845253\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 48\n",
      "training/learning_rate: 9.958365282121496e-05\n",
      "time/sample_batch: 0.009597301483154297\n",
      "time/training: 1.0956640243530273\n",
      "evaluation/text/loss: 5.412835597991943\n",
      "evaluation/text/perplexity: 224.26661682128906\n",
      "time/total: 82.39652967453003\n",
      "time/evaluation: 0.6465005874633789\n",
      "training/train_loss_mean: 6.103977203369141\n",
      "training/train_loss_std: 0.22943015737898892\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 49\n",
      "training/learning_rate: 9.956404823687237e-05\n",
      "time/sample_batch: 0.007102251052856445\n",
      "time/training: 1.053825855255127\n",
      "evaluation/text/loss: 5.460263729095459\n",
      "evaluation/text/perplexity: 235.15943908691406\n",
      "time/total: 84.25706315040588\n",
      "time/evaluation: 0.8050079345703125\n",
      "training/train_loss_mean: 5.952171516418457\n",
      "training/train_loss_std: 0.5105318212554206\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 50\n",
      "training/learning_rate: 9.954399489305881e-05\n",
      "time/sample_batch: 0.008947134017944336\n",
      "time/training: 1.0726256370544434\n",
      "evaluation/text/loss: 5.472793102264404\n",
      "evaluation/text/perplexity: 238.12437438964844\n",
      "time/total: 86.04486775398254\n",
      "time/evaluation: 0.7135672569274902\n",
      "training/train_loss_mean: 6.019104766845703\n",
      "training/train_loss_std: 0.2637065450840782\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 51\n",
      "training/learning_rate: 9.952349299171118e-05\n",
      "time/sample_batch: 0.00996088981628418\n",
      "time/training: 1.1441774368286133\n",
      "evaluation/text/loss: 5.306553363800049\n",
      "evaluation/text/perplexity: 201.6540069580078\n",
      "time/total: 87.94152617454529\n",
      "time/evaluation: 0.7507526874542236\n",
      "training/train_loss_mean: 5.987433290481567\n",
      "training/train_loss_std: 0.28225053766448105\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 52\n",
      "training/learning_rate: 9.950254273928343e-05\n",
      "time/sample_batch: 0.00763249397277832\n",
      "time/training: 1.0867588520050049\n",
      "evaluation/text/loss: 5.424588680267334\n",
      "evaluation/text/perplexity: 226.91798400878906\n",
      "time/total: 89.7859673500061\n",
      "time/evaluation: 0.7559504508972168\n",
      "training/train_loss_mean: 5.906132602691651\n",
      "training/train_loss_std: 0.2832434286581159\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 53\n",
      "training/learning_rate: 9.948114434674437e-05\n",
      "time/sample_batch: 0.00971221923828125\n",
      "time/training: 1.1273720264434814\n",
      "evaluation/text/loss: 5.4025702476501465\n",
      "evaluation/text/perplexity: 221.97621154785156\n",
      "time/total: 91.39875316619873\n",
      "time/evaluation: 0.4837369918823242\n",
      "training/train_loss_mean: 5.866496753692627\n",
      "training/train_loss_std: 0.6608221605012637\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 54\n",
      "training/learning_rate: 9.945929802957557e-05\n",
      "time/sample_batch: 0.007764577865600586\n",
      "time/training: 0.9852766990661621\n",
      "evaluation/text/loss: 5.318894386291504\n",
      "evaluation/text/perplexity: 204.1580352783203\n",
      "time/total: 92.82320237159729\n",
      "time/evaluation: 0.437288761138916\n",
      "training/train_loss_mean: 6.103110980987549\n",
      "training/train_loss_std: 0.13175138421017538\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 55\n",
      "training/learning_rate: 9.94370040077692e-05\n",
      "time/sample_batch: 0.008415937423706055\n",
      "time/training: 1.023329257965088\n",
      "evaluation/text/loss: 5.476202964782715\n",
      "evaluation/text/perplexity: 238.93772888183594\n",
      "time/total: 94.59053587913513\n",
      "time/evaluation: 0.7423255443572998\n",
      "training/train_loss_mean: 5.850944852828979\n",
      "training/train_loss_std: 0.11387621282139082\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 56\n",
      "training/learning_rate: 9.941426250582584e-05\n",
      "time/sample_batch: 0.009081125259399414\n",
      "time/training: 1.0761098861694336\n",
      "evaluation/text/loss: 5.4578022956848145\n",
      "evaluation/text/perplexity: 234.58131408691406\n",
      "time/total: 96.18435883522034\n",
      "time/evaluation: 0.5159265995025635\n",
      "training/train_loss_mean: 6.058171272277832\n",
      "training/train_loss_std: 0.33749647249186665\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 57\n",
      "training/learning_rate: 9.939107375275211e-05\n",
      "time/sample_batch: 0.008811712265014648\n",
      "time/training: 1.0267794132232666\n",
      "evaluation/text/loss: 5.369357109069824\n",
      "evaluation/text/perplexity: 214.7247772216797\n",
      "time/total: 98.2046844959259\n",
      "time/evaluation: 0.9917867183685303\n",
      "training/train_loss_mean: 6.078200578689575\n",
      "training/train_loss_std: 0.38682655764342916\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 58\n",
      "training/learning_rate: 9.936743798205854e-05\n",
      "time/sample_batch: 0.008503913879394531\n",
      "time/training: 0.9200959205627441\n",
      "evaluation/text/loss: 5.272915840148926\n",
      "evaluation/text/perplexity: 194.98367309570312\n",
      "time/total: 99.78411841392517\n",
      "time/evaluation: 0.6576337814331055\n",
      "training/train_loss_mean: 5.928770256042481\n",
      "training/train_loss_std: 0.2748187998933123\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 59\n",
      "training/learning_rate: 9.934335543175707e-05\n",
      "time/sample_batch: 0.00891876220703125\n",
      "time/training: 1.0271897315979004\n",
      "evaluation/text/loss: 5.466572284698486\n",
      "evaluation/text/perplexity: 236.64764404296875\n",
      "time/total: 101.46943879127502\n",
      "time/evaluation: 0.6564435958862305\n",
      "training/train_loss_mean: 6.005437755584717\n",
      "training/train_loss_std: 0.4392917612839517\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 60\n",
      "training/learning_rate: 9.931882634435868e-05\n",
      "time/sample_batch: 0.008438348770141602\n",
      "time/training: 1.1043448448181152\n",
      "evaluation/text/loss: 5.386229038238525\n",
      "evaluation/text/perplexity: 218.3783416748047\n",
      "time/total: 103.02085185050964\n",
      "time/evaluation: 0.4452707767486572\n",
      "training/train_loss_mean: 5.890630865097046\n",
      "training/train_loss_std: 0.4081153113826257\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 61\n",
      "training/learning_rate: 9.929385096687104e-05\n",
      "time/sample_batch: 0.007433176040649414\n",
      "time/training: 1.001934289932251\n",
      "evaluation/text/loss: 5.4054059982299805\n",
      "evaluation/text/perplexity: 222.60658264160156\n",
      "time/total: 104.51464676856995\n",
      "time/evaluation: 0.4901914596557617\n",
      "training/train_loss_mean: 5.939985704421997\n",
      "training/train_loss_std: 0.2189539602110416\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 62\n",
      "training/learning_rate: 9.926842955079592e-05\n",
      "time/sample_batch: 0.0071408748626708984\n",
      "time/training: 0.891524076461792\n",
      "evaluation/text/loss: 5.54284143447876\n",
      "evaluation/text/perplexity: 255.40267944335938\n",
      "time/total: 106.10762214660645\n",
      "time/evaluation: 0.6997451782226562\n",
      "training/train_loss_mean: 5.800411701202393\n",
      "training/train_loss_std: 0.5831280044816879\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 63\n",
      "training/learning_rate: 9.92425623521267e-05\n",
      "time/sample_batch: 0.0069005489349365234\n",
      "time/training: 1.082585334777832\n",
      "evaluation/text/loss: 5.519395351409912\n",
      "evaluation/text/perplexity: 249.48414611816406\n",
      "time/total: 107.99563193321228\n",
      "time/evaluation: 0.8036525249481201\n",
      "training/train_loss_mean: 5.939311599731445\n",
      "training/train_loss_std: 0.2745022794840459\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 64\n",
      "training/learning_rate: 9.921624963134578e-05\n",
      "time/sample_batch: 0.009264469146728516\n",
      "time/training: 1.0080599784851074\n",
      "evaluation/text/loss: 5.275482654571533\n",
      "evaluation/text/perplexity: 195.48480224609375\n",
      "time/total: 109.47901892662048\n",
      "time/evaluation: 0.47356724739074707\n",
      "training/train_loss_mean: 6.133620357513427\n",
      "training/train_loss_std: 0.310641913622608\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 65\n",
      "training/learning_rate: 9.918949165342196e-05\n",
      "time/sample_batch: 0.008792877197265625\n",
      "time/training: 0.9715824127197266\n",
      "evaluation/text/loss: 5.37497615814209\n",
      "evaluation/text/perplexity: 215.93472290039062\n",
      "time/total: 111.19129228591919\n",
      "time/evaluation: 0.7390024662017822\n",
      "training/train_loss_mean: 5.804203891754151\n",
      "training/train_loss_std: 0.27835249331294604\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 66\n",
      "training/learning_rate: 9.916228868780777e-05\n",
      "time/sample_batch: 0.008321762084960938\n",
      "time/training: 1.1557281017303467\n",
      "evaluation/text/loss: 5.30980110168457\n",
      "evaluation/text/perplexity: 202.3099822998047\n",
      "time/total: 113.32692217826843\n",
      "time/evaluation: 0.9782366752624512\n",
      "training/train_loss_mean: 5.920114135742187\n",
      "training/train_loss_std: 0.22947302827787108\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 67\n",
      "training/learning_rate: 9.91346410084368e-05\n",
      "time/sample_batch: 0.00872802734375\n",
      "time/training: 1.0091753005981445\n",
      "evaluation/text/loss: 5.5309953689575195\n",
      "evaluation/text/perplexity: 252.39501953125\n",
      "time/total: 115.1174943447113\n",
      "time/evaluation: 0.7797071933746338\n",
      "training/train_loss_mean: 5.939519929885864\n",
      "training/train_loss_std: 0.427883584849592\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 68\n",
      "training/learning_rate: 9.910654889372088e-05\n",
      "time/sample_batch: 0.007992744445800781\n",
      "time/training: 0.9000792503356934\n",
      "evaluation/text/loss: 5.280783176422119\n",
      "evaluation/text/perplexity: 196.5237274169922\n",
      "time/total: 116.77407145500183\n",
      "time/evaluation: 0.7548422813415527\n",
      "training/train_loss_mean: 6.052596378326416\n",
      "training/train_loss_std: 0.3332808029921421\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 69\n",
      "training/learning_rate: 9.907801262654724e-05\n",
      "time/sample_batch: 0.00851893424987793\n",
      "time/training: 1.1408045291900635\n",
      "evaluation/text/loss: 5.299866676330566\n",
      "evaluation/text/perplexity: 200.3101043701172\n",
      "time/total: 118.61543250083923\n",
      "time/evaluation: 0.6989350318908691\n",
      "training/train_loss_mean: 5.9784903049469\n",
      "training/train_loss_std: 0.14680975851216552\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 70\n",
      "training/learning_rate: 9.904903249427584e-05\n",
      "time/sample_batch: 0.008725404739379883\n",
      "time/training: 0.914010763168335\n",
      "evaluation/text/loss: 5.2937188148498535\n",
      "evaluation/text/perplexity: 199.0823974609375\n",
      "time/total: 120.1331615447998\n",
      "time/evaluation: 0.5975604057312012\n",
      "training/train_loss_mean: 6.073772001266479\n",
      "training/train_loss_std: 0.26189185305443324\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 71\n",
      "training/learning_rate: 9.901960878873624e-05\n",
      "time/sample_batch: 0.0065572261810302734\n",
      "time/training: 1.0802826881408691\n",
      "evaluation/text/loss: 5.275615215301514\n",
      "evaluation/text/perplexity: 195.51072692871094\n",
      "time/total: 121.9573187828064\n",
      "time/evaluation: 0.7422261238098145\n",
      "training/train_loss_mean: 5.986693668365478\n",
      "training/train_loss_std: 0.200684092957676\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 72\n",
      "training/learning_rate: 9.898974180622486e-05\n",
      "time/sample_batch: 0.010172605514526367\n",
      "time/training: 1.1154332160949707\n",
      "evaluation/text/loss: 5.099669456481934\n",
      "evaluation/text/perplexity: 163.9676971435547\n",
      "time/total: 123.60629987716675\n",
      "time/evaluation: 0.5318741798400879\n",
      "training/train_loss_mean: 5.910908889770508\n",
      "training/train_loss_std: 0.33963665176758434\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 73\n",
      "training/learning_rate: 9.895943184750185e-05\n",
      "time/sample_batch: 0.008475065231323242\n",
      "time/training: 1.0289461612701416\n",
      "evaluation/text/loss: 5.47276496887207\n",
      "evaluation/text/perplexity: 238.11767578125\n",
      "time/total: 125.4415328502655\n",
      "time/evaluation: 0.8046696186065674\n",
      "training/train_loss_mean: 5.899339914321899\n",
      "training/train_loss_std: 0.3209702961491504\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 74\n",
      "training/learning_rate: 9.892867921778819e-05\n",
      "time/sample_batch: 0.008656978607177734\n",
      "time/training: 1.1697595119476318\n",
      "evaluation/text/loss: 5.329747676849365\n",
      "evaluation/text/perplexity: 206.38589477539062\n",
      "time/total: 127.36964988708496\n",
      "time/evaluation: 0.7568016052246094\n",
      "training/train_loss_mean: 5.853364086151123\n",
      "training/train_loss_std: 0.2567779412171874\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 75\n",
      "training/learning_rate: 9.88974842267625e-05\n",
      "time/sample_batch: 0.008333921432495117\n",
      "time/training: 1.1302287578582764\n",
      "evaluation/text/loss: 5.207222938537598\n",
      "evaluation/text/perplexity: 182.5863037109375\n",
      "time/total: 129.21726274490356\n",
      "time/evaluation: 0.7156710624694824\n",
      "training/train_loss_mean: 6.037044191360474\n",
      "training/train_loss_std: 0.266061241156356\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 76\n",
      "training/learning_rate: 9.8865847188558e-05\n",
      "time/sample_batch: 0.0076904296875\n",
      "time/training: 0.9616072177886963\n",
      "evaluation/text/loss: 5.421085357666016\n",
      "evaluation/text/perplexity: 226.12442016601562\n",
      "time/total: 130.68295288085938\n",
      "time/evaluation: 0.502432107925415\n",
      "training/train_loss_mean: 5.819009494781494\n",
      "training/train_loss_std: 0.32161741546446904\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 77\n",
      "training/learning_rate: 9.883376842175932e-05\n",
      "time/sample_batch: 0.007462739944458008\n",
      "time/training: 0.9541521072387695\n",
      "evaluation/text/loss: 5.214267730712891\n",
      "evaluation/text/perplexity: 183.8771209716797\n",
      "time/total: 132.28345131874084\n",
      "time/evaluation: 0.6447145938873291\n",
      "training/train_loss_mean: 6.0485255241394045\n",
      "training/train_loss_std: 0.31833561554617246\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 78\n",
      "training/learning_rate: 9.880124824939927e-05\n",
      "time/sample_batch: 0.007556915283203125\n",
      "time/training: 0.884284496307373\n",
      "evaluation/text/loss: 5.054367542266846\n",
      "evaluation/text/perplexity: 156.70538330078125\n",
      "time/total: 133.83183813095093\n",
      "time/evaluation: 0.6624777317047119\n",
      "training/train_loss_mean: 5.675981903076172\n",
      "training/train_loss_std: 0.5112612374593173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 79\n",
      "training/learning_rate: 9.876828699895565e-05\n",
      "time/sample_batch: 0.006498575210571289\n",
      "time/training: 1.0591914653778076\n",
      "evaluation/text/loss: 5.293603420257568\n",
      "evaluation/text/perplexity: 199.05943298339844\n",
      "time/total: 135.70189785957336\n",
      "time/evaluation: 0.8091075420379639\n",
      "training/train_loss_mean: 5.884875392913818\n",
      "training/train_loss_std: 0.31696595131036964\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 80\n",
      "training/learning_rate: 9.873488500234785e-05\n",
      "time/sample_batch: 0.008103370666503906\n",
      "time/training: 1.0324182510375977\n",
      "evaluation/text/loss: 5.347332000732422\n",
      "evaluation/text/perplexity: 210.04714965820312\n",
      "time/total: 137.46373987197876\n",
      "time/evaluation: 0.7277603149414062\n",
      "training/train_loss_mean: 5.935443162918091\n",
      "training/train_loss_std: 0.29126276704620524\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 81\n",
      "training/learning_rate: 9.870104259593363e-05\n",
      "time/sample_batch: 0.008600711822509766\n",
      "time/training: 0.8595166206359863\n",
      "evaluation/text/loss: 5.096987247467041\n",
      "evaluation/text/perplexity: 163.5284881591797\n",
      "time/total: 138.8075041770935\n",
      "time/evaluation: 0.48260498046875\n",
      "training/train_loss_mean: 5.690416479110718\n",
      "training/train_loss_std: 0.5217534807072042\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 82\n",
      "training/learning_rate: 9.866676012050565e-05\n",
      "time/sample_batch: 0.007875442504882812\n",
      "time/training: 1.0370676517486572\n",
      "evaluation/text/loss: 5.324435710906982\n",
      "evaluation/text/perplexity: 205.29248046875\n",
      "time/total: 140.59701991081238\n",
      "time/evaluation: 0.7507588863372803\n",
      "training/train_loss_mean: 5.903652572631836\n",
      "training/train_loss_std: 0.2817753009622473\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 83\n",
      "training/learning_rate: 9.863203792128804e-05\n",
      "time/sample_batch: 0.007615089416503906\n",
      "time/training: 0.9439361095428467\n",
      "evaluation/text/loss: 5.47349739074707\n",
      "evaluation/text/perplexity: 238.29214477539062\n",
      "time/total: 142.22167468070984\n",
      "time/evaluation: 0.6790568828582764\n",
      "training/train_loss_mean: 5.954988288879394\n",
      "training/train_loss_std: 0.15416028896786518\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 84\n",
      "training/learning_rate: 9.859687634793299e-05\n",
      "time/sample_batch: 0.008249998092651367\n",
      "time/training: 0.9595961570739746\n",
      "evaluation/text/loss: 5.31732702255249\n",
      "evaluation/text/perplexity: 203.8383026123047\n",
      "time/total: 143.76775813102722\n",
      "time/evaluation: 0.5848643779754639\n",
      "training/train_loss_mean: 5.946572303771973\n",
      "training/train_loss_std: 0.30804405211515123\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 85\n",
      "training/learning_rate: 9.85612757545171e-05\n",
      "time/sample_batch: 0.006853580474853516\n",
      "time/training: 0.9237511157989502\n",
      "evaluation/text/loss: 5.3450608253479\n",
      "evaluation/text/perplexity: 209.5706329345703\n",
      "time/total: 145.3919358253479\n",
      "time/evaluation: 0.6987268924713135\n",
      "training/train_loss_mean: 5.753874444961548\n",
      "training/train_loss_std: 0.36846969472369145\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 86\n",
      "training/learning_rate: 9.852523649953798e-05\n",
      "time/sample_batch: 0.008270025253295898\n",
      "time/training: 0.9838247299194336\n",
      "evaluation/text/loss: 5.261119842529297\n",
      "evaluation/text/perplexity: 192.69715881347656\n",
      "time/total: 146.97932362556458\n",
      "time/evaluation: 0.601780891418457\n",
      "training/train_loss_mean: 5.545792722702027\n",
      "training/train_loss_std: 0.5183643390010553\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 87\n",
      "training/learning_rate: 9.848875894591056e-05\n",
      "time/sample_batch: 0.00846409797668457\n",
      "time/training: 1.051823377609253\n",
      "evaluation/text/loss: 5.271712779998779\n",
      "evaluation/text/perplexity: 194.74923706054688\n",
      "time/total: 148.7742097377777\n",
      "time/evaluation: 0.7413346767425537\n",
      "training/train_loss_mean: 6.036978006362915\n",
      "training/train_loss_std: 0.2781139840638024\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 88\n",
      "training/learning_rate: 9.845184346096336e-05\n",
      "time/sample_batch: 0.007790565490722656\n",
      "time/training: 0.9570133686065674\n",
      "evaluation/text/loss: 5.235279083251953\n",
      "evaluation/text/perplexity: 187.78150939941406\n",
      "time/total: 150.20033025741577\n",
      "time/evaluation: 0.46740007400512695\n",
      "training/train_loss_mean: 5.941881227493286\n",
      "training/train_loss_std: 0.19808699256628334\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 89\n",
      "training/learning_rate: 9.84144904164349e-05\n",
      "time/sample_batch: 0.008168220520019531\n",
      "time/training: 1.0107710361480713\n",
      "evaluation/text/loss: 5.304581165313721\n",
      "evaluation/text/perplexity: 201.25669860839844\n",
      "time/total: 151.9534604549408\n",
      "time/evaluation: 0.7406635284423828\n",
      "training/train_loss_mean: 5.658968353271485\n",
      "training/train_loss_std: 0.6998701447937187\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 90\n",
      "training/learning_rate: 9.837670018846996e-05\n",
      "time/sample_batch: 0.009213447570800781\n",
      "time/training: 1.2387261390686035\n",
      "evaluation/text/loss: 5.255066871643066\n",
      "evaluation/text/perplexity: 191.53428649902344\n",
      "time/total: 153.98346161842346\n",
      "time/evaluation: 0.7895116806030273\n",
      "training/train_loss_mean: 5.6515202760696415\n",
      "training/train_loss_std: 1.2445024539488687\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 91\n",
      "training/learning_rate: 9.833847315761567e-05\n",
      "time/sample_batch: 0.009145498275756836\n",
      "time/training: 1.004598617553711\n",
      "evaluation/text/loss: 5.154661178588867\n",
      "evaluation/text/perplexity: 173.2371063232422\n",
      "time/total: 155.54172253608704\n",
      "time/evaluation: 0.5519499778747559\n",
      "training/train_loss_mean: 5.883993244171142\n",
      "training/train_loss_std: 0.17868079812768156\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 92\n",
      "training/learning_rate: 9.829980970881785e-05\n",
      "time/sample_batch: 0.008464336395263672\n",
      "time/training: 0.9441814422607422\n",
      "evaluation/text/loss: 5.365736484527588\n",
      "evaluation/text/perplexity: 213.94874572753906\n",
      "time/total: 157.23056030273438\n",
      "time/evaluation: 0.7429585456848145\n",
      "training/train_loss_mean: 5.728333520889282\n",
      "training/train_loss_std: 0.4444850422954352\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 93\n",
      "training/learning_rate: 9.826071023141697e-05\n",
      "time/sample_batch: 0.007531881332397461\n",
      "time/training: 1.1563770771026611\n",
      "evaluation/text/loss: 5.165574550628662\n",
      "evaluation/text/perplexity: 175.1380615234375\n",
      "time/total: 159.05894708633423\n",
      "time/evaluation: 0.6702480316162109\n",
      "training/train_loss_mean: 5.889259958267212\n",
      "training/train_loss_std: 0.315251438743014\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 94\n",
      "training/learning_rate: 9.822117511914434e-05\n",
      "time/sample_batch: 0.008696556091308594\n",
      "time/training: 1.1452999114990234\n",
      "evaluation/text/loss: 5.223123073577881\n",
      "evaluation/text/perplexity: 185.5126495361328\n",
      "time/total: 160.71240234375\n",
      "time/evaluation: 0.5063521862030029\n",
      "training/train_loss_mean: 6.025124025344849\n",
      "training/train_loss_std: 0.19488643947919224\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 95\n",
      "training/learning_rate: 9.818120477011813e-05\n",
      "time/sample_batch: 0.005924224853515625\n",
      "time/training: 0.920107364654541\n",
      "evaluation/text/loss: 5.2693562507629395\n",
      "evaluation/text/perplexity: 194.2908477783203\n",
      "time/total: 162.2050473690033\n",
      "time/evaluation: 0.5707833766937256\n",
      "training/train_loss_mean: 5.564700794219971\n",
      "training/train_loss_std: 0.697956321213018\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 96\n",
      "training/learning_rate: 9.814079958683925e-05\n",
      "time/sample_batch: 0.008501768112182617\n",
      "time/training: 1.0146560668945312\n",
      "evaluation/text/loss: 5.280490875244141\n",
      "evaluation/text/perplexity: 196.46629333496094\n",
      "time/total: 164.2053565979004\n",
      "time/evaluation: 0.9831521511077881\n",
      "training/train_loss_mean: 5.875785303115845\n",
      "training/train_loss_std: 0.26185189927776553\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 97\n",
      "training/learning_rate: 9.80999599761875e-05\n",
      "time/sample_batch: 0.007126331329345703\n",
      "time/training: 1.1979458332061768\n",
      "evaluation/text/loss: 5.298591136932373\n",
      "evaluation/text/perplexity: 200.0547637939453\n",
      "time/total: 166.17263054847717\n",
      "time/evaluation: 0.7676019668579102\n",
      "training/train_loss_mean: 5.906031799316406\n",
      "training/train_loss_std: 0.2552193531438383\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 98\n",
      "training/learning_rate: 9.80586863494173e-05\n",
      "time/sample_batch: 0.007281780242919922\n",
      "time/training: 0.9238181114196777\n",
      "evaluation/text/loss: 5.369638919830322\n",
      "evaluation/text/perplexity: 214.78529357910156\n",
      "time/total: 167.58546590805054\n",
      "time/evaluation: 0.4869694709777832\n",
      "training/train_loss_mean: 5.935097169876099\n",
      "training/train_loss_std: 0.2924818323484789\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 99\n",
      "training/learning_rate: 9.801697912215359e-05\n",
      "time/sample_batch: 0.009679317474365234\n",
      "time/training: 1.1780617237091064\n",
      "evaluation/text/loss: 5.355662822723389\n",
      "evaluation/text/perplexity: 211.8043212890625\n",
      "time/total: 169.2926468849182\n",
      "time/evaluation: 0.5275313854217529\n",
      "training/train_loss_mean: 5.885499382019043\n",
      "training/train_loss_std: 0.2601078103722067\n",
      "================================================================================\n",
      "Input: Despite<unk>'smeasurestoremainpopularwiththeSenateand | Output : theRomanpeople,supportfor<unk>remainedstronginthearmy,whichhadcalledforhis<unk>immediatelyaftertheassassination.Inanattempttoappeasethesoldiersofthe<unk>Guard,<unk>haddismissedtheirprefectTitus<unk><unk>—oneofthechiefconspiratorsagainst<unk>—andreplacedhimwithaformercommander,<unk><unk>. | Prediction:  meant Reg limited, and the children, causing the first of Virginia Tech that previous fifth took place Bowl\n",
      "Input: Meanwhile,5miles(8@.@0km)northof<unk> | Output : andACompany'sposition,BCompany,9thInfantry,heldasimilarpositiononHill<unk>overlookingthe<unk>ferrycrossingoftheriver.Thisferrywaslocatedatthemiddleofthe<unk><unk>wherethe<unk>roadcamedowntothe<unk>andcrossedit.TheUS2ndInfantryDivisionhadplannedareconnaissancemissiontostartfromtherethenightofAugust31,thesamenightthatthe<unk>ICorpsoffensiverolledacrosstheriver. | Prediction:  now visiting grip and by the effects and defense. \n",
      ", president ( charges ) at the Quarter\n",
      "Input: Manilaalsohostsseveralwell@-@knownsportsfacilitiessuchastheEnriqueM.<unk>SportsCenterandtheUniversityofSantoTomasSportsComplex,bothofwhichareprivatevenuesownedbyauniversity;collegiatesportsarealsoheld,withtheUniversityAthleticAssociationofthePhilippinesandtheNationalCollegiateAthleticAssociationbasketballgamesheldat<unk>MemorialColiseumand<unk><unk>Stadium,althoughbasketballeventshadtransferredtoSanJuan's<unk>FlyingVArenaandthe<unk>Coliseumin<unk>City.Othercollegiatesportsarestillheldatthe<unk>MemorialSportsComplex.Professionalbasketballalsousedtoplayatthecity, | Output : butthePhilippineBasketballAssociationnowholdstheirgamesat<unk>Coliseumand<unk><unk>at<unk>;thenowdefunctPhilippineBasketballLeagueplayedsomeoftheirgamesatthe<unk>MemorialSportsComplex. | Prediction: oul, speed molecules. Johnish generally have chance, on G sex. Both bookstore as a guidance\n",
      "Input: == | Output : =Discoveryofoilandgasreserves=== | Prediction:  covered Will and by most important Park pul, down.ward, <unk> cells <unk>\n",
      "================================================================================\n",
      "Iteration 100\n",
      "training/learning_rate: 9.79748387143877e-05\n",
      "time/sample_batch: 0.007810831069946289\n",
      "time/training: 1.1573207378387451\n",
      "evaluation/text/loss: 5.28314208984375\n",
      "evaluation/text/perplexity: 196.98785400390625\n",
      "time/total: 171.6829116344452\n",
      "time/evaluation: 1.2313005924224854\n",
      "training/train_loss_mean: 5.779743385314942\n",
      "training/train_loss_std: 0.3462703221560444\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 101\n",
      "training/learning_rate: 9.793226555047306e-05\n",
      "time/sample_batch: 0.009229183197021484\n",
      "time/training: 1.0870070457458496\n",
      "evaluation/text/loss: 5.214150905609131\n",
      "evaluation/text/perplexity: 183.8556365966797\n",
      "time/total: 173.30411100387573\n",
      "time/evaluation: 0.5330677032470703\n",
      "training/train_loss_mean: 5.86970887184143\n",
      "training/train_loss_std: 0.22559738457984516\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 102\n",
      "training/learning_rate: 9.788926005912098e-05\n",
      "time/sample_batch: 0.008163928985595703\n",
      "time/training: 0.9780576229095459\n",
      "evaluation/text/loss: 5.374429225921631\n",
      "evaluation/text/perplexity: 215.816650390625\n",
      "time/total: 174.89486956596375\n",
      "time/evaluation: 0.6110224723815918\n",
      "training/train_loss_mean: 5.787544679641724\n",
      "training/train_loss_std: 0.2613908938872832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 103\n",
      "training/learning_rate: 9.784582267339625e-05\n",
      "time/sample_batch: 0.008267402648925781\n",
      "time/training: 1.1827170848846436\n",
      "evaluation/text/loss: 5.255529403686523\n",
      "evaluation/text/perplexity: 191.62290954589844\n",
      "time/total: 176.8128321170807\n",
      "time/evaluation: 0.7335238456726074\n",
      "training/train_loss_mean: 5.784826707839966\n",
      "training/train_loss_std: 0.29902027804011067\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 104\n",
      "training/learning_rate: 9.780195383071288e-05\n",
      "time/sample_batch: 0.010108709335327148\n",
      "time/training: 0.9361209869384766\n",
      "evaluation/text/loss: 5.314377784729004\n",
      "evaluation/text/perplexity: 203.23802185058594\n",
      "time/total: 178.22974586486816\n",
      "time/evaluation: 0.4791381359100342\n",
      "training/train_loss_mean: 5.879369783401489\n",
      "training/train_loss_std: 0.30620721387170324\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 105\n",
      "training/learning_rate: 9.775765397282963e-05\n",
      "time/sample_batch: 0.008404970169067383\n",
      "time/training: 0.9759156703948975\n",
      "evaluation/text/loss: 5.280663967132568\n",
      "evaluation/text/perplexity: 196.50030517578125\n",
      "time/total: 179.95667147636414\n",
      "time/evaluation: 0.7492396831512451\n",
      "training/train_loss_mean: 5.919428730010987\n",
      "training/train_loss_std: 0.43048447114936345\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 106\n",
      "training/learning_rate: 9.771292354584557e-05\n",
      "time/sample_batch: 0.007185220718383789\n",
      "time/training: 0.9060728549957275\n",
      "evaluation/text/loss: 5.219172477722168\n",
      "evaluation/text/perplexity: 184.7812042236328\n",
      "time/total: 181.47504425048828\n",
      "time/evaluation: 0.6105561256408691\n",
      "training/train_loss_mean: 5.874632215499878\n",
      "training/train_loss_std: 0.3675530358722772\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 107\n",
      "training/learning_rate: 9.76677630001956e-05\n",
      "time/sample_batch: 0.0076007843017578125\n",
      "time/training: 1.247706651687622\n",
      "evaluation/text/loss: 5.249119281768799\n",
      "evaluation/text/perplexity: 190.3985137939453\n",
      "time/total: 183.42310643196106\n",
      "time/evaluation: 0.6987221240997314\n",
      "training/train_loss_mean: 5.813133192062378\n",
      "training/train_loss_std: 0.2768299166154837\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 108\n",
      "training/learning_rate: 9.762217279064592e-05\n",
      "time/sample_batch: 0.007584571838378906\n",
      "time/training: 1.0262136459350586\n",
      "evaluation/text/loss: 5.149814128875732\n",
      "evaluation/text/perplexity: 172.39944458007812\n",
      "time/total: 184.99407076835632\n",
      "time/evaluation: 0.5431015491485596\n",
      "training/train_loss_mean: 6.003598403930664\n",
      "training/train_loss_std: 0.3178557426390492\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 109\n",
      "training/learning_rate: 9.757615337628944e-05\n",
      "time/sample_batch: 0.008728742599487305\n",
      "time/training: 1.059044599533081\n",
      "evaluation/text/loss: 5.126054286956787\n",
      "evaluation/text/perplexity: 168.35153198242188\n",
      "time/total: 186.75832295417786\n",
      "time/evaluation: 0.6989388465881348\n",
      "training/train_loss_mean: 5.909770250320435\n",
      "training/train_loss_std: 0.16202408396484208\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 110\n",
      "training/learning_rate: 9.752970522054115e-05\n",
      "time/sample_batch: 0.009487628936767578\n",
      "time/training: 0.8261916637420654\n",
      "evaluation/text/loss: 5.3705668449401855\n",
      "evaluation/text/perplexity: 214.9846954345703\n",
      "time/total: 188.2620346546173\n",
      "time/evaluation: 0.6758360862731934\n",
      "training/train_loss_mean: 5.453752708435059\n",
      "training/train_loss_std: 0.6290397249237396\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 111\n",
      "training/learning_rate: 9.748282879113343e-05\n",
      "time/sample_batch: 0.009397029876708984\n",
      "time/training: 1.0865230560302734\n",
      "evaluation/text/loss: 5.368488788604736\n",
      "evaluation/text/perplexity: 214.5384063720703\n",
      "time/total: 189.8797800540924\n",
      "time/evaluation: 0.5295352935791016\n",
      "training/train_loss_mean: 5.652151441574096\n",
      "training/train_loss_std: 0.2669904399414618\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 112\n",
      "training/learning_rate: 9.743552456011147e-05\n",
      "time/sample_batch: 0.009057760238647461\n",
      "time/training: 1.116546630859375\n",
      "evaluation/text/loss: 5.409237861633301\n",
      "evaluation/text/perplexity: 223.46121215820312\n",
      "time/total: 191.57370924949646\n",
      "time/evaluation: 0.575685977935791\n",
      "training/train_loss_mean: 5.632257556915283\n",
      "training/train_loss_std: 0.434060371657684\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 113\n",
      "training/learning_rate: 9.738779300382829e-05\n",
      "time/sample_batch: 0.010007619857788086\n",
      "time/training: 1.166553020477295\n",
      "evaluation/text/loss: 5.02017068862915\n",
      "evaluation/text/perplexity: 151.43714904785156\n",
      "time/total: 193.48993015289307\n",
      "time/evaluation: 0.7479798793792725\n",
      "training/train_loss_mean: 5.784805679321289\n",
      "training/train_loss_std: 0.21698769468674553\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 114\n",
      "training/learning_rate: 9.733963460294015e-05\n",
      "time/sample_batch: 0.005915403366088867\n",
      "time/training: 0.8361008167266846\n",
      "evaluation/text/loss: 5.23010778427124\n",
      "evaluation/text/perplexity: 186.8129425048828\n",
      "time/total: 195.14653873443604\n",
      "time/evaluation: 0.8187072277069092\n",
      "training/train_loss_mean: 5.353115367889404\n",
      "training/train_loss_std: 0.8438789495358594\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 115\n",
      "training/learning_rate: 9.729104984240162e-05\n",
      "time/sample_batch: 0.008247137069702148\n",
      "time/training: 0.9223237037658691\n",
      "evaluation/text/loss: 5.162489891052246\n",
      "evaluation/text/perplexity: 174.59864807128906\n",
      "time/total: 196.77126049995422\n",
      "time/evaluation: 0.7006754875183105\n",
      "training/train_loss_mean: 5.779144239425659\n",
      "training/train_loss_std: 0.3099641881482246\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 116\n",
      "training/learning_rate: 9.72420392114607e-05\n",
      "time/sample_batch: 0.007004261016845703\n",
      "time/training: 1.118405818939209\n",
      "evaluation/text/loss: 5.2070159912109375\n",
      "evaluation/text/perplexity: 182.54852294921875\n",
      "time/total: 198.4553735256195\n",
      "time/evaluation: 0.5641043186187744\n",
      "training/train_loss_mean: 5.62503719329834\n",
      "training/train_loss_std: 0.41027618051901515\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 117\n",
      "training/learning_rate: 9.71926032036539e-05\n",
      "time/sample_batch: 0.008767843246459961\n",
      "time/training: 1.0509212017059326\n",
      "evaluation/text/loss: 5.316605567932129\n",
      "evaluation/text/perplexity: 203.6912841796875\n",
      "time/total: 200.29045963287354\n",
      "time/evaluation: 0.7824215888977051\n",
      "training/train_loss_mean: 5.680547094345092\n",
      "training/train_loss_std: 0.38469553294659714\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 118\n",
      "training/learning_rate: 9.714274231680125e-05\n",
      "time/sample_batch: 0.0069010257720947266\n",
      "time/training: 0.8372097015380859\n",
      "evaluation/text/loss: 5.223614692687988\n",
      "evaluation/text/perplexity: 185.60386657714844\n",
      "time/total: 201.74257397651672\n",
      "time/evaluation: 0.6129050254821777\n",
      "training/train_loss_mean: 5.552610254287719\n",
      "training/train_loss_std: 0.3759159789758665\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 119\n",
      "training/learning_rate: 9.709245705300136e-05\n",
      "time/sample_batch: 0.00733184814453125\n",
      "time/training: 0.9736764430999756\n",
      "evaluation/text/loss: 5.181028842926025\n",
      "evaluation/text/perplexity: 177.86570739746094\n",
      "time/total: 203.38329458236694\n",
      "time/evaluation: 0.665393590927124\n",
      "training/train_loss_mean: 5.707210636138916\n",
      "training/train_loss_std: 0.31484345706999795\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 120\n",
      "training/learning_rate: 9.704174791862625e-05\n",
      "time/sample_batch: 0.008533000946044922\n",
      "time/training: 1.1316850185394287\n",
      "evaluation/text/loss: 5.293550491333008\n",
      "evaluation/text/perplexity: 199.04888916015625\n",
      "time/total: 205.11790919303894\n",
      "time/evaluation: 0.6012692451477051\n",
      "training/train_loss_mean: 5.7310662269592285\n",
      "training/train_loss_std: 0.2451819355569072\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 121\n",
      "training/learning_rate: 9.699061542431634e-05\n",
      "time/sample_batch: 0.007869243621826172\n",
      "time/training: 1.0767908096313477\n",
      "evaluation/text/loss: 5.203638553619385\n",
      "evaluation/text/perplexity: 181.93301391601562\n",
      "time/total: 206.73755264282227\n",
      "time/evaluation: 0.5410387516021729\n",
      "training/train_loss_mean: 5.6726432800292965\n",
      "training/train_loss_std: 0.4475687698496599\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 122\n",
      "training/learning_rate: 9.693906008497531e-05\n",
      "time/sample_batch: 0.007839441299438477\n",
      "time/training: 1.1059346199035645\n",
      "evaluation/text/loss: 5.28787899017334\n",
      "evaluation/text/perplexity: 197.92318725585938\n",
      "time/total: 208.39025688171387\n",
      "time/evaluation: 0.545067310333252\n",
      "training/train_loss_mean: 5.632797002792358\n",
      "training/train_loss_std: 0.4991793438844194\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 123\n",
      "training/learning_rate: 9.688708241976482e-05\n",
      "time/sample_batch: 0.008452177047729492\n",
      "time/training: 1.0834286212921143\n",
      "evaluation/text/loss: 5.153101444244385\n",
      "evaluation/text/perplexity: 172.96710205078125\n",
      "time/total: 209.94161772727966\n",
      "time/evaluation: 0.4663259983062744\n",
      "training/train_loss_mean: 5.854089069366455\n",
      "training/train_loss_std: 0.3498064049182728\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 124\n",
      "training/learning_rate: 9.68346829520994e-05\n",
      "time/sample_batch: 0.008394718170166016\n",
      "time/training: 1.1272623538970947\n",
      "evaluation/text/loss: 5.246481418609619\n",
      "evaluation/text/perplexity: 189.8969268798828\n",
      "time/total: 211.5913212299347\n",
      "time/evaluation: 0.520721435546875\n",
      "training/train_loss_mean: 5.87159833908081\n",
      "training/train_loss_std: 0.217301947663739\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 125\n",
      "training/learning_rate: 9.678186220964111e-05\n",
      "time/sample_batch: 0.007109403610229492\n",
      "time/training: 1.1186392307281494\n",
      "evaluation/text/loss: 5.109006404876709\n",
      "evaluation/text/perplexity: 165.50582885742188\n",
      "time/total: 213.19032955169678\n",
      "time/evaluation: 0.47869157791137695\n",
      "training/train_loss_mean: 5.881881475448608\n",
      "training/train_loss_std: 0.12096902619957428\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 126\n",
      "training/learning_rate: 9.672862072429423e-05\n",
      "time/sample_batch: 0.006925106048583984\n",
      "time/training: 1.0153815746307373\n",
      "evaluation/text/loss: 5.217473983764648\n",
      "evaluation/text/perplexity: 184.46762084960938\n",
      "time/total: 214.6745901107788\n",
      "time/evaluation: 0.46728086471557617\n",
      "training/train_loss_mean: 5.766046714782715\n",
      "training/train_loss_std: 0.2734444791798745\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 127\n",
      "training/learning_rate: 9.667495903219993e-05\n",
      "time/sample_batch: 0.006669759750366211\n",
      "time/training: 1.219193935394287\n",
      "evaluation/text/loss: 5.265790939331055\n",
      "evaluation/text/perplexity: 193.59938049316406\n",
      "time/total: 216.48408722877502\n",
      "time/evaluation: 0.5884864330291748\n",
      "training/train_loss_mean: 5.704398345947266\n",
      "training/train_loss_std: 0.3360816832759259\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 128\n",
      "training/learning_rate: 9.662087767373086e-05\n",
      "time/sample_batch: 0.008194446563720703\n",
      "time/training: 0.9344031810760498\n",
      "evaluation/text/loss: 5.148040294647217\n",
      "evaluation/text/perplexity: 172.09390258789062\n",
      "time/total: 218.1710159778595\n",
      "time/evaluation: 0.7508366107940674\n",
      "training/train_loss_mean: 5.704587030410766\n",
      "training/train_loss_std: 0.20752141052909495\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 129\n",
      "training/learning_rate: 9.65663771934857e-05\n",
      "time/sample_batch: 0.008585453033447266\n",
      "time/training: 1.1552062034606934\n",
      "evaluation/text/loss: 5.2496514320373535\n",
      "evaluation/text/perplexity: 190.49984741210938\n",
      "time/total: 220.05697321891785\n",
      "time/evaluation: 0.7290029525756836\n",
      "training/train_loss_mean: 5.863367223739624\n",
      "training/train_loss_std: 0.0846737121972116\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 130\n",
      "training/learning_rate: 9.651145814028366e-05\n",
      "time/sample_batch: 0.006677150726318359\n",
      "time/training: 1.050652027130127\n",
      "evaluation/text/loss: 5.16218900680542\n",
      "evaluation/text/perplexity: 174.54612731933594\n",
      "time/total: 221.75461173057556\n",
      "time/evaluation: 0.645310640335083\n",
      "training/train_loss_mean: 5.705689477920532\n",
      "training/train_loss_std: 0.3754066926819086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 131\n",
      "training/learning_rate: 9.645612106715901e-05\n",
      "time/sample_batch: 0.00764155387878418\n",
      "time/training: 0.9852283000946045\n",
      "evaluation/text/loss: 5.020129203796387\n",
      "evaluation/text/perplexity: 151.4308624267578\n",
      "time/total: 223.7197961807251\n",
      "time/evaluation: 0.9778716564178467\n",
      "training/train_loss_mean: 5.70548038482666\n",
      "training/train_loss_std: 0.2695772915499672\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 132\n",
      "training/learning_rate: 9.640036653135546e-05\n",
      "time/sample_batch: 0.010448455810546875\n",
      "time/training: 1.111140251159668\n",
      "evaluation/text/loss: 5.189301013946533\n",
      "evaluation/text/perplexity: 179.34315490722656\n",
      "time/total: 225.41637086868286\n",
      "time/evaluation: 0.5837552547454834\n",
      "training/train_loss_mean: 5.750795650482178\n",
      "training/train_loss_std: 0.36400923991619955\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 133\n",
      "training/learning_rate: 9.634419509432059e-05\n",
      "time/sample_batch: 0.008730411529541016\n",
      "time/training: 1.2296617031097412\n",
      "evaluation/text/loss: 5.110595226287842\n",
      "evaluation/text/perplexity: 165.7689971923828\n",
      "time/total: 227.15056037902832\n",
      "time/evaluation: 0.5025830268859863\n",
      "training/train_loss_mean: 5.751209497451782\n",
      "training/train_loss_std: 0.31349366418849156\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 134\n",
      "training/learning_rate: 9.628760732170011e-05\n",
      "time/sample_batch: 0.008267402648925781\n",
      "time/training: 0.9996898174285889\n",
      "evaluation/text/loss: 5.1304802894592285\n",
      "evaluation/text/perplexity: 169.0983123779297\n",
      "time/total: 228.71893787384033\n",
      "time/evaluation: 0.5670113563537598\n",
      "training/train_loss_mean: 5.379115295410156\n",
      "training/train_loss_std: 1.12653869303164\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 135\n",
      "training/learning_rate: 9.62306037833323e-05\n",
      "time/sample_batch: 0.008110523223876953\n",
      "time/training: 1.2241933345794678\n",
      "evaluation/text/loss: 5.330904483795166\n",
      "evaluation/text/perplexity: 206.62477111816406\n",
      "time/total: 230.43197011947632\n",
      "time/evaluation: 0.4871697425842285\n",
      "training/train_loss_mean: 5.6885413646698\n",
      "training/train_loss_std: 0.4304379382112457\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 136\n",
      "training/learning_rate: 9.617318505324213e-05\n",
      "time/sample_batch: 0.007386922836303711\n",
      "time/training: 0.9440984725952148\n",
      "evaluation/text/loss: 5.266666889190674\n",
      "evaluation/text/perplexity: 193.76902770996094\n",
      "time/total: 231.98409605026245\n",
      "time/evaluation: 0.6062307357788086\n",
      "training/train_loss_mean: 5.702939796447754\n",
      "training/train_loss_std: 0.46231816097363987\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 137\n",
      "training/learning_rate: 9.611535170963555e-05\n",
      "time/sample_batch: 0.00859522819519043\n",
      "time/training: 0.9286177158355713\n",
      "evaluation/text/loss: 5.223723888397217\n",
      "evaluation/text/perplexity: 185.6241455078125\n",
      "time/total: 233.65606713294983\n",
      "time/evaluation: 0.7415890693664551\n",
      "training/train_loss_mean: 5.647963619232177\n",
      "training/train_loss_std: 0.22731886983598976\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 138\n",
      "training/learning_rate: 9.605710433489374e-05\n",
      "time/sample_batch: 0.007783174514770508\n",
      "time/training: 1.1965200901031494\n",
      "evaluation/text/loss: 5.140230178833008\n",
      "evaluation/text/perplexity: 170.75506591796875\n",
      "time/total: 235.67278671264648\n",
      "time/evaluation: 0.818474292755127\n",
      "training/train_loss_mean: 5.85888295173645\n",
      "training/train_loss_std: 0.3141300733860802\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 139\n",
      "training/learning_rate: 9.599844351556711e-05\n",
      "time/sample_batch: 0.008837223052978516\n",
      "time/training: 0.9798800945281982\n",
      "evaluation/text/loss: 5.0844879150390625\n",
      "evaluation/text/perplexity: 161.49722290039062\n",
      "time/total: 237.1715259552002\n",
      "time/evaluation: 0.5170962810516357\n",
      "training/train_loss_mean: 5.671724557876587\n",
      "training/train_loss_std: 0.3957856755227372\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 140\n",
      "training/learning_rate: 9.593936984236943e-05\n",
      "time/sample_batch: 0.007926464080810547\n",
      "time/training: 0.9709177017211914\n",
      "evaluation/text/loss: 4.968217849731445\n",
      "evaluation/text/perplexity: 143.7704315185547\n",
      "time/total: 238.53783202171326\n",
      "time/evaluation: 0.39369988441467285\n",
      "training/train_loss_mean: 5.7404032230377195\n",
      "training/train_loss_std: 0.24403220260889666\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 141\n",
      "training/learning_rate: 9.587988391017197e-05\n",
      "time/sample_batch: 0.007436990737915039\n",
      "time/training: 1.2215771675109863\n",
      "evaluation/text/loss: 4.998751163482666\n",
      "evaluation/text/perplexity: 148.22793579101562\n",
      "time/total: 240.3580379486084\n",
      "time/evaluation: 0.596968412399292\n",
      "training/train_loss_mean: 5.736474943161011\n",
      "training/train_loss_std: 0.38749953759035854\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 142\n",
      "training/learning_rate: 9.581998631799737e-05\n",
      "time/sample_batch: 0.008462190628051758\n",
      "time/training: 1.1101086139678955\n",
      "evaluation/text/loss: 5.250922203063965\n",
      "evaluation/text/perplexity: 190.74209594726562\n",
      "time/total: 241.93837928771973\n",
      "time/evaluation: 0.46852827072143555\n",
      "training/train_loss_mean: 5.544686222076416\n",
      "training/train_loss_std: 0.3363994860353837\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 143\n",
      "training/learning_rate: 9.575967766901373e-05\n",
      "time/sample_batch: 0.008393526077270508\n",
      "time/training: 1.1234397888183594\n",
      "evaluation/text/loss: 5.235719203948975\n",
      "evaluation/text/perplexity: 187.86416625976562\n",
      "time/total: 243.58582162857056\n",
      "time/evaluation: 0.5222899913787842\n",
      "training/train_loss_mean: 5.529033374786377\n",
      "training/train_loss_std: 0.2185748348023242\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 144\n",
      "training/learning_rate: 9.569895857052849e-05\n",
      "time/sample_batch: 0.00937509536743164\n",
      "time/training: 1.1838841438293457\n",
      "evaluation/text/loss: 5.353138446807861\n",
      "evaluation/text/perplexity: 211.27032470703125\n",
      "time/total: 245.2113811969757\n",
      "time/evaluation: 0.4399228096008301\n",
      "training/train_loss_mean: 5.671419334411621\n",
      "training/train_loss_std: 0.4873582601846103\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 145\n",
      "training/learning_rate: 9.563782963398229e-05\n",
      "time/sample_batch: 0.008379936218261719\n",
      "time/training: 1.0392637252807617\n",
      "evaluation/text/loss: 5.341097831726074\n",
      "evaluation/text/perplexity: 208.7417449951172\n",
      "time/total: 246.9331088066101\n",
      "time/evaluation: 0.6806929111480713\n",
      "training/train_loss_mean: 5.853247499465942\n",
      "training/train_loss_std: 0.1753181076594327\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 146\n",
      "training/learning_rate: 9.557629147494284e-05\n",
      "time/sample_batch: 0.00835561752319336\n",
      "time/training: 1.0915312767028809\n",
      "evaluation/text/loss: 5.226160049438477\n",
      "evaluation/text/perplexity: 186.076904296875\n",
      "time/total: 248.7694354057312\n",
      "time/evaluation: 0.7430830001831055\n",
      "training/train_loss_mean: 5.76519627571106\n",
      "training/train_loss_std: 0.282007875835264\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 147\n",
      "training/learning_rate: 9.55143447130987e-05\n",
      "time/sample_batch: 0.0085296630859375\n",
      "time/training: 1.1262705326080322\n",
      "evaluation/text/loss: 5.07826566696167\n",
      "evaluation/text/perplexity: 160.49546813964844\n",
      "time/total: 250.3651168346405\n",
      "time/evaluation: 0.46773219108581543\n",
      "training/train_loss_mean: 5.703029346466065\n",
      "training/train_loss_std: 0.29701533075167086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 148\n",
      "training/learning_rate: 9.545198997225312e-05\n",
      "time/sample_batch: 0.006450176239013672\n",
      "time/training: 1.0288348197937012\n",
      "evaluation/text/loss: 5.003077983856201\n",
      "evaluation/text/perplexity: 148.8706817626953\n",
      "time/total: 252.13251900672913\n",
      "time/evaluation: 0.7326400279998779\n",
      "training/train_loss_mean: 5.651639366149903\n",
      "training/train_loss_std: 0.397127791375334\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 149\n",
      "training/learning_rate: 9.538922788031761e-05\n",
      "time/sample_batch: 0.00904989242553711\n",
      "time/training: 1.0462770462036133\n",
      "evaluation/text/loss: 5.0797929763793945\n",
      "evaluation/text/perplexity: 160.7407684326172\n",
      "time/total: 253.69074082374573\n",
      "time/evaluation: 0.5102384090423584\n",
      "training/train_loss_mean: 5.812685918807984\n",
      "training/train_loss_std: 0.36567396009537784\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 150\n",
      "training/learning_rate: 9.532605906930574e-05\n",
      "time/sample_batch: 0.007515430450439453\n",
      "time/training: 0.9565646648406982\n",
      "evaluation/text/loss: 5.097695827484131\n",
      "evaluation/text/perplexity: 163.6444091796875\n",
      "time/total: 255.39393496513367\n",
      "time/evaluation: 0.744926929473877\n",
      "training/train_loss_mean: 5.66755690574646\n",
      "training/train_loss_std: 0.29898804306502424\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 151\n",
      "training/learning_rate: 9.526248417532675e-05\n",
      "time/sample_batch: 0.007525444030761719\n",
      "time/training: 1.0292668342590332\n",
      "evaluation/text/loss: 5.085981845855713\n",
      "evaluation/text/perplexity: 161.73866271972656\n",
      "time/total: 257.2043957710266\n",
      "time/evaluation: 0.7795467376708984\n",
      "training/train_loss_mean: 5.500068283081054\n",
      "training/train_loss_std: 0.4611054951377052\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 152\n",
      "training/learning_rate: 9.519850383857911e-05\n",
      "time/sample_batch: 0.008978605270385742\n",
      "time/training: 1.0421438217163086\n",
      "evaluation/text/loss: 5.111169815063477\n",
      "evaluation/text/perplexity: 165.86427307128906\n",
      "time/total: 258.8037197589874\n",
      "time/evaluation: 0.5555105209350586\n",
      "training/train_loss_mean: 5.63687834739685\n",
      "training/train_loss_std: 0.20271507762423363\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 153\n",
      "training/learning_rate: 9.513411870334411e-05\n",
      "time/sample_batch: 0.008470296859741211\n",
      "time/training: 0.9545803070068359\n",
      "evaluation/text/loss: 5.194962501525879\n",
      "evaluation/text/perplexity: 180.3613739013672\n",
      "time/total: 260.33347821235657\n",
      "time/evaluation: 0.5736515522003174\n",
      "training/train_loss_mean: 5.571423482894898\n",
      "training/train_loss_std: 0.3674479529861843\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 154\n",
      "training/learning_rate: 9.506932941797933e-05\n",
      "time/sample_batch: 0.007618904113769531\n",
      "time/training: 0.9905917644500732\n",
      "evaluation/text/loss: 5.256963729858398\n",
      "evaluation/text/perplexity: 191.89794921875\n",
      "time/total: 262.0669684410095\n",
      "time/evaluation: 0.7413814067840576\n",
      "training/train_loss_mean: 5.748898029327393\n",
      "training/train_loss_std: 0.18259925278757252\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 155\n",
      "training/learning_rate: 9.500413663491218e-05\n",
      "time/sample_batch: 0.008782625198364258\n",
      "time/training: 0.9745914936065674\n",
      "evaluation/text/loss: 5.156988620758057\n",
      "evaluation/text/perplexity: 173.64076232910156\n",
      "time/total: 263.69851183891296\n",
      "time/evaluation: 0.6552491188049316\n",
      "training/train_loss_mean: 5.705394124984741\n",
      "training/train_loss_std: 0.2121484062648835\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 156\n",
      "training/learning_rate: 9.493854101063323e-05\n",
      "time/sample_batch: 0.008276224136352539\n",
      "time/training: 1.2604138851165771\n",
      "evaluation/text/loss: 5.393332481384277\n",
      "evaluation/text/perplexity: 219.93508911132812\n",
      "time/total: 265.6493535041809\n",
      "time/evaluation: 0.6886775493621826\n",
      "training/train_loss_mean: 5.531825017929077\n",
      "training/train_loss_std: 0.4054760248189681\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 157\n",
      "training/learning_rate: 9.487254320568971e-05\n",
      "time/sample_batch: 0.008486032485961914\n",
      "time/training: 1.2611103057861328\n",
      "evaluation/text/loss: 5.051053047180176\n",
      "evaluation/text/perplexity: 156.1868438720703\n",
      "time/total: 267.67448830604553\n",
      "time/evaluation: 0.7622115612030029\n",
      "training/train_loss_mean: 5.583489179611206\n",
      "training/train_loss_std: 0.25295569315335054\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 158\n",
      "training/learning_rate: 9.480614388467878e-05\n",
      "time/sample_batch: 0.008962392807006836\n",
      "time/training: 1.0799610614776611\n",
      "evaluation/text/loss: 5.146942138671875\n",
      "evaluation/text/perplexity: 171.905029296875\n",
      "time/total: 269.20843982696533\n",
      "time/evaluation: 0.45245981216430664\n",
      "training/train_loss_mean: 5.734737062454224\n",
      "training/train_loss_std: 0.2327205796896982\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 159\n",
      "training/learning_rate: 9.473934371624086e-05\n",
      "time/sample_batch: 0.008760452270507812\n",
      "time/training: 1.147230625152588\n",
      "evaluation/text/loss: 5.074866771697998\n",
      "evaluation/text/perplexity: 159.9508819580078\n",
      "time/total: 271.34935235977173\n",
      "time/evaluation: 0.9919404983520508\n",
      "training/train_loss_mean: 5.676836824417114\n",
      "training/train_loss_std: 0.2979495624089651\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 160\n",
      "training/learning_rate: 9.46721433730529e-05\n",
      "time/sample_batch: 0.009259462356567383\n",
      "time/training: 1.0199096202850342\n",
      "evaluation/text/loss: 5.041436195373535\n",
      "evaluation/text/perplexity: 154.6920166015625\n",
      "time/total: 272.91641330718994\n",
      "time/evaluation: 0.5453650951385498\n",
      "training/train_loss_mean: 5.614162826538086\n",
      "training/train_loss_std: 0.2104480355954261\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 161\n",
      "training/learning_rate: 9.460454353182162e-05\n",
      "time/sample_batch: 0.007365226745605469\n",
      "time/training: 0.907794713973999\n",
      "evaluation/text/loss: 5.321239471435547\n",
      "evaluation/text/perplexity: 204.6373748779297\n",
      "time/total: 274.4022071361542\n",
      "time/evaluation: 0.576291561126709\n",
      "training/train_loss_mean: 5.588629794120789\n",
      "training/train_loss_std: 0.6966990171997068\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 162\n",
      "training/learning_rate: 9.453654487327668e-05\n",
      "time/sample_batch: 0.009570598602294922\n",
      "time/training: 1.2194576263427734\n",
      "evaluation/text/loss: 5.232307434082031\n",
      "evaluation/text/perplexity: 187.2243194580078\n",
      "time/total: 276.1816437244415\n",
      "time/evaluation: 0.558457612991333\n",
      "training/train_loss_mean: 5.816854047775268\n",
      "training/train_loss_std: 0.18121014738063454\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 163\n",
      "training/learning_rate: 9.446814808216379e-05\n",
      "time/sample_batch: 0.007881402969360352\n",
      "time/training: 0.9648323059082031\n",
      "evaluation/text/loss: 5.086660385131836\n",
      "evaluation/text/perplexity: 161.84844970703125\n",
      "time/total: 277.65162467956543\n",
      "time/evaluation: 0.5035545825958252\n",
      "training/train_loss_mean: 5.588711833953857\n",
      "training/train_loss_std: 0.2625218608628076\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 164\n",
      "training/learning_rate: 9.439935384723789e-05\n",
      "time/sample_batch: 0.00960540771484375\n",
      "time/training: 1.123002529144287\n",
      "evaluation/text/loss: 5.2942891120910645\n",
      "evaluation/text/perplexity: 199.1959686279297\n",
      "time/total: 279.3485758304596\n",
      "time/evaluation: 0.5724122524261475\n",
      "training/train_loss_mean: 5.597657060623169\n",
      "training/train_loss_std: 0.23704334095246832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 165\n",
      "training/learning_rate: 9.433016286125616e-05\n",
      "time/sample_batch: 0.009368419647216797\n",
      "time/training: 0.9043385982513428\n",
      "evaluation/text/loss: 5.000166416168213\n",
      "evaluation/text/perplexity: 148.4378662109375\n",
      "time/total: 280.79414105415344\n",
      "time/evaluation: 0.5396718978881836\n",
      "training/train_loss_mean: 5.225132524967194\n",
      "training/train_loss_std: 1.2855875960611831\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 166\n",
      "training/learning_rate: 9.426057582097107e-05\n",
      "time/sample_batch: 0.00855875015258789\n",
      "time/training: 0.8878581523895264\n",
      "evaluation/text/loss: 5.107196807861328\n",
      "evaluation/text/perplexity: 165.20660400390625\n",
      "time/total: 282.24874925613403\n",
      "time/evaluation: 0.5651919841766357\n",
      "training/train_loss_mean: 5.617787504196167\n",
      "training/train_loss_std: 0.24803417960224142\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 167\n",
      "training/learning_rate: 9.419059342712335e-05\n",
      "time/sample_batch: 0.008056640625\n",
      "time/training: 0.9719040393829346\n",
      "evaluation/text/loss: 5.106257438659668\n",
      "evaluation/text/perplexity: 165.05148315429688\n",
      "time/total: 283.9226334095001\n",
      "time/evaluation: 0.7001538276672363\n",
      "training/train_loss_mean: 5.639386558532715\n",
      "training/train_loss_std: 0.23363504123986434\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 168\n",
      "training/learning_rate: 9.41202163844349e-05\n",
      "time/sample_batch: 0.009206056594848633\n",
      "time/training: 1.1351027488708496\n",
      "evaluation/text/loss: 5.158635139465332\n",
      "evaluation/text/perplexity: 173.92691040039062\n",
      "time/total: 285.6639790534973\n",
      "time/evaluation: 0.6045024394989014\n",
      "training/train_loss_mean: 5.60411491394043\n",
      "training/train_loss_std: 0.28790750977257856\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 169\n",
      "training/learning_rate: 9.404944540160178e-05\n",
      "time/sample_batch: 0.008548259735107422\n",
      "time/training: 1.1661157608032227\n",
      "evaluation/text/loss: 5.199803352355957\n",
      "evaluation/text/perplexity: 181.23660278320312\n",
      "time/total: 287.3792860507965\n",
      "time/evaluation: 0.5474369525909424\n",
      "training/train_loss_mean: 5.592983436584473\n",
      "training/train_loss_std: 0.3493091921997545\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 170\n",
      "training/learning_rate: 9.3978281191287e-05\n",
      "time/sample_batch: 0.008968114852905273\n",
      "time/training: 1.1494777202606201\n",
      "evaluation/text/loss: 5.234529972076416\n",
      "evaluation/text/perplexity: 187.64088439941406\n",
      "time/total: 289.20955538749695\n",
      "time/evaluation: 0.6792426109313965\n",
      "training/train_loss_mean: 5.759337472915649\n",
      "training/train_loss_std: 0.33820448067777403\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 171\n",
      "training/learning_rate: 9.390672447011335e-05\n",
      "time/sample_batch: 0.009803533554077148\n",
      "time/training: 1.174555778503418\n",
      "evaluation/text/loss: 5.03694486618042\n",
      "evaluation/text/perplexity: 153.99880981445312\n",
      "time/total: 291.2063829898834\n",
      "time/evaluation: 0.8206512928009033\n",
      "training/train_loss_mean: 5.786636161804199\n",
      "training/train_loss_std: 0.2053423408636832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 172\n",
      "training/learning_rate: 9.383477595865625e-05\n",
      "time/sample_batch: 0.009386539459228516\n",
      "time/training: 0.9043641090393066\n",
      "evaluation/text/loss: 5.170838356018066\n",
      "evaluation/text/perplexity: 176.0623779296875\n",
      "time/total: 292.6987473964691\n",
      "time/evaluation: 0.5862610340118408\n",
      "training/train_loss_mean: 5.662788152694702\n",
      "training/train_loss_std: 0.2590624602058698\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 173\n",
      "training/learning_rate: 9.376243638143637e-05\n",
      "time/sample_batch: 0.007190227508544922\n",
      "time/training: 1.2074520587921143\n",
      "evaluation/text/loss: 5.097918510437012\n",
      "evaluation/text/perplexity: 163.68084716796875\n",
      "time/total: 294.50828886032104\n",
      "time/evaluation: 0.6003563404083252\n",
      "training/train_loss_mean: 5.7733160018920895\n",
      "training/train_loss_std: 0.21731918748032533\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 174\n",
      "training/learning_rate: 9.36897064669125e-05\n",
      "time/sample_batch: 0.007397890090942383\n",
      "time/training: 1.0484843254089355\n",
      "evaluation/text/loss: 5.288044452667236\n",
      "evaluation/text/perplexity: 197.9559326171875\n",
      "time/total: 296.1725342273712\n",
      "time/evaluation: 0.6140072345733643\n",
      "training/train_loss_mean: 5.5070547580719\n",
      "training/train_loss_std: 0.40959482965516364\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 175\n",
      "training/learning_rate: 9.361658694747403e-05\n",
      "time/sample_batch: 0.008055686950683594\n",
      "time/training: 1.0019874572753906\n",
      "evaluation/text/loss: 5.149205207824707\n",
      "evaluation/text/perplexity: 172.29449462890625\n",
      "time/total: 297.6614143848419\n",
      "time/evaluation: 0.48523807525634766\n",
      "training/train_loss_mean: 5.635065269470215\n",
      "training/train_loss_std: 0.48400110410310243\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 176\n",
      "training/learning_rate: 9.354307855943372e-05\n",
      "time/sample_batch: 0.007474184036254883\n",
      "time/training: 1.033456563949585\n",
      "evaluation/text/loss: 5.196211338043213\n",
      "evaluation/text/perplexity: 180.58676147460938\n",
      "time/total: 299.12929582595825\n",
      "time/evaluation: 0.4327218532562256\n",
      "training/train_loss_mean: 5.498471641540528\n",
      "training/train_loss_std: 0.22002883664567716\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 177\n",
      "training/learning_rate: 9.346918204302022e-05\n",
      "time/sample_batch: 0.007565975189208984\n",
      "time/training: 0.9686276912689209\n",
      "evaluation/text/loss: 4.941281318664551\n",
      "evaluation/text/perplexity: 139.94944763183594\n",
      "time/total: 301.0904173851013\n",
      "time/evaluation: 0.9907875061035156\n",
      "training/train_loss_mean: 5.509360408782959\n",
      "training/train_loss_std: 0.40306044175546296\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 178\n",
      "training/learning_rate: 9.339489814237065e-05\n",
      "time/sample_batch: 0.009560585021972656\n",
      "time/training: 0.99833083152771\n",
      "evaluation/text/loss: 5.36841344833374\n",
      "evaluation/text/perplexity: 214.52224731445312\n",
      "time/total: 302.73825907707214\n",
      "time/evaluation: 0.6475977897644043\n",
      "training/train_loss_mean: 5.670515775680542\n",
      "training/train_loss_std: 0.4441696905437393\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 179\n",
      "training/learning_rate: 9.332022760552303e-05\n",
      "time/sample_batch: 0.008348464965820312\n",
      "time/training: 1.184818983078003\n",
      "evaluation/text/loss: 4.880859851837158\n",
      "evaluation/text/perplexity: 131.743896484375\n",
      "time/total: 304.4599623680115\n",
      "time/evaluation: 0.5352613925933838\n",
      "training/train_loss_mean: 5.512328958511352\n",
      "training/train_loss_std: 0.3522878007000288\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 180\n",
      "training/learning_rate: 9.324517118440888e-05\n",
      "time/sample_batch: 0.008799552917480469\n",
      "time/training: 0.9612503051757812\n",
      "evaluation/text/loss: 5.141738414764404\n",
      "evaluation/text/perplexity: 171.01280212402344\n",
      "time/total: 305.934232711792\n",
      "time/evaluation: 0.511286735534668\n",
      "training/train_loss_mean: 5.4548424243927\n",
      "training/train_loss_std: 0.3514379029544045\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 181\n",
      "training/learning_rate: 9.316972963484549e-05\n",
      "time/sample_batch: 0.009639978408813477\n",
      "time/training: 1.3305282592773438\n",
      "evaluation/text/loss: 4.848632335662842\n",
      "evaluation/text/perplexity: 127.56580352783203\n",
      "time/total: 307.73076844215393\n",
      "time/evaluation: 0.4643208980560303\n",
      "training/train_loss_mean: 5.683193063735962\n",
      "training/train_loss_std: 0.1676940674854796\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 182\n",
      "training/learning_rate: 9.309390371652845e-05\n",
      "time/sample_batch: 0.007279872894287109\n",
      "time/training: 1.0121910572052002\n",
      "evaluation/text/loss: 4.998801231384277\n",
      "evaluation/text/perplexity: 148.2353515625\n",
      "time/total: 309.4602584838867\n",
      "time/evaluation: 0.715761661529541\n",
      "training/train_loss_mean: 5.551751136779785\n",
      "training/train_loss_std: 0.4972929028222846\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 183\n",
      "training/learning_rate: 9.30176941930239e-05\n",
      "time/sample_batch: 0.009210348129272461\n",
      "time/training: 1.2996885776519775\n",
      "evaluation/text/loss: 5.0779619216918945\n",
      "evaluation/text/perplexity: 160.44671630859375\n",
      "time/total: 311.42033886909485\n",
      "time/evaluation: 0.6587369441986084\n",
      "training/train_loss_mean: 5.4751122951507565\n",
      "training/train_loss_std: 0.420379013900503\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 184\n",
      "training/learning_rate: 9.294110183176091e-05\n",
      "time/sample_batch: 0.00832986831665039\n",
      "time/training: 1.0030009746551514\n",
      "evaluation/text/loss: 5.264370918273926\n",
      "evaluation/text/perplexity: 193.32464599609375\n",
      "time/total: 312.86434864997864\n",
      "time/evaluation: 0.43944573402404785\n",
      "training/train_loss_mean: 5.718148040771484\n",
      "training/train_loss_std: 0.21564166103039778\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 185\n",
      "training/learning_rate: 9.286412740402368e-05\n",
      "time/sample_batch: 0.008462905883789062\n",
      "time/training: 0.958810567855835\n",
      "evaluation/text/loss: 4.854504585266113\n",
      "evaluation/text/perplexity: 128.31710815429688\n",
      "time/total: 314.43629026412964\n",
      "time/evaluation: 0.6113443374633789\n",
      "training/train_loss_mean: 5.652873563766479\n",
      "training/train_loss_std: 0.3990412395175734\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 186\n",
      "training/learning_rate: 9.278677168494387e-05\n",
      "time/sample_batch: 0.007415056228637695\n",
      "time/training: 1.0830399990081787\n",
      "evaluation/text/loss: 5.269130229949951\n",
      "evaluation/text/perplexity: 194.24693298339844\n",
      "time/total: 316.2208592891693\n",
      "time/evaluation: 0.6999599933624268\n",
      "training/train_loss_mean: 5.806671905517578\n",
      "training/train_loss_std: 0.1930718917698419\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 187\n",
      "training/learning_rate: 9.270903545349272e-05\n",
      "time/sample_batch: 0.008341073989868164\n",
      "time/training: 1.068162441253662\n",
      "evaluation/text/loss: 5.1053547859191895\n",
      "evaluation/text/perplexity: 164.90257263183594\n",
      "time/total: 317.8678035736084\n",
      "time/evaluation: 0.5727424621582031\n",
      "training/train_loss_mean: 5.545418405532837\n",
      "training/train_loss_std: 0.34389997229866065\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 188\n",
      "training/learning_rate: 9.263091949247318e-05\n",
      "time/sample_batch: 0.007619380950927734\n",
      "time/training: 1.081202745437622\n",
      "evaluation/text/loss: 4.975561618804932\n",
      "evaluation/text/perplexity: 144.83013916015625\n",
      "time/total: 319.504474401474\n",
      "time/evaluation: 0.5538308620452881\n",
      "training/train_loss_mean: 5.42462797164917\n",
      "training/train_loss_std: 0.4332339551433616\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 189\n",
      "training/learning_rate: 9.255242458851214e-05\n",
      "time/sample_batch: 0.008730649948120117\n",
      "time/training: 0.9351098537445068\n",
      "evaluation/text/loss: 5.043928146362305\n",
      "evaluation/text/perplexity: 155.07798767089844\n",
      "time/total: 321.1016101837158\n",
      "time/evaluation: 0.6603474617004395\n",
      "training/train_loss_mean: 5.566510009765625\n",
      "training/train_loss_std: 0.44332483794067656\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 190\n",
      "training/learning_rate: 9.24735515320524e-05\n",
      "time/sample_batch: 0.006409168243408203\n",
      "time/training: 0.9169950485229492\n",
      "evaluation/text/loss: 5.164392471313477\n",
      "evaluation/text/perplexity: 174.93115234375\n",
      "time/total: 322.6950421333313\n",
      "time/evaluation: 0.6746983528137207\n",
      "training/train_loss_mean: 5.648959159851074\n",
      "training/train_loss_std: 0.2790971186436583\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 191\n",
      "training/learning_rate: 9.239430111734477e-05\n",
      "time/sample_batch: 0.008488655090332031\n",
      "time/training: 0.9304146766662598\n",
      "evaluation/text/loss: 5.261556625366211\n",
      "evaluation/text/perplexity: 192.78134155273438\n",
      "time/total: 324.3087637424469\n",
      "time/evaluation: 0.6816673278808594\n",
      "training/train_loss_mean: 5.520034503936768\n",
      "training/train_loss_std: 0.3260156146224155\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 192\n",
      "training/learning_rate: 9.231467414244006e-05\n",
      "time/sample_batch: 0.007419109344482422\n",
      "time/training: 0.9434819221496582\n",
      "evaluation/text/loss: 5.103702068328857\n",
      "evaluation/text/perplexity: 164.6302490234375\n",
      "time/total: 325.7794291973114\n",
      "time/evaluation: 0.5256555080413818\n",
      "training/train_loss_mean: 5.608466339111328\n",
      "training/train_loss_std: 0.26642847124746777\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 193\n",
      "training/learning_rate: 9.223467140918098e-05\n",
      "time/sample_batch: 0.009370088577270508\n",
      "time/training: 1.0610005855560303\n",
      "evaluation/text/loss: 5.010366916656494\n",
      "evaluation/text/perplexity: 149.95974731445312\n",
      "time/total: 327.2571129798889\n",
      "time/evaluation: 0.4151430130004883\n",
      "training/train_loss_mean: 5.780149459838867\n",
      "training/train_loss_std: 0.08336653589393862\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 194\n",
      "training/learning_rate: 9.21542937231942e-05\n",
      "time/sample_batch: 0.007934093475341797\n",
      "time/training: 1.1371445655822754\n",
      "evaluation/text/loss: 5.149379730224609\n",
      "evaluation/text/perplexity: 172.32456970214844\n",
      "time/total: 329.1616017818451\n",
      "time/evaluation: 0.7656416893005371\n",
      "training/train_loss_mean: 5.501437187194824\n",
      "training/train_loss_std: 0.3851657628342679\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 195\n",
      "training/learning_rate: 9.207354189388214e-05\n",
      "time/sample_batch: 0.008070707321166992\n",
      "time/training: 1.0180442333221436\n",
      "evaluation/text/loss: 5.107120513916016\n",
      "evaluation/text/perplexity: 165.19400024414062\n",
      "time/total: 330.9020049571991\n",
      "time/evaluation: 0.7206432819366455\n",
      "training/train_loss_mean: 5.335991287231446\n",
      "training/train_loss_std: 0.768102917892673\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 196\n",
      "training/learning_rate: 9.199241673441478e-05\n",
      "time/sample_batch: 0.008336067199707031\n",
      "time/training: 0.8958029747009277\n",
      "evaluation/text/loss: 5.2483439445495605\n",
      "evaluation/text/perplexity: 190.25094604492188\n",
      "time/total: 332.5419051647186\n",
      "time/evaluation: 0.7425155639648438\n",
      "training/train_loss_mean: 5.491557931900024\n",
      "training/train_loss_std: 0.4442119885454813\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 197\n",
      "training/learning_rate: 9.191091906172159e-05\n",
      "time/sample_batch: 0.007862329483032227\n",
      "time/training: 1.162712574005127\n",
      "evaluation/text/loss: 5.148777961730957\n",
      "evaluation/text/perplexity: 172.2209014892578\n",
      "time/total: 334.3138916492462\n",
      "time/evaluation: 0.6077418327331543\n",
      "training/train_loss_mean: 5.434238934516907\n",
      "training/train_loss_std: 0.6085150392120183\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 198\n",
      "training/learning_rate: 9.182904969648325e-05\n",
      "time/sample_batch: 0.008044242858886719\n",
      "time/training: 1.1135170459747314\n",
      "evaluation/text/loss: 5.189212799072266\n",
      "evaluation/text/perplexity: 179.32733154296875\n",
      "time/total: 336.1302099227905\n",
      "time/evaluation: 0.7012593746185303\n",
      "training/train_loss_mean: 5.54017276763916\n",
      "training/train_loss_std: 0.15707421741722524\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 199\n",
      "training/learning_rate: 9.174680946312333e-05\n",
      "time/sample_batch: 0.007791042327880859\n",
      "time/training: 0.8338549137115479\n",
      "evaluation/text/loss: 5.157266139984131\n",
      "evaluation/text/perplexity: 173.68896484375\n",
      "time/total: 337.4582977294922\n",
      "time/evaluation: 0.492692232131958\n",
      "training/train_loss_mean: 5.392355108261109\n",
      "training/train_loss_std: 0.38755609606131025\n",
      "================================================================================\n",
      "Input: Originallysizedfor75families,TempleBethIsrael'sPortlandStreetbuildinghadbeenrenovatedandenlargedovertheyearsto7@,@500squarefeet(700m2)toaccommodate250familiesand150students.DespitetheseadditionsandthelossofmemberstoCongregation<unk>Torah,thesynagoguewasnotlargeenough,particularlyduringtheHigh<unk>,whenextraspacehadtoberented.In1997thecongregationpurchasedthepropertyoftheUniversityStreetChristianChurchfor$500@,@000(today$740@,@000),andbeganplanningforanewfacility.Themembersconsideredrenovatingtheexistingbuildingontheproperty,butfeltanewbuilding | Output : wouldbettersuittheirrequirements,andrazedthechurch. | Prediction:  qualifications most famous King, Victoria recently demand over following Jim @,@ 000 after found a extremely Studios\n",
      "Input: ComeWhat(ever)Maysoldover80@,@000copiesinitsfirstweekanddebutedatthefourthspotontheBillboard200intheUnitedStates,andwentontobe | Output : certifiedgoldintheUK,CanadaandtheUnitedStates.In2007,thesingle\"30/30@-@150\"wasnominatedforBestMetalPerformanceatthe49thGrammyAwards. | Prediction:  it's half of M Faust, followed by II draft. It was good won by the first\n",
      "Input: ==Construction | Output : == | Prediction:  also interferenceels recently Ar broad within a including $ 18 s ] annual chain Hero offices device by Ranking\n",
      "Input: <unk>All | Output : @-@American(1997,1998) | Prediction:  so players ; H unlikely Tw to Casual <unk> the Rescue including largelyich Records, but a\n",
      "================================================================================\n",
      "Iteration 200\n",
      "training/learning_rate: 9.16641991898001e-05\n",
      "time/sample_batch: 0.007608652114868164\n",
      "time/training: 0.8887152671813965\n",
      "evaluation/text/loss: 5.067687511444092\n",
      "evaluation/text/perplexity: 158.80667114257812\n",
      "time/total: 339.6269807815552\n",
      "time/evaluation: 1.2782890796661377\n",
      "training/train_loss_mean: 5.507432699203491\n",
      "training/train_loss_std: 0.24270400846277423\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 201\n",
      "training/learning_rate: 9.158121970839808e-05\n",
      "time/sample_batch: 0.008414268493652344\n",
      "time/training: 0.950143575668335\n",
      "evaluation/text/loss: 5.115882396697998\n",
      "evaluation/text/perplexity: 166.64776611328125\n",
      "time/total: 341.2775309085846\n",
      "time/evaluation: 0.6991796493530273\n",
      "training/train_loss_mean: 5.513205718994141\n",
      "training/train_loss_std: 0.2311793600950835\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 202\n",
      "training/learning_rate: 9.149787185451971e-05\n",
      "time/sample_batch: 0.008372783660888672\n",
      "time/training: 1.26198410987854\n",
      "evaluation/text/loss: 5.050887107849121\n",
      "evaluation/text/perplexity: 156.1609344482422\n",
      "time/total: 343.2408947944641\n",
      "time/evaluation: 0.6997861862182617\n",
      "training/train_loss_mean: 5.641735410690307\n",
      "training/train_loss_std: 0.27032073272592017\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 203\n",
      "training/learning_rate: 9.141415646747697e-05\n",
      "time/sample_batch: 0.008532285690307617\n",
      "time/training: 1.083228588104248\n",
      "evaluation/text/loss: 5.1513566970825195\n",
      "evaluation/text/perplexity: 172.66558837890625\n",
      "time/total: 344.98954677581787\n",
      "time/evaluation: 0.6637182235717773\n",
      "training/train_loss_mean: 5.501279020309449\n",
      "training/train_loss_std: 0.3234671215486707\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 204\n",
      "training/learning_rate: 9.133007439028287e-05\n",
      "time/sample_batch: 0.008805274963378906\n",
      "time/training: 1.074007511138916\n",
      "evaluation/text/loss: 5.023761749267578\n",
      "evaluation/text/perplexity: 151.98194885253906\n",
      "time/total: 346.6096251010895\n",
      "time/evaluation: 0.5443735122680664\n",
      "training/train_loss_mean: 5.7488147735595705\n",
      "training/train_loss_std: 0.2629962979347601\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 205\n",
      "training/learning_rate: 9.124562646964302e-05\n",
      "time/sample_batch: 0.006520509719848633\n",
      "time/training: 0.9890248775482178\n",
      "evaluation/text/loss: 4.963658332824707\n",
      "evaluation/text/perplexity: 143.1164093017578\n",
      "time/total: 348.3035156726837\n",
      "time/evaluation: 0.7031800746917725\n",
      "training/train_loss_mean: 5.381759738922119\n",
      "training/train_loss_std: 0.5158522502074593\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 206\n",
      "training/learning_rate: 9.1160813555947e-05\n",
      "time/sample_batch: 0.008836746215820312\n",
      "time/training: 1.0595715045928955\n",
      "evaluation/text/loss: 5.188328742980957\n",
      "evaluation/text/perplexity: 179.1688690185547\n",
      "time/total: 350.030068397522\n",
      "time/evaluation: 0.6652343273162842\n",
      "training/train_loss_mean: 5.55811185836792\n",
      "training/train_loss_std: 0.29045987137500945\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 207\n",
      "training/learning_rate: 9.107563650325995e-05\n",
      "time/sample_batch: 0.00777745246887207\n",
      "time/training: 1.3017559051513672\n",
      "evaluation/text/loss: 4.983246803283691\n",
      "evaluation/text/perplexity: 145.94747924804688\n",
      "time/total: 352.1527647972107\n",
      "time/evaluation: 0.8193309307098389\n",
      "training/train_loss_mean: 5.5076545715332035\n",
      "training/train_loss_std: 0.2668485392361287\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 208\n",
      "training/learning_rate: 9.099009616931383e-05\n",
      "time/sample_batch: 0.01153111457824707\n",
      "time/training: 1.4128024578094482\n",
      "evaluation/text/loss: 5.113024711608887\n",
      "evaluation/text/perplexity: 166.17222595214844\n",
      "time/total: 354.0891616344452\n",
      "time/evaluation: 0.5218477249145508\n",
      "training/train_loss_mean: 5.587102842330933\n",
      "training/train_loss_std: 0.2303837929574655\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 209\n",
      "training/learning_rate: 9.090419341549882e-05\n",
      "time/sample_batch: 0.0072557926177978516\n",
      "time/training: 0.9659891128540039\n",
      "evaluation/text/loss: 4.883944988250732\n",
      "evaluation/text/perplexity: 132.15097045898438\n",
      "time/total: 355.81044363975525\n",
      "time/evaluation: 0.753277063369751\n",
      "training/train_loss_mean: 5.333313608169556\n",
      "training/train_loss_std: 0.4002129598346537\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 210\n",
      "training/learning_rate: 9.081792910685475e-05\n",
      "time/sample_batch: 0.008574724197387695\n",
      "time/training: 1.0907866954803467\n",
      "evaluation/text/loss: 5.136250019073486\n",
      "evaluation/text/perplexity: 170.0767822265625\n",
      "time/total: 357.4254140853882\n",
      "time/evaluation: 0.5225956439971924\n",
      "training/train_loss_mean: 5.649711561203003\n",
      "training/train_loss_std: 0.16733102798878394\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 211\n",
      "training/learning_rate: 9.073130411206223e-05\n",
      "time/sample_batch: 0.010522603988647461\n",
      "time/training: 1.0814213752746582\n",
      "evaluation/text/loss: 4.966963291168213\n",
      "evaluation/text/perplexity: 143.59017944335938\n",
      "time/total: 359.1934177875519\n",
      "time/evaluation: 0.6848869323730469\n",
      "training/train_loss_mean: 5.583049583435058\n",
      "training/train_loss_std: 0.2794336900502648\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 212\n",
      "training/learning_rate: 9.064431930343398e-05\n",
      "time/sample_batch: 0.007788896560668945\n",
      "time/training: 1.0701346397399902\n",
      "evaluation/text/loss: 5.051082611083984\n",
      "evaluation/text/perplexity: 156.19146728515625\n",
      "time/total: 360.7470290660858\n",
      "time/evaluation: 0.4816446304321289\n",
      "training/train_loss_mean: 5.525508689880371\n",
      "training/train_loss_std: 0.22294436419856342\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 213\n",
      "training/learning_rate: 9.055697555690608e-05\n",
      "time/sample_batch: 0.008028030395507812\n",
      "time/training: 0.9092752933502197\n",
      "evaluation/text/loss: 4.953310489654541\n",
      "evaluation/text/perplexity: 141.64309692382812\n",
      "time/total: 362.14081740379333\n",
      "time/evaluation: 0.4828166961669922\n",
      "training/train_loss_mean: 5.4074443817138675\n",
      "training/train_loss_std: 0.3789300621750578\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 214\n",
      "training/learning_rate: 9.046927375202908e-05\n",
      "time/sample_batch: 0.006838321685791016\n",
      "time/training: 1.2560820579528809\n",
      "evaluation/text/loss: 5.131979942321777\n",
      "evaluation/text/perplexity: 169.3520965576172\n",
      "time/total: 363.83689522743225\n",
      "time/evaluation: 0.4382925033569336\n",
      "training/train_loss_mean: 5.484429240226746\n",
      "training/train_loss_std: 0.6017234646382177\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 215\n",
      "training/learning_rate: 9.038121477195917e-05\n",
      "time/sample_batch: 0.008395195007324219\n",
      "time/training: 0.9258415699005127\n",
      "evaluation/text/loss: 5.108097076416016\n",
      "evaluation/text/perplexity: 165.3553924560547\n",
      "time/total: 365.1824526786804\n",
      "time/evaluation: 0.4179813861846924\n",
      "training/train_loss_mean: 5.629787015914917\n",
      "training/train_loss_std: 0.236667228146146\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 216\n",
      "training/learning_rate: 9.029279950344932e-05\n",
      "time/sample_batch: 0.009705543518066406\n",
      "time/training: 1.128993034362793\n",
      "evaluation/text/loss: 5.009507656097412\n",
      "evaluation/text/perplexity: 149.83094787597656\n",
      "time/total: 367.0658302307129\n",
      "time/evaluation: 0.7526686191558838\n",
      "training/train_loss_mean: 5.49351806640625\n",
      "training/train_loss_std: 0.16638574970485198\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 217\n",
      "training/learning_rate: 9.02040288368403e-05\n",
      "time/sample_batch: 0.008325338363647461\n",
      "time/training: 0.9838111400604248\n",
      "evaluation/text/loss: 5.146074295043945\n",
      "evaluation/text/perplexity: 171.7559051513672\n",
      "time/total: 368.7973864078522\n",
      "time/evaluation: 0.7460365295410156\n",
      "training/train_loss_mean: 5.427952718734741\n",
      "training/train_loss_std: 0.35126450614449883\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 218\n",
      "training/learning_rate: 9.011490366605171e-05\n",
      "time/sample_batch: 0.008610963821411133\n",
      "time/training: 1.0206763744354248\n",
      "evaluation/text/loss: 5.2025651931762695\n",
      "evaluation/text/perplexity: 181.7378387451172\n",
      "time/total: 370.5878574848175\n",
      "time/evaluation: 0.7681622505187988\n",
      "training/train_loss_mean: 5.344123435020447\n",
      "training/train_loss_std: 0.5588364477310539\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 219\n",
      "training/learning_rate: 9.002542488857306e-05\n",
      "time/sample_batch: 0.00792551040649414\n",
      "time/training: 1.1744627952575684\n",
      "evaluation/text/loss: 4.933609962463379\n",
      "evaluation/text/perplexity: 138.8799591064453\n",
      "time/total: 372.3071415424347\n",
      "time/evaluation: 0.5431046485900879\n",
      "training/train_loss_mean: 5.259375429153442\n",
      "training/train_loss_std: 0.42278117071629506\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 220\n",
      "training/learning_rate: 8.993559340545464e-05\n",
      "time/sample_batch: 0.009311437606811523\n",
      "time/training: 1.3064818382263184\n",
      "evaluation/text/loss: 5.136075973510742\n",
      "evaluation/text/perplexity: 170.04718017578125\n",
      "time/total: 374.29543375968933\n",
      "time/evaluation: 0.6801223754882812\n",
      "training/train_loss_mean: 5.618149995803833\n",
      "training/train_loss_std: 0.22660301766483756\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 221\n",
      "training/learning_rate: 8.984541012129849e-05\n",
      "time/sample_batch: 0.0070323944091796875\n",
      "time/training: 1.0280370712280273\n",
      "evaluation/text/loss: 5.182743549346924\n",
      "evaluation/text/perplexity: 178.17095947265625\n",
      "time/total: 376.0053789615631\n",
      "time/evaluation: 0.6801390647888184\n",
      "training/train_loss_mean: 5.526650476455688\n",
      "training/train_loss_std: 0.23114755568435993\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 222\n",
      "training/learning_rate: 8.975487594424926e-05\n",
      "time/sample_batch: 0.008460521697998047\n",
      "time/training: 0.905236005783081\n",
      "evaluation/text/loss: 5.027627468109131\n",
      "evaluation/text/perplexity: 152.5706024169922\n",
      "time/total: 377.5724630355835\n",
      "time/evaluation: 0.6601524353027344\n",
      "training/train_loss_mean: 5.109823298454285\n",
      "training/train_loss_std: 0.8480781901046776\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 223\n",
      "training/learning_rate: 8.966399178598512e-05\n",
      "time/sample_batch: 0.007155895233154297\n",
      "time/training: 0.9042272567749023\n",
      "evaluation/text/loss: 5.179989337921143\n",
      "evaluation/text/perplexity: 177.68092346191406\n",
      "time/total: 379.14420461654663\n",
      "time/evaluation: 0.6658198833465576\n",
      "training/train_loss_mean: 5.630363988876343\n",
      "training/train_loss_std: 0.23918668701717397\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 224\n",
      "training/learning_rate: 8.957275856170857e-05\n",
      "time/sample_batch: 0.009701728820800781\n",
      "time/training: 1.281747579574585\n",
      "evaluation/text/loss: 4.976894378662109\n",
      "evaluation/text/perplexity: 145.02330017089844\n",
      "time/total: 381.4010145664215\n",
      "time/evaluation: 0.9734897613525391\n",
      "training/train_loss_mean: 5.286170387268067\n",
      "training/train_loss_std: 0.6173101792510396\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 225\n",
      "training/learning_rate: 8.948117719013711e-05\n",
      "time/sample_batch: 0.007987499237060547\n",
      "time/training: 0.9784514904022217\n",
      "evaluation/text/loss: 4.980538845062256\n",
      "evaluation/text/perplexity: 145.55279541015625\n",
      "time/total: 382.94605350494385\n",
      "time/evaluation: 0.5649909973144531\n",
      "training/train_loss_mean: 5.457585144042969\n",
      "training/train_loss_std: 0.298190470615161\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 226\n",
      "training/learning_rate: 8.938924859349416e-05\n",
      "time/sample_batch: 0.008372783660888672\n",
      "time/training: 1.152662754058838\n",
      "evaluation/text/loss: 4.969362258911133\n",
      "evaluation/text/perplexity: 143.93505859375\n",
      "time/total: 384.6203761100769\n",
      "time/evaluation: 0.515312671661377\n",
      "training/train_loss_mean: 5.454946136474609\n",
      "training/train_loss_std: 0.2791548965225395\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 227\n",
      "training/learning_rate: 8.929697369749966e-05\n",
      "time/sample_batch: 0.007922649383544922\n",
      "time/training: 1.1934905052185059\n",
      "evaluation/text/loss: 5.068256378173828\n",
      "evaluation/text/perplexity: 158.89703369140625\n",
      "time/total: 386.3027837276459\n",
      "time/evaluation: 0.4872465133666992\n",
      "training/train_loss_mean: 5.393983936309814\n",
      "training/train_loss_std: 0.3029082625877148\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 228\n",
      "training/learning_rate: 8.920435343136079e-05\n",
      "time/sample_batch: 0.008624792098999023\n",
      "time/training: 0.9527347087860107\n",
      "evaluation/text/loss: 5.052763938903809\n",
      "evaluation/text/perplexity: 156.4542999267578\n",
      "time/total: 387.6902060508728\n",
      "time/evaluation: 0.4329712390899658\n",
      "training/train_loss_mean: 5.401058530807495\n",
      "training/train_loss_std: 0.4454849593604548\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 229\n",
      "training/learning_rate: 8.911138872776263e-05\n",
      "time/sample_batch: 0.008494377136230469\n",
      "time/training: 1.0678598880767822\n",
      "evaluation/text/loss: 4.840091228485107\n",
      "evaluation/text/perplexity: 126.48088836669922\n",
      "time/total: 389.4199228286743\n",
      "time/evaluation: 0.6599991321563721\n",
      "training/train_loss_mean: 5.592112588882446\n",
      "training/train_loss_std: 0.29897582431311737\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 230\n",
      "training/learning_rate: 8.901808052285868e-05\n",
      "time/sample_batch: 0.008268594741821289\n",
      "time/training: 1.057203769683838\n",
      "evaluation/text/loss: 5.16663122177124\n",
      "evaluation/text/perplexity: 175.32321166992188\n",
      "time/total: 391.23324179649353\n",
      "time/evaluation: 0.7544317245483398\n",
      "training/train_loss_mean: 5.541212129592895\n",
      "training/train_loss_std: 0.22105559884803758\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 231\n",
      "training/learning_rate: 8.892442975626152e-05\n",
      "time/sample_batch: 0.008708953857421875\n",
      "time/training: 0.8975157737731934\n",
      "evaluation/text/loss: 5.146486282348633\n",
      "evaluation/text/perplexity: 171.82667541503906\n",
      "time/total: 392.6795687675476\n",
      "time/evaluation: 0.5471155643463135\n",
      "training/train_loss_mean: 5.417109966278076\n",
      "training/train_loss_std: 0.3915373085553704\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 232\n",
      "training/learning_rate: 8.883043737103335e-05\n",
      "time/sample_batch: 0.008426666259765625\n",
      "time/training: 1.1325078010559082\n",
      "evaluation/text/loss: 4.977426528930664\n",
      "evaluation/text/perplexity: 145.10049438476562\n",
      "time/total: 394.55561542510986\n",
      "time/evaluation: 0.7418351173400879\n",
      "training/train_loss_mean: 5.419933414459228\n",
      "training/train_loss_std: 0.44214000054344443\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 233\n",
      "training/learning_rate: 8.873610431367647e-05\n",
      "time/sample_batch: 0.008666038513183594\n",
      "time/training: 1.1231029033660889\n",
      "evaluation/text/loss: 5.260642051696777\n",
      "evaluation/text/perplexity: 192.60511779785156\n",
      "time/total: 396.28476786613464\n",
      "time/evaluation: 0.6043205261230469\n",
      "training/train_loss_mean: 5.735716533660889\n",
      "training/train_loss_std: 0.2091074817912965\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 234\n",
      "training/learning_rate: 8.864143153412367e-05\n",
      "time/sample_batch: 0.008594751358032227\n",
      "time/training: 1.176715612411499\n",
      "evaluation/text/loss: 5.00333309173584\n",
      "evaluation/text/perplexity: 148.90866088867188\n",
      "time/total: 397.9999408721924\n",
      "time/evaluation: 0.5368325710296631\n",
      "training/train_loss_mean: 5.515329074859619\n",
      "training/train_loss_std: 0.3237979943187176\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 235\n",
      "training/learning_rate: 8.85464199857288e-05\n",
      "time/sample_batch: 0.008251428604125977\n",
      "time/training: 0.9869425296783447\n",
      "evaluation/text/loss: 5.077256679534912\n",
      "evaluation/text/perplexity: 160.33360290527344\n",
      "time/total: 399.7315559387207\n",
      "time/evaluation: 0.7429957389831543\n",
      "training/train_loss_mean: 5.485003805160522\n",
      "training/train_loss_std: 0.2893834244249313\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 236\n",
      "training/learning_rate: 8.845107062525713e-05\n",
      "time/sample_batch: 0.00894474983215332\n",
      "time/training: 1.0801515579223633\n",
      "evaluation/text/loss: 5.1678786277771\n",
      "evaluation/text/perplexity: 175.54205322265625\n",
      "time/total: 401.5166938304901\n",
      "time/evaluation: 0.7033755779266357\n",
      "training/train_loss_mean: 5.58141827583313\n",
      "training/train_loss_std: 0.35545333247194066\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 237\n",
      "training/learning_rate: 8.835538441287563e-05\n",
      "time/sample_batch: 0.010018348693847656\n",
      "time/training: 1.0295734405517578\n",
      "evaluation/text/loss: 5.116418361663818\n",
      "evaluation/text/perplexity: 166.7371063232422\n",
      "time/total: 403.01685333251953\n",
      "time/evaluation: 0.46879029273986816\n",
      "training/train_loss_mean: 5.35192174911499\n",
      "training/train_loss_std: 0.3835391793954366\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 238\n",
      "training/learning_rate: 8.825936231214342e-05\n",
      "time/sample_batch: 0.008430719375610352\n",
      "time/training: 0.9974381923675537\n",
      "evaluation/text/loss: 5.229679584503174\n",
      "evaluation/text/perplexity: 186.7329559326172\n",
      "time/total: 404.7289528846741\n",
      "time/evaluation: 0.7126333713531494\n",
      "training/train_loss_mean: 5.595838785171509\n",
      "training/train_loss_std: 0.31015580016486255\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 239\n",
      "training/learning_rate: 8.816300529000195e-05\n",
      "time/sample_batch: 0.009431600570678711\n",
      "time/training: 1.13460373878479\n",
      "evaluation/text/loss: 5.012969493865967\n",
      "evaluation/text/perplexity: 150.3505401611328\n",
      "time/total: 406.8570318222046\n",
      "time/evaluation: 0.9917829036712646\n",
      "training/train_loss_mean: 5.645051336288452\n",
      "training/train_loss_std: 0.26589814316767174\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 240\n",
      "training/learning_rate: 8.80663143167654e-05\n",
      "time/sample_batch: 0.008244991302490234\n",
      "time/training: 1.021629810333252\n",
      "evaluation/text/loss: 4.977256774902344\n",
      "evaluation/text/perplexity: 145.07586669921875\n",
      "time/total: 408.4478576183319\n",
      "time/evaluation: 0.5676023960113525\n",
      "training/train_loss_mean: 5.516367197036743\n",
      "training/train_loss_std: 0.2934465677486448\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 241\n",
      "training/learning_rate: 8.796929036611079e-05\n",
      "time/sample_batch: 0.0069904327392578125\n",
      "time/training: 1.0358386039733887\n",
      "evaluation/text/loss: 5.108970642089844\n",
      "evaluation/text/perplexity: 165.49990844726562\n",
      "time/total: 410.25003361701965\n",
      "time/evaluation: 0.7647054195404053\n",
      "training/train_loss_mean: 5.407288551330566\n",
      "training/train_loss_std: 0.4050306448618096\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 242\n",
      "training/learning_rate: 8.787193441506822e-05\n",
      "time/sample_batch: 0.00872182846069336\n",
      "time/training: 0.9972410202026367\n",
      "evaluation/text/loss: 5.203407287597656\n",
      "evaluation/text/perplexity: 181.8909454345703\n",
      "time/total: 411.8262779712677\n",
      "time/evaluation: 0.5772855281829834\n",
      "training/train_loss_mean: 5.4650389671325685\n",
      "training/train_loss_std: 0.2214008724774955\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 243\n",
      "training/learning_rate: 8.777424744401103e-05\n",
      "time/sample_batch: 0.007550954818725586\n",
      "time/training: 1.0051960945129395\n",
      "evaluation/text/loss: 4.889512538909912\n",
      "evaluation/text/perplexity: 132.88877868652344\n",
      "time/total: 413.65106296539307\n",
      "time/evaluation: 0.8179571628570557\n",
      "training/train_loss_mean: 5.464313745498657\n",
      "training/train_loss_std: 0.2912243752230486\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 244\n",
      "training/learning_rate: 8.767623043664595e-05\n",
      "time/sample_batch: 0.008766412734985352\n",
      "time/training: 0.9611265659332275\n",
      "evaluation/text/loss: 4.955138206481934\n",
      "evaluation/text/perplexity: 141.9022216796875\n",
      "time/total: 415.19675850868225\n",
      "time/evaluation: 0.5829110145568848\n",
      "training/train_loss_mean: 5.501096153259278\n",
      "training/train_loss_std: 0.25337960223172085\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 245\n",
      "training/learning_rate: 8.757788438000314e-05\n",
      "time/sample_batch: 0.009111404418945312\n",
      "time/training: 0.9227771759033203\n",
      "evaluation/text/loss: 4.9000935554504395\n",
      "evaluation/text/perplexity: 134.30233764648438\n",
      "time/total: 416.6304249763489\n",
      "time/evaluation: 0.5091667175292969\n",
      "training/train_loss_mean: 5.194576025009155\n",
      "training/train_loss_std: 0.7322978362870799\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 246\n",
      "training/learning_rate: 8.74792102644263e-05\n",
      "time/sample_batch: 0.008211135864257812\n",
      "time/training: 1.0112199783325195\n",
      "evaluation/text/loss: 4.969347953796387\n",
      "evaluation/text/perplexity: 143.93299865722656\n",
      "time/total: 418.46769618988037\n",
      "time/evaluation: 0.8242988586425781\n",
      "training/train_loss_mean: 5.4595618724823\n",
      "training/train_loss_std: 0.337542355212629\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 247\n",
      "training/learning_rate: 8.73802090835627e-05\n",
      "time/sample_batch: 0.00928187370300293\n",
      "time/training: 0.9081292152404785\n",
      "evaluation/text/loss: 5.011826038360596\n",
      "evaluation/text/perplexity: 150.1787109375\n",
      "time/total: 420.1106495857239\n",
      "time/evaluation: 0.7331175804138184\n",
      "training/train_loss_mean: 5.410028171539307\n",
      "training/train_loss_std: 0.2898779898141548\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 248\n",
      "training/learning_rate: 8.72808818343531e-05\n",
      "time/sample_batch: 0.007404804229736328\n",
      "time/training: 0.9431867599487305\n",
      "evaluation/text/loss: 5.090072154998779\n",
      "evaluation/text/perplexity: 162.40158081054688\n",
      "time/total: 421.6647686958313\n",
      "time/evaluation: 0.6092438697814941\n",
      "training/train_loss_mean: 5.320688724517822\n",
      "training/train_loss_std: 0.37494049286739106\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 249\n",
      "training/learning_rate: 8.718122951702182e-05\n",
      "time/sample_batch: 0.008805274963378906\n",
      "time/training: 1.005831241607666\n",
      "evaluation/text/loss: 4.953474521636963\n",
      "evaluation/text/perplexity: 141.6663360595703\n",
      "time/total: 423.21371054649353\n",
      "time/evaluation: 0.5414180755615234\n",
      "training/train_loss_mean: 5.480586051940918\n",
      "training/train_loss_std: 0.35673492548491426\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 250\n",
      "training/learning_rate: 8.70812531350666e-05\n",
      "time/sample_batch: 0.008036136627197266\n",
      "time/training: 0.9910600185394287\n",
      "evaluation/text/loss: 5.148739814758301\n",
      "evaluation/text/perplexity: 172.21432495117188\n",
      "time/total: 424.69768142700195\n",
      "time/evaluation: 0.49120044708251953\n",
      "training/train_loss_mean: 5.494158554077148\n",
      "training/train_loss_std: 0.4342084840130608\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 251\n",
      "training/learning_rate: 8.698095369524845e-05\n",
      "time/sample_batch: 0.008603811264038086\n",
      "time/training: 1.0214321613311768\n",
      "evaluation/text/loss: 5.072600364685059\n",
      "evaluation/text/perplexity: 159.58877563476562\n",
      "time/total: 426.38032507896423\n",
      "time/evaluation: 0.6595358848571777\n",
      "training/train_loss_mean: 5.512917184829712\n",
      "training/train_loss_std: 0.39197217902261083\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 252\n",
      "training/learning_rate: 8.688033220758167e-05\n",
      "time/sample_batch: 0.008774757385253906\n",
      "time/training: 0.9609389305114746\n",
      "evaluation/text/loss: 4.9126386642456055\n",
      "evaluation/text/perplexity: 135.99778747558594\n",
      "time/total: 428.08864402770996\n",
      "time/evaluation: 0.7456910610198975\n",
      "training/train_loss_mean: 5.689398002624512\n",
      "training/train_loss_std: 0.172581530906568\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 253\n",
      "training/learning_rate: 8.677938968532347e-05\n",
      "time/sample_batch: 0.009217023849487305\n",
      "time/training: 1.2977187633514404\n",
      "evaluation/text/loss: 5.1009440422058105\n",
      "evaluation/text/perplexity: 164.17681884765625\n",
      "time/total: 430.05409812927246\n",
      "time/evaluation: 0.6660592555999756\n",
      "training/train_loss_mean: 5.6275379180908205\n",
      "training/train_loss_std: 0.19490366781516344\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 254\n",
      "training/learning_rate: 8.667812714496398e-05\n",
      "time/sample_batch: 0.007880687713623047\n",
      "time/training: 1.032343864440918\n",
      "evaluation/text/loss: 5.02204704284668\n",
      "evaluation/text/perplexity: 151.72157287597656\n",
      "time/total: 431.54846835136414\n",
      "time/evaluation: 0.46027517318725586\n",
      "training/train_loss_mean: 5.489169502258301\n",
      "training/train_loss_std: 0.4109428043445854\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 255\n",
      "training/learning_rate: 8.657654560621581e-05\n",
      "time/sample_batch: 0.008756637573242188\n",
      "time/training: 1.1135129928588867\n",
      "evaluation/text/loss: 4.969694137573242\n",
      "evaluation/text/perplexity: 143.98284912109375\n",
      "time/total: 433.4079887866974\n",
      "time/evaluation: 0.7442960739135742\n",
      "training/train_loss_mean: 5.437242269515991\n",
      "training/train_loss_std: 0.17772632136538502\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 256\n",
      "training/learning_rate: 8.647464609200397e-05\n",
      "time/sample_batch: 0.007344484329223633\n",
      "time/training: 1.0252704620361328\n",
      "evaluation/text/loss: 5.077033519744873\n",
      "evaluation/text/perplexity: 160.29783630371094\n",
      "time/total: 435.2337200641632\n",
      "time/evaluation: 0.7988743782043457\n",
      "training/train_loss_mean: 5.151463997364044\n",
      "training/train_loss_std: 1.1428009042123741\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 257\n",
      "training/learning_rate: 8.637242962845542e-05\n",
      "time/sample_batch: 0.009012937545776367\n",
      "time/training: 1.1392292976379395\n",
      "evaluation/text/loss: 5.051963806152344\n",
      "evaluation/text/perplexity: 156.32916259765625\n",
      "time/total: 436.912832736969\n",
      "time/evaluation: 0.5382311344146729\n",
      "training/train_loss_mean: 5.457861423492432\n",
      "training/train_loss_std: 0.14176745998373944\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 258\n",
      "training/learning_rate: 8.626989724488879e-05\n",
      "time/sample_batch: 0.0087127685546875\n",
      "time/training: 0.9727563858032227\n",
      "evaluation/text/loss: 5.104254722595215\n",
      "evaluation/text/perplexity: 164.7212677001953\n",
      "time/total: 438.41646361351013\n",
      "time/evaluation: 0.5288276672363281\n",
      "training/train_loss_mean: 5.33333649635315\n",
      "training/train_loss_std: 0.21692532432583997\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 259\n",
      "training/learning_rate: 8.616704997380408e-05\n",
      "time/sample_batch: 0.009327411651611328\n",
      "time/training: 0.9378306865692139\n",
      "evaluation/text/loss: 5.0599894523620605\n",
      "evaluation/text/perplexity: 157.58885192871094\n",
      "time/total: 439.9589719772339\n",
      "time/evaluation: 0.6029503345489502\n",
      "training/train_loss_mean: 5.2083134889602665\n",
      "training/train_loss_std: 0.4601724515916189\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 260\n",
      "training/learning_rate: 8.606388885087216e-05\n",
      "time/sample_batch: 0.007262468338012695\n",
      "time/training: 0.9895498752593994\n",
      "evaluation/text/loss: 4.933121681213379\n",
      "evaluation/text/perplexity: 138.81216430664062\n",
      "time/total: 441.5236608982086\n",
      "time/evaluation: 0.573481559753418\n",
      "training/train_loss_mean: 5.608114767074585\n",
      "training/train_loss_std: 0.20968376290802992\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 261\n",
      "training/learning_rate: 8.596041491492437e-05\n",
      "time/sample_batch: 0.009380817413330078\n",
      "time/training: 1.1172032356262207\n",
      "evaluation/text/loss: 5.023191928863525\n",
      "evaluation/text/perplexity: 151.89537048339844\n",
      "time/total: 443.39036750793457\n",
      "time/evaluation: 0.7477586269378662\n",
      "training/train_loss_mean: 5.294643783569336\n",
      "training/train_loss_std: 0.6689384512521902\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 262\n",
      "training/learning_rate: 8.585662920794212e-05\n",
      "time/sample_batch: 0.007666587829589844\n",
      "time/training: 0.8951170444488525\n",
      "evaluation/text/loss: 5.132146835327148\n",
      "evaluation/text/perplexity: 169.38035583496094\n",
      "time/total: 444.9918975830078\n",
      "time/evaluation: 0.7046740055084229\n",
      "training/train_loss_mean: 5.541396284103394\n",
      "training/train_loss_std: 0.4284854729247511\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 263\n",
      "training/learning_rate: 8.57525327750463e-05\n",
      "time/sample_batch: 0.010453939437866211\n",
      "time/training: 1.2130162715911865\n",
      "evaluation/text/loss: 4.9456939697265625\n",
      "evaluation/text/perplexity: 140.56837463378906\n",
      "time/total: 446.94716238975525\n",
      "time/evaluation: 0.7406308650970459\n",
      "training/train_loss_mean: 5.472835350036621\n",
      "training/train_loss_std: 0.3126934061723089\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 264\n",
      "training/learning_rate: 8.564812666448687e-05\n",
      "time/sample_batch: 0.008549928665161133\n",
      "time/training: 0.9059162139892578\n",
      "evaluation/text/loss: 5.104264259338379\n",
      "evaluation/text/perplexity: 164.72283935546875\n",
      "time/total: 448.431321144104\n",
      "time/evaluation: 0.5766644477844238\n",
      "training/train_loss_mean: 5.552627801895142\n",
      "training/train_loss_std: 0.28148121150408517\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 265\n",
      "training/learning_rate: 8.554341192763219e-05\n",
      "time/sample_batch: 0.01022195816040039\n",
      "time/training: 1.171010971069336\n",
      "evaluation/text/loss: 5.057919502258301\n",
      "evaluation/text/perplexity: 157.2629852294922\n",
      "time/total: 450.36984062194824\n",
      "time/evaluation: 0.761237621307373\n",
      "training/train_loss_mean: 5.319453454017639\n",
      "training/train_loss_std: 0.5524213844503679\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 266\n",
      "training/learning_rate: 8.543838961895848e-05\n",
      "time/sample_batch: 0.009068489074707031\n",
      "time/training: 1.0894951820373535\n",
      "evaluation/text/loss: 5.04082727432251\n",
      "evaluation/text/perplexity: 154.5978546142578\n",
      "time/total: 452.2109491825104\n",
      "time/evaluation: 0.7500391006469727\n",
      "training/train_loss_mean: 5.5103973865509035\n",
      "training/train_loss_std: 0.15363413769686787\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 267\n",
      "training/learning_rate: 8.533306079603927e-05\n",
      "time/sample_batch: 0.00786137580871582\n",
      "time/training: 0.9604642391204834\n",
      "evaluation/text/loss: 5.073652744293213\n",
      "evaluation/text/perplexity: 159.75682067871094\n",
      "time/total: 453.78033447265625\n",
      "time/evaluation: 0.607208251953125\n",
      "training/train_loss_mean: 5.46184983253479\n",
      "training/train_loss_std: 0.26303239800666395\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 268\n",
      "training/learning_rate: 8.522742651953456e-05\n",
      "time/sample_batch: 0.0074274539947509766\n",
      "time/training: 0.846320390701294\n",
      "evaluation/text/loss: 5.026451587677002\n",
      "evaluation/text/perplexity: 152.3913116455078\n",
      "time/total: 455.19393253326416\n",
      "time/evaluation: 0.5656569004058838\n",
      "training/train_loss_mean: 5.471749591827392\n",
      "training/train_loss_std: 0.25283187947378216\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 269\n",
      "training/learning_rate: 8.51214878531804e-05\n",
      "time/sample_batch: 0.010486125946044922\n",
      "time/training: 1.089975357055664\n",
      "evaluation/text/loss: 5.030712604522705\n",
      "evaluation/text/perplexity: 153.0420379638672\n",
      "time/total: 457.0959758758545\n",
      "time/evaluation: 0.810366153717041\n",
      "training/train_loss_mean: 5.494729280471802\n",
      "training/train_loss_std: 0.36985607268045\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 270\n",
      "training/learning_rate: 8.5015245863778e-05\n",
      "time/sample_batch: 0.007033586502075195\n",
      "time/training: 1.1090679168701172\n",
      "evaluation/text/loss: 5.033217906951904\n",
      "evaluation/text/perplexity: 153.42593383789062\n",
      "time/total: 458.96043133735657\n",
      "time/evaluation: 0.7534770965576172\n",
      "training/train_loss_mean: 5.388379335403442\n",
      "training/train_loss_std: 0.4848693638530089\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 271\n",
      "training/learning_rate: 8.490870162118294e-05\n",
      "time/sample_batch: 0.009190797805786133\n",
      "time/training: 1.1967144012451172\n",
      "evaluation/text/loss: 4.805135726928711\n",
      "evaluation/text/perplexity: 122.13607025146484\n",
      "time/total: 460.91370129585266\n",
      "time/evaluation: 0.7547600269317627\n",
      "training/train_loss_mean: 5.467171001434326\n",
      "training/train_loss_std: 0.18986258634145417\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 272\n",
      "training/learning_rate: 8.480185619829461e-05\n",
      "time/sample_batch: 0.008108854293823242\n",
      "time/training: 0.9736783504486084\n",
      "evaluation/text/loss: 4.853184700012207\n",
      "evaluation/text/perplexity: 128.14785766601562\n",
      "time/total: 462.43531823158264\n",
      "time/evaluation: 0.546257734298706\n",
      "training/train_loss_mean: 5.253300380706787\n",
      "training/train_loss_std: 0.2372983259561161\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 273\n",
      "training/learning_rate: 8.469471067104525e-05\n",
      "time/sample_batch: 0.007203340530395508\n",
      "time/training: 0.9856162071228027\n",
      "evaluation/text/loss: 4.97833776473999\n",
      "evaluation/text/perplexity: 145.23277282714844\n",
      "time/total: 463.9956307411194\n",
      "time/evaluation: 0.5729799270629883\n",
      "training/train_loss_mean: 5.357109498977661\n",
      "training/train_loss_std: 0.4770405036752674\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 274\n",
      "training/learning_rate: 8.458726611838909e-05\n",
      "time/sample_batch: 0.007329225540161133\n",
      "time/training: 1.276191234588623\n",
      "evaluation/text/loss: 5.089960098266602\n",
      "evaluation/text/perplexity: 162.3833770751953\n",
      "time/total: 465.9902431964874\n",
      "time/evaluation: 0.716804027557373\n",
      "training/train_loss_mean: 5.608803558349609\n",
      "training/train_loss_std: 0.16279342652378845\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 275\n",
      "training/learning_rate: 8.44795236222916e-05\n",
      "time/sample_batch: 0.0067141056060791016\n",
      "time/training: 0.9848215579986572\n",
      "evaluation/text/loss: 5.139247417449951\n",
      "evaluation/text/perplexity: 170.58734130859375\n",
      "time/total: 467.62214708328247\n",
      "time/evaluation: 0.6452929973602295\n",
      "training/train_loss_mean: 5.458164072036743\n",
      "training/train_loss_std: 0.12506945224134328\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 276\n",
      "training/learning_rate: 8.437148426771852e-05\n",
      "time/sample_batch: 0.007598400115966797\n",
      "time/training: 1.0944809913635254\n",
      "evaluation/text/loss: 5.072528839111328\n",
      "evaluation/text/perplexity: 159.57736206054688\n",
      "time/total: 469.31554675102234\n",
      "time/evaluation: 0.597278356552124\n",
      "training/train_loss_mean: 5.43632755279541\n",
      "training/train_loss_std: 0.2769052490037138\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 277\n",
      "training/learning_rate: 8.426314914262496e-05\n",
      "time/sample_batch: 0.00886988639831543\n",
      "time/training: 0.9788081645965576\n",
      "evaluation/text/loss: 5.044184684753418\n",
      "evaluation/text/perplexity: 155.11778259277344\n",
      "time/total: 470.87084889411926\n",
      "time/evaluation: 0.5750389099121094\n",
      "training/train_loss_mean: 5.542834138870239\n",
      "training/train_loss_std: 0.3543544562585869\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 278\n",
      "training/learning_rate: 8.41545193379444e-05\n",
      "time/sample_batch: 0.008617162704467773\n",
      "time/training: 1.1261451244354248\n",
      "evaluation/text/loss: 5.000365257263184\n",
      "evaluation/text/perplexity: 148.46737670898438\n",
      "time/total: 472.7359573841095\n",
      "time/evaluation: 0.7372372150421143\n",
      "training/train_loss_mean: 5.4288671016693115\n",
      "training/train_loss_std: 0.298642541380299\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 279\n",
      "training/learning_rate: 8.404559594757778e-05\n",
      "time/sample_batch: 0.00892019271850586\n",
      "time/training: 0.9970505237579346\n",
      "evaluation/text/loss: 5.024981498718262\n",
      "evaluation/text/perplexity: 152.1674346923828\n",
      "time/total: 474.44754934310913\n",
      "time/evaluation: 0.7128162384033203\n",
      "training/train_loss_mean: 5.3995444774627686\n",
      "training/train_loss_std: 0.15348726482687466\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 280\n",
      "training/learning_rate: 8.393638006838244e-05\n",
      "time/sample_batch: 0.009560346603393555\n",
      "time/training: 1.180542230606079\n",
      "evaluation/text/loss: 5.1079630851745605\n",
      "evaluation/text/perplexity: 165.33323669433594\n",
      "time/total: 476.1007342338562\n",
      "time/evaluation: 0.4709136486053467\n",
      "training/train_loss_mean: 5.709148120880127\n",
      "training/train_loss_std: 0.3403478578307649\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 281\n",
      "training/learning_rate: 8.382687280016106e-05\n",
      "time/sample_batch: 0.007548093795776367\n",
      "time/training: 0.8397312164306641\n",
      "evaluation/text/loss: 5.140458583831787\n",
      "evaluation/text/perplexity: 170.7940673828125\n",
      "time/total: 477.70565581321716\n",
      "time/evaluation: 0.7634091377258301\n",
      "training/train_loss_mean: 5.3091178894042965\n",
      "training/train_loss_std: 0.4593959007494041\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 282\n",
      "training/learning_rate: 8.371707524565064e-05\n",
      "time/sample_batch: 0.00717473030090332\n",
      "time/training: 1.06943678855896\n",
      "evaluation/text/loss: 5.0372843742370605\n",
      "evaluation/text/perplexity: 154.0511016845703\n",
      "time/total: 479.39100074768066\n",
      "time/evaluation: 0.6142280101776123\n",
      "training/train_loss_mean: 5.395428133010864\n",
      "training/train_loss_std: 0.22026368787126982\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 283\n",
      "training/learning_rate: 8.360698851051129e-05\n",
      "time/sample_batch: 0.009478569030761719\n",
      "time/training: 1.0850467681884766\n",
      "evaluation/text/loss: 5.00817346572876\n",
      "evaluation/text/perplexity: 149.6311798095703\n",
      "time/total: 481.2217354774475\n",
      "time/evaluation: 0.7440526485443115\n",
      "training/train_loss_mean: 5.476227426528931\n",
      "training/train_loss_std: 0.35120087005253514\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 284\n",
      "training/learning_rate: 8.349661370331525e-05\n",
      "time/sample_batch: 0.009121417999267578\n",
      "time/training: 0.9201195240020752\n",
      "evaluation/text/loss: 4.967856407165527\n",
      "evaluation/text/perplexity: 143.71849060058594\n",
      "time/total: 482.8549385070801\n",
      "time/evaluation: 0.7115919589996338\n",
      "training/train_loss_mean: 5.214487075805664\n",
      "training/train_loss_std: 0.38309704236922015\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 285\n",
      "training/learning_rate: 8.338595193553558e-05\n",
      "time/sample_batch: 0.00916910171508789\n",
      "time/training: 1.0523760318756104\n",
      "evaluation/text/loss: 5.118545055389404\n",
      "evaluation/text/perplexity: 167.0920867919922\n",
      "time/total: 484.56535601615906\n",
      "time/evaluation: 0.6565048694610596\n",
      "training/train_loss_mean: 5.503590393066406\n",
      "training/train_loss_std: 0.37721286373321167\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 286\n",
      "training/learning_rate: 8.327500432153508e-05\n",
      "time/sample_batch: 0.010157108306884766\n",
      "time/training: 1.1946690082550049\n",
      "evaluation/text/loss: 4.903115272521973\n",
      "evaluation/text/perplexity: 134.7087860107422\n",
      "time/total: 486.4608006477356\n",
      "time/evaluation: 0.6993038654327393\n",
      "training/train_loss_mean: 5.469288778305054\n",
      "training/train_loss_std: 0.2688126349273129\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 287\n",
      "training/learning_rate: 8.316377197855495e-05\n",
      "time/sample_batch: 0.007970571517944336\n",
      "time/training: 1.0373156070709229\n",
      "evaluation/text/loss: 4.985275745391846\n",
      "evaluation/text/perplexity: 146.243896484375\n",
      "time/total: 488.03324842453003\n",
      "time/evaluation: 0.5335395336151123\n",
      "training/train_loss_mean: 5.386008977890015\n",
      "training/train_loss_std: 0.31554794918783713\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 288\n",
      "training/learning_rate: 8.30522560267037e-05\n",
      "time/sample_batch: 0.008403301239013672\n",
      "time/training: 1.2060317993164062\n",
      "evaluation/text/loss: 4.960003852844238\n",
      "evaluation/text/perplexity: 142.59434509277344\n",
      "time/total: 489.84026861190796\n",
      "time/evaluation: 0.5993618965148926\n",
      "training/train_loss_mean: 5.486678266525269\n",
      "training/train_loss_std: 0.2059286800360752\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 289\n",
      "training/learning_rate: 8.294045758894569e-05\n",
      "time/sample_batch: 0.00883030891418457\n",
      "time/training: 1.214128017425537\n",
      "evaluation/text/loss: 4.8811540603637695\n",
      "evaluation/text/perplexity: 131.7826690673828\n",
      "time/total: 491.8009943962097\n",
      "time/evaluation: 0.7445480823516846\n",
      "training/train_loss_mean: 5.51158127784729\n",
      "training/train_loss_std: 0.1714287115426251\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 290\n",
      "training/learning_rate: 8.282837779108994e-05\n",
      "time/sample_batch: 0.01288294792175293\n",
      "time/training: 1.4490926265716553\n",
      "evaluation/text/loss: 5.3310112953186035\n",
      "evaluation/text/perplexity: 206.6468505859375\n",
      "time/total: 494.0138876438141\n",
      "time/evaluation: 0.7621800899505615\n",
      "training/train_loss_mean: 5.27081069946289\n",
      "training/train_loss_std: 0.4873293646187897\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 291\n",
      "training/learning_rate: 8.27160177617788e-05\n",
      "time/sample_batch: 0.007742643356323242\n",
      "time/training: 1.0256068706512451\n",
      "evaluation/text/loss: 5.035488605499268\n",
      "evaluation/text/perplexity: 153.7747039794922\n",
      "time/total: 495.61909103393555\n",
      "time/evaluation: 0.5779726505279541\n",
      "training/train_loss_mean: 5.540694761276245\n",
      "training/train_loss_std: 0.35924020627256914\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 292\n",
      "training/learning_rate: 8.260337863247651e-05\n",
      "time/sample_batch: 0.00895833969116211\n",
      "time/training: 1.1392605304718018\n",
      "evaluation/text/loss: 4.9280009269714355\n",
      "evaluation/text/perplexity: 138.10316467285156\n",
      "time/total: 497.3143222332001\n",
      "time/evaluation: 0.554173469543457\n",
      "training/train_loss_mean: 5.4534989356994625\n",
      "training/train_loss_std: 0.3151876304597538\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 293\n",
      "training/learning_rate: 8.249046153745783e-05\n",
      "time/sample_batch: 0.007692098617553711\n",
      "time/training: 0.8137414455413818\n",
      "evaluation/text/loss: 5.120863437652588\n",
      "evaluation/text/perplexity: 167.47991943359375\n",
      "time/total: 498.81989431381226\n",
      "time/evaluation: 0.6901192665100098\n",
      "training/train_loss_mean: 5.36289381980896\n",
      "training/train_loss_std: 0.16048862509166195\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 294\n",
      "training/learning_rate: 8.237726761379668e-05\n",
      "time/sample_batch: 0.008239984512329102\n",
      "time/training: 1.0000629425048828\n",
      "evaluation/text/loss: 4.888950347900391\n",
      "evaluation/text/perplexity: 132.8140869140625\n",
      "time/total: 500.35521697998047\n",
      "time/evaluation: 0.5334770679473877\n",
      "training/train_loss_mean: 5.493922376632691\n",
      "training/train_loss_std: 0.24517199864674424\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 295\n",
      "training/learning_rate: 8.22637980013546e-05\n",
      "time/sample_batch: 0.008942604064941406\n",
      "time/training: 0.958634614944458\n",
      "evaluation/text/loss: 5.039977550506592\n",
      "evaluation/text/perplexity: 154.466552734375\n",
      "time/total: 501.915887594223\n",
      "time/evaluation: 0.600470781326294\n",
      "training/train_loss_mean: 5.419273853302002\n",
      "training/train_loss_std: 0.32115788565903824\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 296\n",
      "training/learning_rate: 8.215005384276932e-05\n",
      "time/sample_batch: 0.00836038589477539\n",
      "time/training: 1.0702762603759766\n",
      "evaluation/text/loss: 5.10945987701416\n",
      "evaluation/text/perplexity: 165.58090209960938\n",
      "time/total: 503.51123118400574\n",
      "time/evaluation: 0.5234725475311279\n",
      "training/train_loss_mean: 5.677696657180786\n",
      "training/train_loss_std: 0.2651948727113385\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 297\n",
      "training/learning_rate: 8.203603628344329e-05\n",
      "time/sample_batch: 0.008374691009521484\n",
      "time/training: 1.0089213848114014\n",
      "evaluation/text/loss: 4.9630866050720215\n",
      "evaluation/text/perplexity: 143.03460693359375\n",
      "time/total: 505.3015651702881\n",
      "time/evaluation: 0.7798783779144287\n",
      "training/train_loss_mean: 5.438452577590942\n",
      "training/train_loss_std: 0.39749617974698503\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 298\n",
      "training/learning_rate: 8.192174647153201e-05\n",
      "time/sample_batch: 0.006849765777587891\n",
      "time/training: 1.0288074016571045\n",
      "evaluation/text/loss: 4.9765472412109375\n",
      "evaluation/text/perplexity: 144.97296142578125\n",
      "time/total: 507.0349807739258\n",
      "time/evaluation: 0.7030172348022461\n",
      "training/train_loss_mean: 5.010833001136779\n",
      "training/train_loss_std: 0.7376641335572044\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 299\n",
      "training/learning_rate: 8.180718555793268e-05\n",
      "time/sample_batch: 0.009267091751098633\n",
      "time/training: 1.024193525314331\n",
      "evaluation/text/loss: 5.068531036376953\n",
      "evaluation/text/perplexity: 158.940673828125\n",
      "time/total: 508.6728277206421\n",
      "time/evaluation: 0.6121108531951904\n",
      "training/train_loss_mean: 5.435108089447022\n",
      "training/train_loss_std: 0.3889285670888899\n",
      "================================================================================\n",
      "Input: ==OperationFelix:1940– | Output : 1941== | Prediction:  created to be Taylor, and significance and provided a script for thege, such asgem for him\n",
      "Input: ===Aftermathattheembassy= | Output : == | Prediction:  only using I. 2 on underground Lo His Football League losses, which cm ( straightator, and\n",
      "Input: ViewoftheWorldfrom9thAvenue(sometimesA<unk>NewYorker'sViewoftheWorld,ANewYorker'sViewofthe | Output : WorldorsimplyViewoftheWorld)isa1976illustrationbySaul<unk>thatservedasthecoveroftheMarch29,1976,editionofTheNewYorker.TheworkpresentstheviewfromManhattanoftherestoftheworldshowingManhattanasthecenteroftheworld. | Prediction:  of England Book Marcus formation inundle exclusive acrossás class series. nuclear Muslimiqu Sylvia effort to be\n",
      "Input: Intheaftermath,theJapaneseEmpireimmediatelyturneditsfocustothetworemainingrivalsforimperialdominanceinthePacificOcean:BritainandtheUnitedStates.<unk><unk>,aJapaneseNavyadmiralandmilitarytheorist,speculatedthatconflictwouldinevitablyarisebetweenJapanandatleastoneofits | Output : twomainrivals.Tothatend,hecalledfortheJapaneseNavytomaintainafleetwithatleast70%asmanycapitalshipsastheUSNavy.Thisratio,<unk>theorized,wouldenabletheImperialJapaneseNavytodefeattheUSNavyinonemajorbattleinJapanesewatersinanyeventualconflict.Accordingly,the1907ImperialDefencePolicycalledfortheconstructionofabattlefleetofeightmodernbattleships,20@,@000longtons(20@,@321t)each,andeightmodernarmouredcruisers,18@,@000longtons(18@,@<unk>t)each.ThiswasthegenesisoftheEight@-@EightFleetProgram,thedevelopmentofacohesivebattlelineofsixteencapitalships. | Prediction:  received found populations of We's B E Wells'followed : Base ] also alsoil of underlying\n",
      "Input: InadditiontotheRussiangenerals,thecouncilincludedAustriancommandersLieutenantFieldMarshalJohannHeinrichvon<unk>andFriedrichKarlWilhelm,<unk><unk><unk>.<unk>,whohadretiredfromthemilitaryin1800,hadbeenrecalledintoserviceaftertheUlm<unk>andhadcometo<unk>highlyrecommendedbytheEmperor.Hewasanexperienced<unk>andstrategistandhadservedinavarietyofpostsintheHabsburgmilitary;hehadbeen<unk>Charles'trustedadviserduringthecampaignsfrom1796to1800andhadassistedinplanningseveralofCharles'victories. | Output : Uponhisrecall,<unk>wasappointedChiefofthe<unk>GeneralStaffoftheCoalitionArmy.ThegeneralshadfoundamongtheAustrianforceone<unk>ChristophFreiherrvon<unk>(1753–1824),whohadknowledgeofthelocalgeography. | Prediction:  well @-@ period, near installed on 2 @-@ Battalion, and Spanish July 19 –\n",
      "================================================================================\n",
      "Iteration 300\n",
      "training/learning_rate: 8.16923546962724e-05\n",
      "time/sample_batch: 0.00797128677368164\n",
      "time/training: 1.1204371452331543\n",
      "evaluation/text/loss: 5.007112503051758\n",
      "evaluation/text/perplexity: 149.47250366210938\n",
      "time/total: 511.17416620254517\n",
      "time/evaluation: 1.379366159439087\n",
      "training/train_loss_mean: 5.429595899581909\n",
      "training/train_loss_std: 0.3044102612268514\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 301\n",
      "training/learning_rate: 8.157725504289664e-05\n",
      "time/sample_batch: 0.008819818496704102\n",
      "time/training: 0.975513219833374\n",
      "evaluation/text/loss: 5.017404556274414\n",
      "evaluation/text/perplexity: 151.01882934570312\n",
      "time/total: 512.5734646320343\n",
      "time/evaluation: 0.422637939453125\n",
      "training/train_loss_mean: 5.336680626869201\n",
      "training/train_loss_std: 0.5247993206737515\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 302\n",
      "training/learning_rate: 8.146188775685768e-05\n",
      "time/sample_batch: 0.007088899612426758\n",
      "time/training: 0.929398775100708\n",
      "evaluation/text/loss: 4.899672508239746\n",
      "evaluation/text/perplexity: 134.2458038330078\n",
      "time/total: 514.0599389076233\n",
      "time/evaluation: 0.5555644035339355\n",
      "training/train_loss_mean: 5.378666162490845\n",
      "training/train_loss_std: 0.3728657220456839\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 303\n",
      "training/learning_rate: 8.134625399990279e-05\n",
      "time/sample_batch: 0.008380651473999023\n",
      "time/training: 1.0670979022979736\n",
      "evaluation/text/loss: 4.957909107208252\n",
      "evaluation/text/perplexity: 142.29595947265625\n",
      "time/total: 515.831550359726\n",
      "time/evaluation: 0.7030174732208252\n",
      "training/train_loss_mean: 5.3976459980010985\n",
      "training/train_loss_std: 0.4973555265324777\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 304\n",
      "training/learning_rate: 8.12303549364626e-05\n",
      "time/sample_batch: 0.009025812149047852\n",
      "time/training: 1.2205581665039062\n",
      "evaluation/text/loss: 5.085277557373047\n",
      "evaluation/text/perplexity: 161.62478637695312\n",
      "time/total: 517.7287724018097\n",
      "time/evaluation: 0.670280933380127\n",
      "training/train_loss_mean: 5.456413745880127\n",
      "training/train_loss_std: 0.2879828779506566\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 305\n",
      "training/learning_rate: 8.111419173363945e-05\n",
      "time/sample_batch: 0.007506370544433594\n",
      "time/training: 1.028311014175415\n",
      "evaluation/text/loss: 5.044039726257324\n",
      "evaluation/text/perplexity: 155.0952911376953\n",
      "time/total: 519.3442208766937\n",
      "time/evaluation: 0.5854723453521729\n",
      "training/train_loss_mean: 5.326410102844238\n",
      "training/train_loss_std: 0.5271123620316064\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 306\n",
      "training/learning_rate: 8.099776556119543e-05\n",
      "time/sample_batch: 0.008551836013793945\n",
      "time/training: 1.053084373474121\n",
      "evaluation/text/loss: 4.806952953338623\n",
      "evaluation/text/perplexity: 122.35821533203125\n",
      "time/total: 521.1806516647339\n",
      "time/evaluation: 0.7815656661987305\n",
      "training/train_loss_mean: 5.421080446243286\n",
      "training/train_loss_std: 0.22398968562327637\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 307\n",
      "training/learning_rate: 8.088107759154085e-05\n",
      "time/sample_batch: 0.009741544723510742\n",
      "time/training: 1.0945489406585693\n",
      "evaluation/text/loss: 5.138669967651367\n",
      "evaluation/text/perplexity: 170.48886108398438\n",
      "time/total: 522.956743478775\n",
      "time/evaluation: 0.6798539161682129\n",
      "training/train_loss_mean: 5.4957986831665036\n",
      "training/train_loss_std: 0.40680507121421994\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 308\n",
      "training/learning_rate: 8.076412899972224e-05\n",
      "time/sample_batch: 0.007048130035400391\n",
      "time/training: 1.0291666984558105\n",
      "evaluation/text/loss: 4.980826377868652\n",
      "evaluation/text/perplexity: 145.5946502685547\n",
      "time/total: 524.4946193695068\n",
      "time/evaluation: 0.5071084499359131\n",
      "training/train_loss_mean: 5.453555393218994\n",
      "training/train_loss_std: 0.19119200897399963\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 309\n",
      "training/learning_rate: 8.064692096341068e-05\n",
      "time/sample_batch: 0.007606029510498047\n",
      "time/training: 0.9606213569641113\n",
      "evaluation/text/loss: 4.716436862945557\n",
      "evaluation/text/perplexity: 111.76929473876953\n",
      "time/total: 526.1387784481049\n",
      "time/evaluation: 0.6820077896118164\n",
      "training/train_loss_mean: 5.213303756713867\n",
      "training/train_loss_std: 0.30871181358787453\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 310\n",
      "training/learning_rate: 8.052945466288972e-05\n",
      "time/sample_batch: 0.0070896148681640625\n",
      "time/training: 1.083812952041626\n",
      "evaluation/text/loss: 5.068418502807617\n",
      "evaluation/text/perplexity: 158.92279052734375\n",
      "time/total: 527.9707462787628\n",
      "time/evaluation: 0.7466297149658203\n",
      "training/train_loss_mean: 5.530875301361084\n",
      "training/train_loss_std: 0.20121583457621378\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 311\n",
      "training/learning_rate: 8.041173128104374e-05\n",
      "time/sample_batch: 0.008089780807495117\n",
      "time/training: 1.018549919128418\n",
      "evaluation/text/loss: 4.915900707244873\n",
      "evaluation/text/perplexity: 136.44215393066406\n",
      "time/total: 529.5212254524231\n",
      "time/evaluation: 0.5303783416748047\n",
      "training/train_loss_mean: 5.550399303436279\n",
      "training/train_loss_std: 0.3031031372855178\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 312\n",
      "training/learning_rate: 8.029375200334588e-05\n",
      "time/sample_batch: 0.008194923400878906\n",
      "time/training: 1.0680184364318848\n",
      "evaluation/text/loss: 4.759571075439453\n",
      "evaluation/text/perplexity: 116.69586181640625\n",
      "time/total: 531.1585235595703\n",
      "time/evaluation: 0.5677535533905029\n",
      "training/train_loss_mean: 5.256636476516723\n",
      "training/train_loss_std: 0.2971390060325868\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 313\n",
      "training/learning_rate: 8.017551801784616e-05\n",
      "time/sample_batch: 0.0077593326568603516\n",
      "time/training: 0.884077787399292\n",
      "evaluation/text/loss: 5.054007530212402\n",
      "evaluation/text/perplexity: 156.64898681640625\n",
      "time/total: 532.7426609992981\n",
      "time/evaluation: 0.6984572410583496\n",
      "training/train_loss_mean: 5.422116374969482\n",
      "training/train_loss_std: 0.41800848520984907\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 314\n",
      "training/learning_rate: 8.005703051515951e-05\n",
      "time/sample_batch: 0.007336854934692383\n",
      "time/training: 1.0277152061462402\n",
      "evaluation/text/loss: 5.03904390335083\n",
      "evaluation/text/perplexity: 154.32240295410156\n",
      "time/total: 534.3570342063904\n",
      "time/evaluation: 0.5850348472595215\n",
      "training/train_loss_mean: 5.363474035263062\n",
      "training/train_loss_std: 0.31748965668009294\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 315\n",
      "training/learning_rate: 7.993829068845374e-05\n",
      "time/sample_batch: 0.009182929992675781\n",
      "time/training: 0.9911470413208008\n",
      "evaluation/text/loss: 4.87653923034668\n",
      "evaluation/text/perplexity: 131.1759033203125\n",
      "time/total: 536.015855550766\n",
      "time/evaluation: 0.6660361289978027\n",
      "training/train_loss_mean: 5.294364786148071\n",
      "training/train_loss_std: 0.4263392922566324\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 316\n",
      "training/learning_rate: 7.981929973343763e-05\n",
      "time/sample_batch: 0.00777125358581543\n",
      "time/training: 1.002939224243164\n",
      "evaluation/text/loss: 5.080012798309326\n",
      "evaluation/text/perplexity: 160.77610778808594\n",
      "time/total: 537.5446512699127\n",
      "time/evaluation: 0.5242204666137695\n",
      "training/train_loss_mean: 5.3316551685333256\n",
      "training/train_loss_std: 0.8429678016196533\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 317\n",
      "training/learning_rate: 7.970005884834875e-05\n",
      "time/sample_batch: 0.008627176284790039\n",
      "time/training: 1.0795083045959473\n",
      "evaluation/text/loss: 5.13972282409668\n",
      "evaluation/text/perplexity: 170.66845703125\n",
      "time/total: 539.1353085041046\n",
      "time/evaluation: 0.5095386505126953\n",
      "training/train_loss_mean: 5.553375911712647\n",
      "training/train_loss_std: 0.3275208027621903\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 318\n",
      "training/learning_rate: 7.958056923394146e-05\n",
      "time/sample_batch: 0.008049726486206055\n",
      "time/training: 1.016040563583374\n",
      "evaluation/text/loss: 4.987671852111816\n",
      "evaluation/text/perplexity: 146.5947265625\n",
      "time/total: 540.851395368576\n",
      "time/evaluation: 0.698519229888916\n",
      "training/train_loss_mean: 5.49551739692688\n",
      "training/train_loss_std: 0.3806264273403576\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 319\n",
      "training/learning_rate: 7.94608320934749e-05\n",
      "time/sample_batch: 0.0067980289459228516\n",
      "time/training: 0.9937212467193604\n",
      "evaluation/text/loss: 4.783984661102295\n",
      "evaluation/text/perplexity: 119.57988739013672\n",
      "time/total: 542.5052416324615\n",
      "time/evaluation: 0.6584134101867676\n",
      "training/train_loss_mean: 5.498968315124512\n",
      "training/train_loss_std: 0.2844215554770883\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 320\n",
      "training/learning_rate: 7.934084863270072e-05\n",
      "time/sample_batch: 0.008545637130737305\n",
      "time/training: 1.0694053173065186\n",
      "evaluation/text/loss: 5.023808479309082\n",
      "evaluation/text/perplexity: 151.98904418945312\n",
      "time/total: 544.1833407878876\n",
      "time/evaluation: 0.6070601940155029\n",
      "training/train_loss_mean: 5.493075275421143\n",
      "training/train_loss_std: 0.31200300180301255\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 321\n",
      "training/learning_rate: 7.922062005985104e-05\n",
      "time/sample_batch: 0.00814962387084961\n",
      "time/training: 1.1386208534240723\n",
      "evaluation/text/loss: 4.9864678382873535\n",
      "evaluation/text/perplexity: 146.4183349609375\n",
      "time/total: 545.8492546081543\n",
      "time/evaluation: 0.5256285667419434\n",
      "training/train_loss_mean: 5.324901580810547\n",
      "training/train_loss_std: 0.38305148233387615\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 322\n",
      "training/learning_rate: 7.910014758562627e-05\n",
      "time/sample_batch: 0.008059978485107422\n",
      "time/training: 1.0464651584625244\n",
      "evaluation/text/loss: 5.029226303100586\n",
      "evaluation/text/perplexity: 152.81472778320312\n",
      "time/total: 547.4408917427063\n",
      "time/evaluation: 0.5436282157897949\n",
      "training/train_loss_mean: 5.41006555557251\n",
      "training/train_loss_std: 0.20390435331417345\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 323\n",
      "training/learning_rate: 7.897943242318285e-05\n",
      "time/sample_batch: 0.007284402847290039\n",
      "time/training: 0.9708044528961182\n",
      "evaluation/text/loss: 4.9718403816223145\n",
      "evaluation/text/perplexity: 144.2921905517578\n",
      "time/total: 549.0008614063263\n",
      "time/evaluation: 0.5875387191772461\n",
      "training/train_loss_mean: 5.366614484786988\n",
      "training/train_loss_std: 0.24691023756093183\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 324\n",
      "training/learning_rate: 7.885847578812116e-05\n",
      "time/sample_batch: 0.009415149688720703\n",
      "time/training: 1.076695442199707\n",
      "evaluation/text/loss: 4.972838401794434\n",
      "evaluation/text/perplexity: 144.436279296875\n",
      "time/total: 550.8974871635437\n",
      "time/evaluation: 0.8182001113891602\n",
      "training/train_loss_mean: 5.544025707244873\n",
      "training/train_loss_std: 0.17554494669985782\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 325\n",
      "training/learning_rate: 7.873727889847317e-05\n",
      "time/sample_batch: 0.008650541305541992\n",
      "time/training: 1.0293457508087158\n",
      "evaluation/text/loss: 5.032586574554443\n",
      "evaluation/text/perplexity: 153.3291015625\n",
      "time/total: 552.4000887870789\n",
      "time/evaluation: 0.47150182723999023\n",
      "training/train_loss_mean: 5.444301462173462\n",
      "training/train_loss_std: 0.31872187212845443\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 326\n",
      "training/learning_rate: 7.86158429746902e-05\n",
      "time/sample_batch: 0.00797414779663086\n",
      "time/training: 1.0231742858886719\n",
      "evaluation/text/loss: 4.983699798583984\n",
      "evaluation/text/perplexity: 146.01361083984375\n",
      "time/total: 553.9360468387604\n",
      "time/evaluation: 0.5112123489379883\n",
      "training/train_loss_mean: 5.2024935483932495\n",
      "training/train_loss_std: 0.48497898226605807\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 327\n",
      "training/learning_rate: 7.849416923963068e-05\n",
      "time/sample_batch: 0.00834965705871582\n",
      "time/training: 0.9712529182434082\n",
      "evaluation/text/loss: 5.075099468231201\n",
      "evaluation/text/perplexity: 159.98809814453125\n",
      "time/total: 555.6611721515656\n",
      "time/evaluation: 0.752133846282959\n",
      "training/train_loss_mean: 5.2436662197113035\n",
      "training/train_loss_std: 0.3843509193440132\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 328\n",
      "training/learning_rate: 7.837225891854772e-05\n",
      "time/sample_batch: 0.008007526397705078\n",
      "time/training: 0.9892969131469727\n",
      "evaluation/text/loss: 4.9299445152282715\n",
      "evaluation/text/perplexity: 138.37184143066406\n",
      "time/total: 557.2996740341187\n",
      "time/evaluation: 0.6476695537567139\n",
      "training/train_loss_mean: 5.235833168029785\n",
      "training/train_loss_std: 0.6633525484855374\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 329\n",
      "training/learning_rate: 7.825011323907695e-05\n",
      "time/sample_batch: 0.007703542709350586\n",
      "time/training: 0.977473258972168\n",
      "evaluation/text/loss: 5.056763172149658\n",
      "evaluation/text/perplexity: 157.0812530517578\n",
      "time/total: 558.7529850006104\n",
      "time/evaluation: 0.4741365909576416\n",
      "training/train_loss_mean: 5.468386602401734\n",
      "training/train_loss_std: 0.3660527526384186\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 330\n",
      "training/learning_rate: 7.812773343122398e-05\n",
      "time/sample_batch: 0.0076792240142822266\n",
      "time/training: 0.9027354717254639\n",
      "evaluation/text/loss: 5.108315467834473\n",
      "evaluation/text/perplexity: 165.39151000976562\n",
      "time/total: 560.1805164813995\n",
      "time/evaluation: 0.5232138633728027\n",
      "training/train_loss_mean: 5.329376912117004\n",
      "training/train_loss_std: 0.5855538099172527\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 331\n",
      "training/learning_rate: 7.800512072735212e-05\n",
      "time/sample_batch: 0.007691860198974609\n",
      "time/training: 0.909099817276001\n",
      "evaluation/text/loss: 5.099557399749756\n",
      "evaluation/text/perplexity: 163.94932556152344\n",
      "time/total: 561.8702154159546\n",
      "time/evaluation: 0.7791280746459961\n",
      "training/train_loss_mean: 5.502722930908203\n",
      "training/train_loss_std: 0.2579992407050776\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 332\n",
      "training/learning_rate: 7.788227636216995e-05\n",
      "time/sample_batch: 0.009238243103027344\n",
      "time/training: 1.3043537139892578\n",
      "evaluation/text/loss: 4.851255416870117\n",
      "evaluation/text/perplexity: 127.9008560180664\n",
      "time/total: 563.7698831558228\n",
      "time/evaluation: 0.5937333106994629\n",
      "training/train_loss_mean: 5.468677091598511\n",
      "training/train_loss_std: 0.24490935559435764\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 333\n",
      "training/learning_rate: 7.775920157271884e-05\n",
      "time/sample_batch: 0.008044004440307617\n",
      "time/training: 1.0849461555480957\n",
      "evaluation/text/loss: 5.022272109985352\n",
      "evaluation/text/perplexity: 151.75572204589844\n",
      "time/total: 565.4207310676575\n",
      "time/evaluation: 0.5643565654754639\n",
      "training/train_loss_mean: 5.380802822113037\n",
      "training/train_loss_std: 0.23517044052060926\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 334\n",
      "training/learning_rate: 7.763589759836059e-05\n",
      "time/sample_batch: 0.0071527957916259766\n",
      "time/training: 1.0440423488616943\n",
      "evaluation/text/loss: 5.088897705078125\n",
      "evaluation/text/perplexity: 162.21095275878906\n",
      "time/total: 567.0020542144775\n",
      "time/evaluation: 0.5352301597595215\n",
      "training/train_loss_mean: 5.465058088302612\n",
      "training/train_loss_std: 0.32165972831161255\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 335\n",
      "training/learning_rate: 7.751236568076482e-05\n",
      "time/sample_batch: 0.006372690200805664\n",
      "time/training: 1.126746654510498\n",
      "evaluation/text/loss: 4.892631530761719\n",
      "evaluation/text/perplexity: 133.3039093017578\n",
      "time/total: 568.7393388748169\n",
      "time/evaluation: 0.608910083770752\n",
      "training/train_loss_mean: 5.161793303489685\n",
      "training/train_loss_std: 1.0065482685651965\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 336\n",
      "training/learning_rate: 7.738860706389656e-05\n",
      "time/sample_batch: 0.009532451629638672\n",
      "time/training: 0.9771394729614258\n",
      "evaluation/text/loss: 4.907689571380615\n",
      "evaluation/text/perplexity: 135.32638549804688\n",
      "time/total: 570.2639291286469\n",
      "time/evaluation: 0.5457751750946045\n",
      "training/train_loss_mean: 5.262481260299682\n",
      "training/train_loss_std: 0.2098007406244808\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 337\n",
      "training/learning_rate: 7.726462299400374e-05\n",
      "time/sample_batch: 0.009744882583618164\n",
      "time/training: 1.0696775913238525\n",
      "evaluation/text/loss: 4.962906837463379\n",
      "evaluation/text/perplexity: 143.00889587402344\n",
      "time/total: 571.9367032051086\n",
      "time/evaluation: 0.6015069484710693\n",
      "training/train_loss_mean: 5.307705926895141\n",
      "training/train_loss_std: 0.3190428813416193\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 338\n",
      "training/learning_rate: 7.714041471960456e-05\n",
      "time/sample_batch: 0.008330106735229492\n",
      "time/training: 0.8386657238006592\n",
      "evaluation/text/loss: 5.0263671875\n",
      "evaluation/text/perplexity: 152.37844848632812\n",
      "time/total: 573.7562828063965\n",
      "time/evaluation: 0.9792134761810303\n",
      "training/train_loss_mean: 5.33489351272583\n",
      "training/train_loss_std: 0.34979009680032996\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 339\n",
      "training/learning_rate: 7.701598349147495e-05\n",
      "time/sample_batch: 0.008888483047485352\n",
      "time/training: 1.0020928382873535\n",
      "evaluation/text/loss: 5.082761764526367\n",
      "evaluation/text/perplexity: 161.21868896484375\n",
      "time/total: 575.4493224620819\n",
      "time/evaluation: 0.6894142627716064\n",
      "training/train_loss_mean: 5.5291650772094725\n",
      "training/train_loss_std: 0.11651195261467884\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 340\n",
      "training/learning_rate: 7.689133056263601e-05\n",
      "time/sample_batch: 0.007929086685180664\n",
      "time/training: 0.9968576431274414\n",
      "evaluation/text/loss: 5.061800479888916\n",
      "evaluation/text/perplexity: 157.87451171875\n",
      "time/total: 577.2128872871399\n",
      "time/evaluation: 0.7650754451751709\n",
      "training/train_loss_mean: 5.383051156997681\n",
      "training/train_loss_std: 0.22256165477385959\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 341\n",
      "training/learning_rate: 7.676645718834135e-05\n",
      "time/sample_batch: 0.007729530334472656\n",
      "time/training: 1.125387191772461\n",
      "evaluation/text/loss: 5.166696071624756\n",
      "evaluation/text/perplexity: 175.33457946777344\n",
      "time/total: 579.0564332008362\n",
      "time/evaluation: 0.7164750099182129\n",
      "training/train_loss_mean: 5.508234310150146\n",
      "training/train_loss_std: 0.3121947840604967\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 342\n",
      "training/learning_rate: 7.664136462606444e-05\n",
      "time/sample_batch: 0.007803916931152344\n",
      "time/training: 0.9460804462432861\n",
      "evaluation/text/loss: 5.010394096374512\n",
      "evaluation/text/perplexity: 149.9638214111328\n",
      "time/total: 580.7856504917145\n",
      "time/evaluation: 0.7813639640808105\n",
      "training/train_loss_mean: 5.36451563835144\n",
      "training/train_loss_std: 0.26108048793192656\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 343\n",
      "training/learning_rate: 7.651605413548602e-05\n",
      "time/sample_batch: 0.008903741836547852\n",
      "time/training: 1.0684828758239746\n",
      "evaluation/text/loss: 4.9618706703186035\n",
      "evaluation/text/perplexity: 142.8607940673828\n",
      "time/total: 582.4404933452606\n",
      "time/evaluation: 0.5801148414611816\n",
      "training/train_loss_mean: 5.438854646682739\n",
      "training/train_loss_std: 0.36260518685508386\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 344\n",
      "training/learning_rate: 7.639052697848131e-05\n",
      "time/sample_batch: 0.007288455963134766\n",
      "time/training: 1.0993502140045166\n",
      "evaluation/text/loss: 5.2068963050842285\n",
      "evaluation/text/perplexity: 182.52667236328125\n",
      "time/total: 584.0750575065613\n",
      "time/evaluation: 0.5335025787353516\n",
      "training/train_loss_mean: 5.243822002410889\n",
      "training/train_loss_std: 0.43834090805704434\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 345\n",
      "training/learning_rate: 7.626478441910743e-05\n",
      "time/sample_batch: 0.007807731628417969\n",
      "time/training: 1.187302589416504\n",
      "evaluation/text/loss: 4.831115245819092\n",
      "evaluation/text/perplexity: 125.35067749023438\n",
      "time/total: 585.8073613643646\n",
      "time/evaluation: 0.5434072017669678\n",
      "training/train_loss_mean: 5.404824686050415\n",
      "training/train_loss_std: 0.18704244139712314\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 346\n",
      "training/learning_rate: 7.613882772359054e-05\n",
      "time/sample_batch: 0.008469104766845703\n",
      "time/training: 0.9108214378356934\n",
      "evaluation/text/loss: 5.021145343780518\n",
      "evaluation/text/perplexity: 151.58482360839844\n",
      "time/total: 587.5426218509674\n",
      "time/evaluation: 0.8228371143341064\n",
      "training/train_loss_mean: 5.412697458267212\n",
      "training/train_loss_std: 0.21998782272355927\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 347\n",
      "training/learning_rate: 7.601265816031313e-05\n",
      "time/sample_batch: 0.010472536087036133\n",
      "time/training: 1.2340731620788574\n",
      "evaluation/text/loss: 4.983590602874756\n",
      "evaluation/text/perplexity: 145.99766540527344\n",
      "time/total: 589.2917184829712\n",
      "time/evaluation: 0.5132269859313965\n",
      "training/train_loss_mean: 5.418748617172241\n",
      "training/train_loss_std: 0.26930768979134173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 348\n",
      "training/learning_rate: 7.588627699980135e-05\n",
      "time/sample_batch: 0.007008075714111328\n",
      "time/training: 0.832019567489624\n",
      "evaluation/text/loss: 4.913501739501953\n",
      "evaluation/text/perplexity: 136.11521911621094\n",
      "time/total: 590.5903823375702\n",
      "time/evaluation: 0.4648723602294922\n",
      "training/train_loss_mean: 5.330820226669312\n",
      "training/train_loss_std: 0.26428338304038473\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 349\n",
      "training/learning_rate: 7.575968551471206e-05\n",
      "time/sample_batch: 0.008600473403930664\n",
      "time/training: 1.0155837535858154\n",
      "evaluation/text/loss: 5.006415843963623\n",
      "evaluation/text/perplexity: 149.36842346191406\n",
      "time/total: 592.3113036155701\n",
      "time/evaluation: 0.7036266326904297\n",
      "training/train_loss_mean: 5.4076169490814205\n",
      "training/train_loss_std: 0.3382167922411371\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 350\n",
      "training/learning_rate: 7.56328849798201e-05\n",
      "time/sample_batch: 0.008365392684936523\n",
      "time/training: 1.0565288066864014\n",
      "evaluation/text/loss: 4.989974498748779\n",
      "evaluation/text/perplexity: 146.93267822265625\n",
      "time/total: 593.8166689872742\n",
      "time/evaluation: 0.4470832347869873\n",
      "training/train_loss_mean: 5.272044897079468\n",
      "training/train_loss_std: 0.8605902902853289\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 351\n",
      "training/learning_rate: 7.550587667200547e-05\n",
      "time/sample_batch: 0.007747173309326172\n",
      "time/training: 1.1352910995483398\n",
      "evaluation/text/loss: 4.970352649688721\n",
      "evaluation/text/perplexity: 144.0776824951172\n",
      "time/total: 595.7744052410126\n",
      "time/evaluation: 0.8207554817199707\n",
      "training/train_loss_mean: 5.5496727466583256\n",
      "training/train_loss_std: 0.25216184620879206\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 352\n",
      "training/learning_rate: 7.53786618702404e-05\n",
      "time/sample_batch: 0.010013103485107422\n",
      "time/training: 1.1719880104064941\n",
      "evaluation/text/loss: 5.142013072967529\n",
      "evaluation/text/perplexity: 171.05978393554688\n",
      "time/total: 597.738067150116\n",
      "time/evaluation: 0.7898898124694824\n",
      "training/train_loss_mean: 5.304285717010498\n",
      "training/train_loss_std: 0.1913503565726996\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 353\n",
      "training/learning_rate: 7.525124185557657e-05\n",
      "time/sample_batch: 0.009134054183959961\n",
      "time/training: 1.017237663269043\n",
      "evaluation/text/loss: 4.874847888946533\n",
      "evaluation/text/perplexity: 130.95423889160156\n",
      "time/total: 599.2140576839447\n",
      "time/evaluation: 0.4570479393005371\n",
      "training/train_loss_mean: 5.424159288406372\n",
      "training/train_loss_std: 0.2340730668538348\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 354\n",
      "training/learning_rate: 7.512361791113206e-05\n",
      "time/sample_batch: 0.009078025817871094\n",
      "time/training: 0.9964196681976318\n",
      "evaluation/text/loss: 4.818435192108154\n",
      "evaluation/text/perplexity: 123.7712631225586\n",
      "time/total: 600.8877744674683\n",
      "time/evaluation: 0.6756806373596191\n",
      "training/train_loss_mean: 5.310504961013794\n",
      "training/train_loss_std: 0.32557464028653754\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 355\n",
      "training/learning_rate: 7.49957913220786e-05\n",
      "time/sample_batch: 0.010195016860961914\n",
      "time/training: 1.037221908569336\n",
      "evaluation/text/loss: 4.777849197387695\n",
      "evaluation/text/perplexity: 118.84845733642578\n",
      "time/total: 602.4244480133057\n",
      "time/evaluation: 0.4977602958679199\n",
      "training/train_loss_mean: 5.157369208335877\n",
      "training/train_loss_std: 0.5645997460861105\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 356\n",
      "training/learning_rate: 7.486776337562854e-05\n",
      "time/sample_batch: 0.009214639663696289\n",
      "time/training: 1.2426645755767822\n",
      "evaluation/text/loss: 5.170259952545166\n",
      "evaluation/text/perplexity: 175.9605712890625\n",
      "time/total: 604.2623987197876\n",
      "time/evaluation: 0.5935945510864258\n",
      "training/train_loss_mean: 5.518092489242553\n",
      "training/train_loss_std: 0.25206184544715327\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 357\n",
      "training/learning_rate: 7.473953536102185e-05\n",
      "time/sample_batch: 0.008017539978027344\n",
      "time/training: 0.9114711284637451\n",
      "evaluation/text/loss: 4.894454002380371\n",
      "evaluation/text/perplexity: 133.5470733642578\n",
      "time/total: 605.7833697795868\n",
      "time/evaluation: 0.6077802181243896\n",
      "training/train_loss_mean: 5.342560243606568\n",
      "training/train_loss_std: 0.20094117548873663\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 358\n",
      "training/learning_rate: 7.461110856951322e-05\n",
      "time/sample_batch: 0.008340835571289062\n",
      "time/training: 0.9696972370147705\n",
      "evaluation/text/loss: 4.882453918457031\n",
      "evaluation/text/perplexity: 131.95407104492188\n",
      "time/total: 607.4986908435822\n",
      "time/evaluation: 0.7439939975738525\n",
      "training/train_loss_mean: 5.166988897323608\n",
      "training/train_loss_std: 0.3871113353762665\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 359\n",
      "training/learning_rate: 7.448248429435902e-05\n",
      "time/sample_batch: 0.00860285758972168\n",
      "time/training: 1.007279872894287\n",
      "evaluation/text/loss: 4.8556132316589355\n",
      "evaluation/text/perplexity: 128.45944213867188\n",
      "time/total: 609.0357246398926\n",
      "time/evaluation: 0.5281190872192383\n",
      "training/train_loss_mean: 5.291547060012817\n",
      "training/train_loss_std: 0.3648158015265558\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 360\n",
      "training/learning_rate: 7.435366383080428e-05\n",
      "time/sample_batch: 0.008482217788696289\n",
      "time/training: 1.0630252361297607\n",
      "evaluation/text/loss: 4.9566850662231445\n",
      "evaluation/text/perplexity: 142.12188720703125\n",
      "time/total: 610.6901962757111\n",
      "time/evaluation: 0.589846134185791\n",
      "training/train_loss_mean: 5.60164155960083\n",
      "training/train_loss_std: 0.3155834254400632\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 361\n",
      "training/learning_rate: 7.422464847606962e-05\n",
      "time/sample_batch: 0.008681058883666992\n",
      "time/training: 1.1118786334991455\n",
      "evaluation/text/loss: 4.920129299163818\n",
      "evaluation/text/perplexity: 137.02032470703125\n",
      "time/total: 612.3946831226349\n",
      "time/evaluation: 0.5910751819610596\n",
      "training/train_loss_mean: 5.520474672317505\n",
      "training/train_loss_std: 0.3598038224069956\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 362\n",
      "training/learning_rate: 7.409543952933826e-05\n",
      "time/sample_batch: 0.008581399917602539\n",
      "time/training: 1.0729866027832031\n",
      "evaluation/text/loss: 4.874715328216553\n",
      "evaluation/text/perplexity: 130.93687438964844\n",
      "time/total: 614.2361576557159\n",
      "time/evaluation: 0.7666144371032715\n",
      "training/train_loss_mean: 5.310540723800659\n",
      "training/train_loss_std: 0.18230343038572364\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 363\n",
      "training/learning_rate: 7.396603829174281e-05\n",
      "time/sample_batch: 0.007820367813110352\n",
      "time/training: 0.8242630958557129\n",
      "evaluation/text/loss: 5.102243423461914\n",
      "evaluation/text/perplexity: 164.39028930664062\n",
      "time/total: 615.6592628955841\n",
      "time/evaluation: 0.5972607135772705\n",
      "training/train_loss_mean: 4.845891547203064\n",
      "training/train_loss_std: 0.8053284766995843\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 364\n",
      "training/learning_rate: 7.383644606635234e-05\n",
      "time/sample_batch: 0.00738072395324707\n",
      "time/training: 1.1933481693267822\n",
      "evaluation/text/loss: 5.061267375946045\n",
      "evaluation/text/perplexity: 157.79037475585938\n",
      "time/total: 617.4547824859619\n",
      "time/evaluation: 0.6004734039306641\n",
      "training/train_loss_mean: 5.451920461654663\n",
      "training/train_loss_std: 0.3392302355519228\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 365\n",
      "training/learning_rate: 7.370666415815911e-05\n",
      "time/sample_batch: 0.009346723556518555\n",
      "time/training: 1.0594561100006104\n",
      "evaluation/text/loss: 5.003584861755371\n",
      "evaluation/text/perplexity: 148.94615173339844\n",
      "time/total: 619.2540194988251\n",
      "time/evaluation: 0.7380499839782715\n",
      "training/train_loss_mean: 5.212356472015381\n",
      "training/train_loss_std: 0.30782379556004896\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 366\n",
      "training/learning_rate: 7.357669387406547e-05\n",
      "time/sample_batch: 0.008846044540405273\n",
      "time/training: 1.1192638874053955\n",
      "evaluation/text/loss: 4.920444965362549\n",
      "evaluation/text/perplexity: 137.06358337402344\n",
      "time/total: 620.9757895469666\n",
      "time/evaluation: 0.6008808612823486\n",
      "training/train_loss_mean: 5.432929754257202\n",
      "training/train_loss_std: 0.15657492507522133\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 367\n",
      "training/learning_rate: 7.344653652287077e-05\n",
      "time/sample_batch: 0.008789777755737305\n",
      "time/training: 0.9939553737640381\n",
      "evaluation/text/loss: 4.84962272644043\n",
      "evaluation/text/perplexity: 127.69220733642578\n",
      "time/total: 622.6731486320496\n",
      "time/evaluation: 0.7018771171569824\n",
      "training/train_loss_mean: 4.9998054027557375\n",
      "training/train_loss_std: 1.1955007069274664\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 368\n",
      "training/learning_rate: 7.33161934152581e-05\n",
      "time/sample_batch: 0.009799003601074219\n",
      "time/training: 1.128927230834961\n",
      "evaluation/text/loss: 4.929014682769775\n",
      "evaluation/text/perplexity: 138.24322509765625\n",
      "time/total: 624.4596977233887\n",
      "time/evaluation: 0.6559956073760986\n",
      "training/train_loss_mean: 5.439162540435791\n",
      "training/train_loss_std: 0.27612661008733796\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 369\n",
      "training/learning_rate: 7.318566586378106e-05\n",
      "time/sample_batch: 0.008924722671508789\n",
      "time/training: 1.0983726978302002\n",
      "evaluation/text/loss: 5.019255638122559\n",
      "evaluation/text/perplexity: 151.29864501953125\n",
      "time/total: 626.1586954593658\n",
      "time/evaluation: 0.5989973545074463\n",
      "training/train_loss_mean: 5.314731216430664\n",
      "training/train_loss_std: 0.31658292155991574\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 370\n",
      "training/learning_rate: 7.305495518285068e-05\n",
      "time/sample_batch: 0.01025080680847168\n",
      "time/training: 1.154559850692749\n",
      "evaluation/text/loss: 5.011371612548828\n",
      "evaluation/text/perplexity: 150.11048889160156\n",
      "time/total: 627.9247996807098\n",
      "time/evaluation: 0.6098072528839111\n",
      "training/train_loss_mean: 5.535127925872803\n",
      "training/train_loss_std: 0.12106772297409024\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 371\n",
      "training/learning_rate: 7.292406268872205e-05\n",
      "time/sample_batch: 0.0068378448486328125\n",
      "time/training: 0.9666574001312256\n",
      "evaluation/text/loss: 4.930115222930908\n",
      "evaluation/text/perplexity: 138.3954620361328\n",
      "time/total: 629.5052726268768\n",
      "time/evaluation: 0.6121039390563965\n",
      "training/train_loss_mean: 5.105711627006531\n",
      "training/train_loss_std: 1.1027009220684123\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 372\n",
      "training/learning_rate: 7.279298969948115e-05\n",
      "time/sample_batch: 0.009435415267944336\n",
      "time/training: 1.0713889598846436\n",
      "evaluation/text/loss: 4.944912433624268\n",
      "evaluation/text/perplexity: 140.45855712890625\n",
      "time/total: 631.3244643211365\n",
      "time/evaluation: 0.7461419105529785\n",
      "training/train_loss_mean: 5.489246940612793\n",
      "training/train_loss_std: 0.20034386577989355\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 373\n",
      "training/learning_rate: 7.26617375350315e-05\n",
      "time/sample_batch: 0.008932352066040039\n",
      "time/training: 1.0182874202728271\n",
      "evaluation/text/loss: 4.890117645263672\n",
      "evaluation/text/perplexity: 132.96922302246094\n",
      "time/total: 632.9256637096405\n",
      "time/evaluation: 0.5810799598693848\n",
      "training/train_loss_mean: 5.433568239212036\n",
      "training/train_loss_std: 0.33040356746347177\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 374\n",
      "training/learning_rate: 7.253030751708096e-05\n",
      "time/sample_batch: 0.007364034652709961\n",
      "time/training: 0.8740160465240479\n",
      "evaluation/text/loss: 5.157042503356934\n",
      "evaluation/text/perplexity: 173.65013122558594\n",
      "time/total: 634.5502061843872\n",
      "time/evaluation: 0.748854398727417\n",
      "training/train_loss_mean: 5.330260705947876\n",
      "training/train_loss_std: 0.17555930814945167\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 375\n",
      "training/learning_rate: 7.239870096912836e-05\n",
      "time/sample_batch: 0.01085972785949707\n",
      "time/training: 1.0282373428344727\n",
      "evaluation/text/loss: 4.905609130859375\n",
      "evaluation/text/perplexity: 135.04515075683594\n",
      "time/total: 636.2939488887787\n",
      "time/evaluation: 0.7138018608093262\n",
      "training/train_loss_mean: 5.350930643081665\n",
      "training/train_loss_std: 0.2221963197721691\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 376\n",
      "training/learning_rate: 7.226691921645014e-05\n",
      "time/sample_batch: 0.00846409797668457\n",
      "time/training: 1.0610723495483398\n",
      "evaluation/text/loss: 4.932043552398682\n",
      "evaluation/text/perplexity: 138.66258239746094\n",
      "time/total: 638.0639667510986\n",
      "time/evaluation: 0.7073400020599365\n",
      "training/train_loss_mean: 5.221670484542846\n",
      "training/train_loss_std: 0.8126214048067167\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 377\n",
      "training/learning_rate: 7.213496358608713e-05\n",
      "time/sample_batch: 0.008558034896850586\n",
      "time/training: 1.1473023891448975\n",
      "evaluation/text/loss: 4.968659400939941\n",
      "evaluation/text/perplexity: 143.8339385986328\n",
      "time/total: 639.9689674377441\n",
      "time/evaluation: 0.7561359405517578\n",
      "training/train_loss_mean: 5.29589958190918\n",
      "training/train_loss_std: 0.3189301771839312\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 378\n",
      "training/learning_rate: 7.200283540683103e-05\n",
      "time/sample_batch: 0.008718013763427734\n",
      "time/training: 0.9501912593841553\n",
      "evaluation/text/loss: 4.732911109924316\n",
      "evaluation/text/perplexity: 113.62586212158203\n",
      "time/total: 641.6638278961182\n",
      "time/evaluation: 0.7430963516235352\n",
      "training/train_loss_mean: 5.335307168960571\n",
      "training/train_loss_std: 0.2043851235715938\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 379\n",
      "training/learning_rate: 7.187053600921115e-05\n",
      "time/sample_batch: 0.009815692901611328\n",
      "time/training: 1.1276230812072754\n",
      "evaluation/text/loss: 5.050662040710449\n",
      "evaluation/text/perplexity: 156.12579345703125\n",
      "time/total: 643.2845621109009\n",
      "time/evaluation: 0.491396427154541\n",
      "training/train_loss_mean: 5.245454740524292\n",
      "training/train_loss_std: 0.31411191836972574\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 380\n",
      "training/learning_rate: 7.173806672548096e-05\n",
      "time/sample_batch: 0.007466316223144531\n",
      "time/training: 1.1271076202392578\n",
      "evaluation/text/loss: 4.862053394317627\n",
      "evaluation/text/perplexity: 129.28941345214844\n",
      "time/total: 645.2322461605072\n",
      "time/evaluation: 0.8188679218292236\n",
      "training/train_loss_mean: 5.307124853134155\n",
      "training/train_loss_std: 0.35363802224497315\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 381\n",
      "training/learning_rate: 7.160542888960466e-05\n",
      "time/sample_batch: 0.010127067565917969\n",
      "time/training: 1.0522222518920898\n",
      "evaluation/text/loss: 4.924536228179932\n",
      "evaluation/text/perplexity: 137.62550354003906\n",
      "time/total: 646.9877941608429\n",
      "time/evaluation: 0.7016112804412842\n",
      "training/train_loss_mean: 5.230544090270996\n",
      "training/train_loss_std: 0.366193579105166\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 382\n",
      "training/learning_rate: 7.147262383724381e-05\n",
      "time/sample_batch: 0.006715536117553711\n",
      "time/training: 0.8889281749725342\n",
      "evaluation/text/loss: 5.07249641418457\n",
      "evaluation/text/perplexity: 159.5721893310547\n",
      "time/total: 648.4982182979584\n",
      "time/evaluation: 0.6155545711517334\n",
      "training/train_loss_mean: 5.115379738807678\n",
      "training/train_loss_std: 0.878669421854139\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 383\n",
      "training/learning_rate: 7.13396529057438e-05\n",
      "time/sample_batch: 0.008228063583374023\n",
      "time/training: 0.9605858325958252\n",
      "evaluation/text/loss: 4.959073543548584\n",
      "evaluation/text/perplexity: 142.4617462158203\n",
      "time/total: 650.2426936626434\n",
      "time/evaluation: 0.7821567058563232\n",
      "training/train_loss_mean: 5.479020023345948\n",
      "training/train_loss_std: 0.24536875300966596\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 384\n",
      "training/learning_rate: 7.120651743412042e-05\n",
      "time/sample_batch: 0.00906515121459961\n",
      "time/training: 0.9336693286895752\n",
      "evaluation/text/loss: 4.852067947387695\n",
      "evaluation/text/perplexity: 128.00482177734375\n",
      "time/total: 651.925947189331\n",
      "time/evaluation: 0.7479467391967773\n",
      "training/train_loss_mean: 4.9907371520996096\n",
      "training/train_loss_std: 1.1445561511231446\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 385\n",
      "training/learning_rate: 7.107321876304642e-05\n",
      "time/sample_batch: 0.00926351547241211\n",
      "time/training: 1.1112327575683594\n",
      "evaluation/text/loss: 4.82027006149292\n",
      "evaluation/text/perplexity: 123.99857330322266\n",
      "time/total: 653.784734249115\n",
      "time/evaluation: 0.7456984519958496\n",
      "training/train_loss_mean: 5.293947172164917\n",
      "training/train_loss_std: 0.2946428436523102\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 386\n",
      "training/learning_rate: 7.093975823483792e-05\n",
      "time/sample_batch: 0.006915092468261719\n",
      "time/training: 1.1263997554779053\n",
      "evaluation/text/loss: 5.097208023071289\n",
      "evaluation/text/perplexity: 163.56460571289062\n",
      "time/total: 655.3897213935852\n",
      "time/evaluation: 0.4769871234893799\n",
      "training/train_loss_mean: 5.136880087852478\n",
      "training/train_loss_std: 0.9419332334866539\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 387\n",
      "training/learning_rate: 7.080613719344095e-05\n",
      "time/sample_batch: 0.008183479309082031\n",
      "time/training: 1.1460487842559814\n",
      "evaluation/text/loss: 5.000225067138672\n",
      "evaluation/text/perplexity: 148.44656372070312\n",
      "time/total: 657.2809402942657\n",
      "time/evaluation: 0.7433912754058838\n",
      "training/train_loss_mean: 5.247413754463196\n",
      "training/train_loss_std: 0.48499428945995554\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 388\n",
      "training/learning_rate: 7.067235698441793e-05\n",
      "time/sample_batch: 0.008250236511230469\n",
      "time/training: 1.0541436672210693\n",
      "evaluation/text/loss: 4.8838276863098145\n",
      "evaluation/text/perplexity: 132.13546752929688\n",
      "time/total: 658.7779004573822\n",
      "time/evaluation: 0.44106388092041016\n",
      "training/train_loss_mean: 5.335810756683349\n",
      "training/train_loss_std: 0.3215751141956165\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 389\n",
      "training/learning_rate: 7.053841895493407e-05\n",
      "time/sample_batch: 0.008417844772338867\n",
      "time/training: 1.0914764404296875\n",
      "evaluation/text/loss: 4.958585739135742\n",
      "evaluation/text/perplexity: 142.39227294921875\n",
      "time/total: 660.4688427448273\n",
      "time/evaluation: 0.5977530479431152\n",
      "training/train_loss_mean: 5.306488466262818\n",
      "training/train_loss_std: 0.2696111737619776\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 390\n",
      "training/learning_rate: 7.040432445374383e-05\n",
      "time/sample_batch: 0.008782386779785156\n",
      "time/training: 1.016601324081421\n",
      "evaluation/text/loss: 4.921471118927002\n",
      "evaluation/text/perplexity: 137.20431518554688\n",
      "time/total: 662.0325746536255\n",
      "time/evaluation: 0.5454227924346924\n",
      "training/train_loss_mean: 5.432603168487549\n",
      "training/train_loss_std: 0.16936902710110088\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 391\n",
      "training/learning_rate: 7.027007483117738e-05\n",
      "time/sample_batch: 0.008124351501464844\n",
      "time/training: 1.0990736484527588\n",
      "evaluation/text/loss: 4.88934850692749\n",
      "evaluation/text/perplexity: 132.8669891357422\n",
      "time/total: 663.8789081573486\n",
      "time/evaluation: 0.7455480098724365\n",
      "training/train_loss_mean: 5.443821048736572\n",
      "training/train_loss_std: 0.1804326183408339\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 392\n",
      "training/learning_rate: 7.01356714391269e-05\n",
      "time/sample_batch: 0.008402824401855469\n",
      "time/training: 0.9392566680908203\n",
      "evaluation/text/loss: 4.953571796417236\n",
      "evaluation/text/perplexity: 141.68011474609375\n",
      "time/total: 665.361168384552\n",
      "time/evaluation: 0.5413839817047119\n",
      "training/train_loss_mean: 5.306006669998169\n",
      "training/train_loss_std: 0.3447144805192025\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 393\n",
      "training/learning_rate: 7.000111563103306e-05\n",
      "time/sample_batch: 0.010401248931884766\n",
      "time/training: 0.9609928131103516\n",
      "evaluation/text/loss: 5.101769924163818\n",
      "evaluation/text/perplexity: 164.31246948242188\n",
      "time/total: 667.0597336292267\n",
      "time/evaluation: 0.735720157623291\n",
      "training/train_loss_mean: 5.323696756362915\n",
      "training/train_loss_std: 0.2839844310481472\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 394\n",
      "training/learning_rate: 6.986640876187138e-05\n",
      "time/sample_batch: 0.008524656295776367\n",
      "time/training: 0.9967608451843262\n",
      "evaluation/text/loss: 4.982937812805176\n",
      "evaluation/text/perplexity: 145.9023895263672\n",
      "time/total: 668.6173574924469\n",
      "time/evaluation: 0.559072732925415\n",
      "training/train_loss_mean: 5.247315549850464\n",
      "training/train_loss_std: 0.2832535434622197\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 395\n",
      "training/learning_rate: 6.97315521881385e-05\n",
      "time/sample_batch: 0.01031804084777832\n",
      "time/training: 1.0980443954467773\n",
      "evaluation/text/loss: 5.068389415740967\n",
      "evaluation/text/perplexity: 158.9181671142578\n",
      "time/total: 670.4623618125916\n",
      "time/evaluation: 0.7451717853546143\n",
      "training/train_loss_mean: 5.444687986373902\n",
      "training/train_loss_std: 0.26483123119697377\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 396\n",
      "training/learning_rate: 6.959654726783865e-05\n",
      "time/sample_batch: 0.007841348648071289\n",
      "time/training: 1.0792944431304932\n",
      "evaluation/text/loss: 4.821162700653076\n",
      "evaluation/text/perplexity: 124.10930633544922\n",
      "time/total: 672.2071969509125\n",
      "time/evaluation: 0.6638848781585693\n",
      "training/train_loss_mean: 5.3256772518157955\n",
      "training/train_loss_std: 0.1967475875346876\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 397\n",
      "training/learning_rate: 6.946139536046986e-05\n",
      "time/sample_batch: 0.008746147155761719\n",
      "time/training: 1.1696255207061768\n",
      "evaluation/text/loss: 4.953874111175537\n",
      "evaluation/text/perplexity: 141.7229461669922\n",
      "time/total: 674.1348848342896\n",
      "time/evaluation: 0.7563009262084961\n",
      "training/train_loss_mean: 5.292958736419678\n",
      "training/train_loss_std: 0.29887145561461415\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 398\n",
      "training/learning_rate: 6.932609782701032e-05\n",
      "time/sample_batch: 0.008785247802734375\n",
      "time/training: 0.9331619739532471\n",
      "evaluation/text/loss: 4.839256286621094\n",
      "evaluation/text/perplexity: 126.37532806396484\n",
      "time/total: 675.8320655822754\n",
      "time/evaluation: 0.7624754905700684\n",
      "training/train_loss_mean: 5.121095323562622\n",
      "training/train_loss_std: 0.1596492652846009\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 399\n",
      "training/learning_rate: 6.919065602990474e-05\n",
      "time/sample_batch: 0.008583784103393555\n",
      "time/training: 1.009474754333496\n",
      "evaluation/text/loss: 4.948814868927002\n",
      "evaluation/text/perplexity: 141.00775146484375\n",
      "time/total: 677.378960609436\n",
      "time/evaluation: 0.535764217376709\n",
      "training/train_loss_mean: 5.377643156051636\n",
      "training/train_loss_std: 0.2792542011742462\n",
      "================================================================================\n",
      "Input: OnMay22,2006thefirstsinglefromthealbum,\"30/30@-@150\",wasmadeavailableonlineasafree<unk>download.Amusicvideoforthesinglewasshotwithdirector<unk>BrowninLosAngeles,thevideoreceivedapremieronMTV's<unk>BallonJune3,2006.Priortothereleaseofthesecondsinglefromthealbum,\"ThroughGlass\",radiostationsthroughouttheUSshowedhighsupportforthesong.AmusicvideoforthesinglewasshotwithdirectorTony<unk>andwasreleased | Output : onJune9,2006onlinethroughYahoo!.Thethirdsinglefromthealbum,\"<unk>\",beganreceivingradioairplayinNovember2006.AmusicvideoforthesinglewasshotinJanuary2007andwasreleasedonlineonMarch8,2007.Thefourthsinglefromthealbum,\"MadeofScars\",featuredamusicvideowhichwasrecordedliveonApril7,2007andwaspostedonlineonJune5,2007.Thefifthandfinalsinglefromthealbum,\"<unk><unk>.\",startedreceivingradioairplayinFall2007andnomusicvideowasmadeforthesingle. | Prediction:  tries toStaff. As protective government is sold highest @-@ lacked again from Ryan his father as\n",
      "Input: On | Output : <unk>@-@<unk>,Typhoon<unk>producedsustainedwindsof102km/h(63mph),withguststo200km/h(124mph),whichwasthethirdstrongestonrecordforthestation.Rainfalltherereached183mm(7@.@2in).WindgustsonIwoJimapeakedat109km/h(68mph). | Prediction:  entitledithae, from other process by a crowd production, and concreteauned in a insurance,\n",
      "Input: <unk>debutedonWWFtelevisionontheMarch18,2002,episodeofRaw,comingthroughthecrowdandattackingAlSnow,<unk>andSpikeDudleyduringtheirmatch.HewasaccompaniedbyPaul<unk>,whowasseengivinginstructionsto<unk>.WhenthebrandextensionwasintroducedintheWWF,<unk>wasdraftedtotheRawbrand.Later,<unk>wasconfirmedtobe<unk>'sagentandgave<unk>thenickname\"TheNextBigThing\".<unk>'sfirstfeudwaswiththeHardyBoyz.<unk>andJeffHardy<unk>offat<unk>in<unk>'sfirstofficialtelevisedmatch.HewonthematchbyknockoutafterHardywasunabletorespondtorefereeTheodoreLong.ThenextnightonRaw,<unk>facedoffagainstJeffHardy'sbrother,MattHardy,anddefeatedhim | Output : inthesamefashion. | Prediction:  Don − theme variety @-@vertis AOL : Child One contrast proposed massive aimed for books \n",
      "\n",
      "================================================================================\n",
      "Iteration 400\n",
      "training/learning_rate: 6.905507133305048e-05\n",
      "time/sample_batch: 0.009517908096313477\n",
      "time/training: 0.9231197834014893\n",
      "evaluation/text/loss: 4.893978118896484\n",
      "evaluation/text/perplexity: 133.48353576660156\n",
      "time/total: 679.2893509864807\n",
      "time/evaluation: 0.9854288101196289\n",
      "training/train_loss_mean: 5.298614120483398\n",
      "training/train_loss_std: 0.1743674895472074\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 401\n",
      "training/learning_rate: 6.891934510178391e-05\n",
      "time/sample_batch: 0.00925755500793457\n",
      "time/training: 1.139892339706421\n",
      "evaluation/text/loss: 4.9465813636779785\n",
      "evaluation/text/perplexity: 140.6931610107422\n",
      "time/total: 681.1332066059113\n",
      "time/evaluation: 0.7027103900909424\n",
      "training/train_loss_mean: 5.392852878570556\n",
      "training/train_loss_std: 0.2660577235184093\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 402\n",
      "training/learning_rate: 6.87970716113208e-05\n",
      "time/sample_batch: 0.00778508186340332\n",
      "time/training: 1.0111405849456787\n",
      "evaluation/text/loss: 4.91802453994751\n",
      "evaluation/text/perplexity: 136.73223876953125\n",
      "time/total: 682.6149990558624\n",
      "time/evaluation: 0.4689624309539795\n",
      "training/train_loss_mean: 5.270878950754802\n",
      "training/train_loss_std: 0.2787028895880871\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 403\n",
      "training/learning_rate: 6.86610802312661e-05\n",
      "time/sample_batch: 0.00786447525024414\n",
      "time/training: 1.0462417602539062\n",
      "evaluation/text/loss: 4.934106349945068\n",
      "evaluation/text/perplexity: 138.94891357421875\n",
      "time/total: 684.2413086891174\n",
      "time/evaluation: 0.5783793926239014\n",
      "training/train_loss_mean: 5.288195943832397\n",
      "training/train_loss_std: 0.2291217003768817\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 404\n",
      "training/learning_rate: 6.852495128428516e-05\n",
      "time/sample_batch: 0.009545087814331055\n",
      "time/training: 0.9277129173278809\n",
      "evaluation/text/loss: 5.001265048980713\n",
      "evaluation/text/perplexity: 148.6010284423828\n",
      "time/total: 685.886590719223\n",
      "time/evaluation: 0.7160074710845947\n",
      "training/train_loss_mean: 5.285856056213379\n",
      "training/train_loss_std: 0.16340385218732684\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 405\n",
      "training/learning_rate: 6.838868614119495e-05\n",
      "time/sample_batch: 0.0093231201171875\n",
      "time/training: 1.0128016471862793\n",
      "evaluation/text/loss: 4.939271926879883\n",
      "evaluation/text/perplexity: 139.66851806640625\n",
      "time/total: 687.6508400440216\n",
      "time/evaluation: 0.74971604347229\n",
      "training/train_loss_mean: 5.358265447616577\n",
      "training/train_loss_std: 0.35285693205986607\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 406\n",
      "training/learning_rate: 6.82522861741839e-05\n",
      "time/sample_batch: 0.008228540420532227\n",
      "time/training: 1.0435898303985596\n",
      "evaluation/text/loss: 4.826267719268799\n",
      "evaluation/text/perplexity: 124.7445068359375\n",
      "time/total: 689.5155913829803\n",
      "time/evaluation: 0.8190035820007324\n",
      "training/train_loss_mean: 5.339550924301148\n",
      "training/train_loss_std: 0.34220055190388404\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 407\n",
      "training/learning_rate: 6.811575275679824e-05\n",
      "time/sample_batch: 0.009315252304077148\n",
      "time/training: 1.0825250148773193\n",
      "evaluation/text/loss: 5.099392414093018\n",
      "evaluation/text/perplexity: 163.9222869873047\n",
      "time/total: 691.2630441188812\n",
      "time/evaluation: 0.6631200313568115\n",
      "training/train_loss_mean: 5.232792854309082\n",
      "training/train_loss_std: 0.35190615943521714\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 408\n",
      "training/learning_rate: 6.797908726392792e-05\n",
      "time/sample_batch: 0.008523941040039062\n",
      "time/training: 1.0610435009002686\n",
      "evaluation/text/loss: 4.9341301918029785\n",
      "evaluation/text/perplexity: 138.9522247314453\n",
      "time/total: 693.0913143157959\n",
      "time/evaluation: 0.765451192855835\n",
      "training/train_loss_mean: 5.340015935897827\n",
      "training/train_loss_std: 0.26419227433918974\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 409\n",
      "training/learning_rate: 6.784229107179294e-05\n",
      "time/sample_batch: 0.009197473526000977\n",
      "time/training: 1.1074166297912598\n",
      "evaluation/text/loss: 4.983030796051025\n",
      "evaluation/text/perplexity: 145.91595458984375\n",
      "time/total: 694.9363219738007\n",
      "time/evaluation: 0.7359135150909424\n",
      "training/train_loss_mean: 5.3241133213043215\n",
      "training/train_loss_std: 0.20886011491124173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 410\n",
      "training/learning_rate: 6.770536555792946e-05\n",
      "time/sample_batch: 0.008913993835449219\n",
      "time/training: 1.0064029693603516\n",
      "evaluation/text/loss: 4.979979515075684\n",
      "evaluation/text/perplexity: 145.47140502929688\n",
      "time/total: 696.6060338020325\n",
      "time/evaluation: 0.661595344543457\n",
      "training/train_loss_mean: 5.189972734451294\n",
      "training/train_loss_std: 0.538649634161332\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 411\n",
      "training/learning_rate: 6.756831210117585e-05\n",
      "time/sample_batch: 0.009953737258911133\n",
      "time/training: 1.1890816688537598\n",
      "evaluation/text/loss: 4.931150436401367\n",
      "evaluation/text/perplexity: 138.53880310058594\n",
      "time/total: 698.3034977912903\n",
      "time/evaluation: 0.5067052841186523\n",
      "training/train_loss_mean: 5.311894845962525\n",
      "training/train_loss_std: 0.24276868360777562\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 412\n",
      "training/learning_rate: 6.743113208165892e-05\n",
      "time/sample_batch: 0.009102344512939453\n",
      "time/training: 1.2042877674102783\n",
      "evaluation/text/loss: 4.8320088386535645\n",
      "evaluation/text/perplexity: 125.4627456665039\n",
      "time/total: 700.1756598949432\n",
      "time/evaluation: 0.666187047958374\n",
      "training/train_loss_mean: 5.3516796112060545\n",
      "training/train_loss_std: 0.2743349131474932\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 413\n",
      "training/learning_rate: 6.729382688077995e-05\n",
      "time/sample_batch: 0.008491754531860352\n",
      "time/training: 1.164215326309204\n",
      "evaluation/text/loss: 4.922913074493408\n",
      "evaluation/text/perplexity: 137.4022979736328\n",
      "time/total: 702.3361840248108\n",
      "time/evaluation: 0.994624137878418\n",
      "training/train_loss_mean: 5.111346006393433\n",
      "training/train_loss_std: 0.7013352998219532\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 414\n",
      "training/learning_rate: 6.715639788120076e-05\n",
      "time/sample_batch: 0.009464502334594727\n",
      "time/training: 1.1190273761749268\n",
      "evaluation/text/loss: 4.891317367553711\n",
      "evaluation/text/perplexity: 133.12884521484375\n",
      "time/total: 704.211320400238\n",
      "time/evaluation: 0.7544853687286377\n",
      "training/train_loss_mean: 5.360115098953247\n",
      "training/train_loss_std: 0.3936636927914383\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 415\n",
      "training/learning_rate: 6.701884646682988e-05\n",
      "time/sample_batch: 0.007244110107421875\n",
      "time/training: 0.9443917274475098\n",
      "evaluation/text/loss: 4.980815410614014\n",
      "evaluation/text/perplexity: 145.59304809570312\n",
      "time/total: 705.681446313858\n",
      "time/evaluation: 0.5240511894226074\n",
      "training/train_loss_mean: 5.296542072296143\n",
      "training/train_loss_std: 0.13069794550150746\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 416\n",
      "training/learning_rate: 6.68811740228085e-05\n",
      "time/sample_batch: 0.008783340454101562\n",
      "time/training: 0.9316964149475098\n",
      "evaluation/text/loss: 4.889713287353516\n",
      "evaluation/text/perplexity: 132.91546630859375\n",
      "time/total: 707.1710431575775\n",
      "time/evaluation: 0.5561683177947998\n",
      "training/train_loss_mean: 5.30339241027832\n",
      "training/train_loss_std: 0.295420475733091\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 417\n",
      "training/learning_rate: 6.674338193549662e-05\n",
      "time/sample_batch: 0.008769035339355469\n",
      "time/training: 1.054342269897461\n",
      "evaluation/text/loss: 4.970329284667969\n",
      "evaluation/text/perplexity: 144.07432556152344\n",
      "time/total: 708.9754905700684\n",
      "time/evaluation: 0.7484638690948486\n",
      "training/train_loss_mean: 5.035239219665527\n",
      "training/train_loss_std: 0.3421513531035315\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 418\n",
      "training/learning_rate: 6.660547159245904e-05\n",
      "time/sample_batch: 0.009080171585083008\n",
      "time/training: 0.8654651641845703\n",
      "evaluation/text/loss: 4.8665852546691895\n",
      "evaluation/text/perplexity: 129.8766632080078\n",
      "time/total: 710.607191324234\n",
      "time/evaluation: 0.7645487785339355\n",
      "training/train_loss_mean: 5.1983592987060545\n",
      "training/train_loss_std: 0.3375281011639415\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 419\n",
      "training/learning_rate: 6.646744438245137e-05\n",
      "time/sample_batch: 0.008183002471923828\n",
      "time/training: 1.0101914405822754\n",
      "evaluation/text/loss: 4.974893569946289\n",
      "evaluation/text/perplexity: 144.73341369628906\n",
      "time/total: 712.3627364635468\n",
      "time/evaluation: 0.7434647083282471\n",
      "training/train_loss_mean: 5.08126015663147\n",
      "training/train_loss_std: 0.35255569217036103\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 420\n",
      "training/learning_rate: 6.632930169540607e-05\n",
      "time/sample_batch: 0.00858616828918457\n",
      "time/training: 1.0382976531982422\n",
      "evaluation/text/loss: 4.797609806060791\n",
      "evaluation/text/perplexity: 121.22032928466797\n",
      "time/total: 714.1389355659485\n",
      "time/evaluation: 0.736220121383667\n",
      "training/train_loss_mean: 5.3642796039581295\n",
      "training/train_loss_std: 0.24378429878773877\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 421\n",
      "training/learning_rate: 6.619104492241849e-05\n",
      "time/sample_batch: 0.009214401245117188\n",
      "time/training: 1.0118076801300049\n",
      "evaluation/text/loss: 4.860190391540527\n",
      "evaluation/text/perplexity: 129.04876708984375\n",
      "time/total: 715.6591815948486\n",
      "time/evaluation: 0.5016531944274902\n",
      "training/train_loss_mean: 5.361890125274658\n",
      "training/train_loss_std: 0.19775249390119382\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 422\n",
      "training/learning_rate: 6.605267545573275e-05\n",
      "time/sample_batch: 0.008361577987670898\n",
      "time/training: 1.0121691226959229\n",
      "evaluation/text/loss: 5.089988708496094\n",
      "evaluation/text/perplexity: 162.38803100585938\n",
      "time/total: 717.676308631897\n",
      "time/evaluation: 1.003218412399292\n",
      "training/train_loss_mean: 5.341990804672241\n",
      "training/train_loss_std: 0.2665940711904976\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 423\n",
      "training/learning_rate: 6.591419468872791e-05\n",
      "time/sample_batch: 0.00812077522277832\n",
      "time/training: 1.0545589923858643\n",
      "evaluation/text/loss: 4.940579891204834\n",
      "evaluation/text/perplexity: 139.851318359375\n",
      "time/total: 719.382336139679\n",
      "time/evaluation: 0.6498575210571289\n",
      "training/train_loss_mean: 5.159116792678833\n",
      "training/train_loss_std: 0.23064422796881187\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 424\n",
      "training/learning_rate: 6.577560401590371e-05\n",
      "time/sample_batch: 0.009677886962890625\n",
      "time/training: 0.9843556880950928\n",
      "evaluation/text/loss: 4.9955010414123535\n",
      "evaluation/text/perplexity: 147.7469482421875\n",
      "time/total: 721.1910648345947\n",
      "time/evaluation: 0.8226406574249268\n",
      "training/train_loss_mean: 5.149398374557495\n",
      "training/train_loss_std: 0.4144338350366173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 425\n",
      "training/learning_rate: 6.56369048328667e-05\n",
      "time/sample_batch: 0.009945154190063477\n",
      "time/training: 1.0378272533416748\n",
      "evaluation/text/loss: 4.749938011169434\n",
      "evaluation/text/perplexity: 115.57711791992188\n",
      "time/total: 722.8900887966156\n",
      "time/evaluation: 0.6595814228057861\n",
      "training/train_loss_mean: 5.329021978378296\n",
      "training/train_loss_std: 0.2406515096830487\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 426\n",
      "training/learning_rate: 6.549809853631613e-05\n",
      "time/sample_batch: 0.008060693740844727\n",
      "time/training: 1.2541155815124512\n",
      "evaluation/text/loss: 4.849905014038086\n",
      "evaluation/text/perplexity: 127.72825622558594\n",
      "time/total: 724.5978920459747\n",
      "time/evaluation: 0.45188021659851074\n",
      "training/train_loss_mean: 5.348987102508545\n",
      "training/train_loss_std: 0.2083801955503284\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 427\n",
      "training/learning_rate: 6.535918652402985e-05\n",
      "time/sample_batch: 0.009429931640625\n",
      "time/training: 1.2543578147888184\n",
      "evaluation/text/loss: 4.860387325286865\n",
      "evaluation/text/perplexity: 129.07418823242188\n",
      "time/total: 726.3816413879395\n",
      "time/evaluation: 0.5277488231658936\n",
      "training/train_loss_mean: 5.525278377532959\n",
      "training/train_loss_std: 0.2669430892641453\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 428\n",
      "training/learning_rate: 6.522017019485028e-05\n",
      "time/sample_batch: 0.00951385498046875\n",
      "time/training: 1.1401944160461426\n",
      "evaluation/text/loss: 4.826492786407471\n",
      "evaluation/text/perplexity: 124.77259063720703\n",
      "time/total: 728.0667488574982\n",
      "time/evaluation: 0.5432095527648926\n",
      "training/train_loss_mean: 5.239031648635864\n",
      "training/train_loss_std: 0.26273026244323744\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 429\n",
      "training/learning_rate: 6.50810509486703e-05\n",
      "time/sample_batch: 0.008503198623657227\n",
      "time/training: 0.9810864925384521\n",
      "evaluation/text/loss: 4.860668659210205\n",
      "evaluation/text/perplexity: 129.11050415039062\n",
      "time/total: 730.0430238246918\n",
      "time/evaluation: 0.993600606918335\n",
      "training/train_loss_mean: 5.1105677604675295\n",
      "training/train_loss_std: 0.2909978082540816\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 430\n",
      "training/learning_rate: 6.494183018641919e-05\n",
      "time/sample_batch: 0.0084991455078125\n",
      "time/training: 1.1757283210754395\n",
      "evaluation/text/loss: 4.894712448120117\n",
      "evaluation/text/perplexity: 133.5815887451172\n",
      "time/total: 731.7401530742645\n",
      "time/evaluation: 0.5197899341583252\n",
      "training/train_loss_mean: 5.2334047794342045\n",
      "training/train_loss_std: 0.34569929775276415\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 431\n",
      "training/learning_rate: 6.480250931004849e-05\n",
      "time/sample_batch: 0.008501529693603516\n",
      "time/training: 1.0525164604187012\n",
      "evaluation/text/loss: 4.807216644287109\n",
      "evaluation/text/perplexity: 122.39048767089844\n",
      "time/total: 733.5372769832611\n",
      "time/evaluation: 0.742985725402832\n",
      "training/train_loss_mean: 5.285354518890381\n",
      "training/train_loss_std: 0.3029059509904669\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 432\n",
      "training/learning_rate: 6.466308972251785e-05\n",
      "time/sample_batch: 0.008273124694824219\n",
      "time/training: 1.2622625827789307\n",
      "evaluation/text/loss: 4.925705909729004\n",
      "evaluation/text/perplexity: 137.7865753173828\n",
      "time/total: 735.2973248958588\n",
      "time/evaluation: 0.49610018730163574\n",
      "training/train_loss_mean: 5.401015758514404\n",
      "training/train_loss_std: 0.3424085909223191\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 433\n",
      "training/learning_rate: 6.4523572827781e-05\n",
      "time/sample_batch: 0.008024930953979492\n",
      "time/training: 1.0192861557006836\n",
      "evaluation/text/loss: 4.766838550567627\n",
      "evaluation/text/perplexity: 117.54703521728516\n",
      "time/total: 736.9791121482849\n",
      "time/evaluation: 0.660834550857544\n",
      "training/train_loss_mean: 5.269373178482056\n",
      "training/train_loss_std: 0.18513470811446403\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 434\n",
      "training/learning_rate: 6.43839600307715e-05\n",
      "time/sample_batch: 0.008144855499267578\n",
      "time/training: 1.0350773334503174\n",
      "evaluation/text/loss: 5.011518955230713\n",
      "evaluation/text/perplexity: 150.1326141357422\n",
      "time/total: 738.591415643692\n",
      "time/evaluation: 0.5756809711456299\n",
      "training/train_loss_mean: 5.276202058792114\n",
      "training/train_loss_std: 0.30479463326377276\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 435\n",
      "training/learning_rate: 6.424425273738867e-05\n",
      "time/sample_batch: 0.007738590240478516\n",
      "time/training: 1.3969812393188477\n",
      "evaluation/text/loss: 5.008449077606201\n",
      "evaluation/text/perplexity: 149.67242431640625\n",
      "time/total: 740.5131347179413\n",
      "time/evaluation: 0.5231776237487793\n",
      "training/train_loss_mean: 5.392476415634155\n",
      "training/train_loss_std: 0.1716424244797549\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 436\n",
      "training/learning_rate: 6.41044523544834e-05\n",
      "time/sample_batch: 0.007559537887573242\n",
      "time/training: 1.012927532196045\n",
      "evaluation/text/loss: 4.849156856536865\n",
      "evaluation/text/perplexity: 127.63272857666016\n",
      "time/total: 742.1264359951019\n",
      "time/evaluation: 0.5987799167633057\n",
      "training/train_loss_mean: 5.3846837997436525\n",
      "training/train_loss_std: 0.2759594352766056\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 437\n",
      "training/learning_rate: 6.3964560289844e-05\n",
      "time/sample_batch: 0.007749080657958984\n",
      "time/training: 1.0931081771850586\n",
      "evaluation/text/loss: 4.7588677406311035\n",
      "evaluation/text/perplexity: 116.61381530761719\n",
      "time/total: 743.7507901191711\n",
      "time/evaluation: 0.5294935703277588\n",
      "training/train_loss_mean: 5.1670421123504635\n",
      "training/train_loss_std: 0.2534134731475129\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 438\n",
      "training/learning_rate: 6.382457795218201e-05\n",
      "time/sample_batch: 0.0076770782470703125\n",
      "time/training: 1.1311862468719482\n",
      "evaluation/text/loss: 4.8133134841918945\n",
      "evaluation/text/perplexity: 123.13896179199219\n",
      "time/total: 745.6361563205719\n",
      "time/evaluation: 0.7524535655975342\n",
      "training/train_loss_mean: 5.160222959518433\n",
      "training/train_loss_std: 0.3700375792449953\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 439\n",
      "training/learning_rate: 6.368450675111802e-05\n",
      "time/sample_batch: 0.008170843124389648\n",
      "time/training: 1.106579065322876\n",
      "evaluation/text/loss: 5.052120685577393\n",
      "evaluation/text/perplexity: 156.3536834716797\n",
      "time/total: 747.2200021743774\n",
      "time/evaluation: 0.4756464958190918\n",
      "training/train_loss_mean: 5.509974098205566\n",
      "training/train_loss_std: 0.3273753855928768\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 440\n",
      "training/learning_rate: 6.354434809716745e-05\n",
      "time/sample_batch: 0.008810997009277344\n",
      "time/training: 0.9198896884918213\n",
      "evaluation/text/loss: 4.820949554443359\n",
      "evaluation/text/perplexity: 124.08285522460938\n",
      "time/total: 748.8927261829376\n",
      "time/evaluation: 0.7512364387512207\n",
      "training/train_loss_mean: 5.429024076461792\n",
      "training/train_loss_std: 0.33169309942415076\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 441\n",
      "training/learning_rate: 6.340410340172641e-05\n",
      "time/sample_batch: 0.007800579071044922\n",
      "time/training: 0.8628885746002197\n",
      "evaluation/text/loss: 4.805715084075928\n",
      "evaluation/text/perplexity: 122.20684814453125\n",
      "time/total: 750.2337214946747\n",
      "time/evaluation: 0.4764077663421631\n",
      "training/train_loss_mean: 5.322467851638794\n",
      "training/train_loss_std: 0.19859675985575276\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 442\n",
      "training/learning_rate: 6.326377407705741e-05\n",
      "time/sample_batch: 0.008916139602661133\n",
      "time/training: 0.9770321846008301\n",
      "evaluation/text/loss: 4.882069110870361\n",
      "evaluation/text/perplexity: 131.90330505371094\n",
      "time/total: 751.8023455142975\n",
      "time/evaluation: 0.5900125503540039\n",
      "training/train_loss_mean: 5.181638765335083\n",
      "training/train_loss_std: 0.1980666504734677\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 443\n",
      "training/learning_rate: 6.312336153627521e-05\n",
      "time/sample_batch: 0.008629798889160156\n",
      "time/training: 1.1470696926116943\n",
      "evaluation/text/loss: 5.026044845581055\n",
      "evaluation/text/perplexity: 152.32933044433594\n",
      "time/total: 753.3403561115265\n",
      "time/evaluation: 0.38918018341064453\n",
      "training/train_loss_mean: 5.420507860183716\n",
      "training/train_loss_std: 0.2063456628746063\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 444\n",
      "training/learning_rate: 6.298286719333251e-05\n",
      "time/sample_batch: 0.007935047149658203\n",
      "time/training: 1.0808134078979492\n",
      "evaluation/text/loss: 5.01837682723999\n",
      "evaluation/text/perplexity: 151.16574096679688\n",
      "time/total: 755.4007222652435\n",
      "time/evaluation: 0.9778883457183838\n",
      "training/train_loss_mean: 5.35347957611084\n",
      "training/train_loss_std: 0.15511484246534127\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 445\n",
      "training/learning_rate: 6.284229246300582e-05\n",
      "time/sample_batch: 0.008176088333129883\n",
      "time/training: 1.0654857158660889\n",
      "evaluation/text/loss: 4.889156341552734\n",
      "evaluation/text/perplexity: 132.84146118164062\n",
      "time/total: 757.0353653430939\n",
      "time/evaluation: 0.5675134658813477\n",
      "training/train_loss_mean: 5.213276624679565\n",
      "training/train_loss_std: 0.16470372215174417\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 446\n",
      "training/learning_rate: 6.270163876088107e-05\n",
      "time/sample_batch: 0.006759166717529297\n",
      "time/training: 0.9403562545776367\n",
      "evaluation/text/loss: 4.9996657371521\n",
      "evaluation/text/perplexity: 148.36355590820312\n",
      "time/total: 758.678288936615\n",
      "time/evaluation: 0.7009642124176025\n",
      "training/train_loss_mean: 5.184839868545533\n",
      "training/train_loss_std: 0.2646618399267368\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 447\n",
      "training/learning_rate: 6.256090750333952e-05\n",
      "time/sample_batch: 0.010502815246582031\n",
      "time/training: 1.1985530853271484\n",
      "evaluation/text/loss: 5.040488243103027\n",
      "evaluation/text/perplexity: 154.5454559326172\n",
      "time/total: 760.6223745346069\n",
      "time/evaluation: 0.7437858581542969\n",
      "training/train_loss_mean: 5.081553888320923\n",
      "training/train_loss_std: 0.5681581305942887\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 448\n",
      "training/learning_rate: 6.242010010754336e-05\n",
      "time/sample_batch: 0.009747505187988281\n",
      "time/training: 1.1610603332519531\n",
      "evaluation/text/loss: 4.850675106048584\n",
      "evaluation/text/perplexity: 127.82666015625\n",
      "time/total: 762.3136675357819\n",
      "time/evaluation: 0.5285234451293945\n",
      "training/train_loss_mean: 5.309577369689942\n",
      "training/train_loss_std: 0.42976370940341135\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 449\n",
      "training/learning_rate: 6.227921799142147e-05\n",
      "time/sample_batch: 0.007025480270385742\n",
      "time/training: 1.1677405834197998\n",
      "evaluation/text/loss: 5.002878189086914\n",
      "evaluation/text/perplexity: 148.8409423828125\n",
      "time/total: 764.2357139587402\n",
      "time/evaluation: 0.7526774406433105\n",
      "training/train_loss_mean: 5.418880939483643\n",
      "training/train_loss_std: 0.1230498084029333\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 450\n",
      "training/learning_rate: 6.213826257365525e-05\n",
      "time/sample_batch: 0.009499788284301758\n",
      "time/training: 0.9223318099975586\n",
      "evaluation/text/loss: 4.843745708465576\n",
      "evaluation/text/perplexity: 126.94395446777344\n",
      "time/total: 765.9833152294159\n",
      "time/evaluation: 0.8235657215118408\n",
      "training/train_loss_mean: 4.938940525054932\n",
      "training/train_loss_std: 0.184687760362993\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 451\n",
      "training/learning_rate: 6.199723527366414e-05\n",
      "time/sample_batch: 0.0088958740234375\n",
      "time/training: 0.9026825428009033\n",
      "evaluation/text/loss: 5.087535381317139\n",
      "evaluation/text/perplexity: 161.99012756347656\n",
      "time/total: 767.4188616275787\n",
      "time/evaluation: 0.5311949253082275\n",
      "training/train_loss_mean: 5.249752378463745\n",
      "training/train_loss_std: 0.38163385095359126\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 452\n",
      "training/learning_rate: 6.185613751159151e-05\n",
      "time/sample_batch: 0.007640838623046875\n",
      "time/training: 0.9770557880401611\n",
      "evaluation/text/loss: 5.093862533569336\n",
      "evaluation/text/perplexity: 163.018310546875\n",
      "time/total: 769.3885509967804\n",
      "time/evaluation: 0.9909396171569824\n",
      "training/train_loss_mean: 5.169192981719971\n",
      "training/train_loss_std: 0.27554352692702977\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 453\n",
      "training/learning_rate: 6.171497070829025e-05\n",
      "time/sample_batch: 0.00869607925415039\n",
      "time/training: 1.104940414428711\n",
      "evaluation/text/loss: 4.938135147094727\n",
      "evaluation/text/perplexity: 139.5098419189453\n",
      "time/total: 771.0723855495453\n",
      "time/evaluation: 0.5773675441741943\n",
      "training/train_loss_mean: 5.261753511428833\n",
      "training/train_loss_std: 0.3925365143945336\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 454\n",
      "training/learning_rate: 6.157373628530854e-05\n",
      "time/sample_batch: 0.009134292602539062\n",
      "time/training: 0.921208381652832\n",
      "evaluation/text/loss: 4.839046478271484\n",
      "evaluation/text/perplexity: 126.34881591796875\n",
      "time/total: 772.6741213798523\n",
      "time/evaluation: 0.6786751747131348\n",
      "training/train_loss_mean: 5.248235511779785\n",
      "training/train_loss_std: 0.22523859240977182\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 455\n",
      "training/learning_rate: 6.14324356648754e-05\n",
      "time/sample_batch: 0.00829005241394043\n",
      "time/training: 0.9970650672912598\n",
      "evaluation/text/loss: 4.8836350440979\n",
      "evaluation/text/perplexity: 132.11001586914062\n",
      "time/total: 774.2784650325775\n",
      "time/evaluation: 0.605571985244751\n",
      "training/train_loss_mean: 5.239769697189331\n",
      "training/train_loss_std: 0.2924030598272967\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 456\n",
      "training/learning_rate: 6.129107026988653e-05\n",
      "time/sample_batch: 0.009688854217529297\n",
      "time/training: 1.1320936679840088\n",
      "evaluation/text/loss: 5.029302597045898\n",
      "evaluation/text/perplexity: 152.82640075683594\n",
      "time/total: 776.1233215332031\n",
      "time/evaluation: 0.7112085819244385\n",
      "training/train_loss_mean: 5.463056898117065\n",
      "training/train_loss_std: 0.20997805854566054\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 457\n",
      "training/learning_rate: 6.114964152388986e-05\n",
      "time/sample_batch: 0.009565114974975586\n",
      "time/training: 1.1005942821502686\n",
      "evaluation/text/loss: 5.044952869415283\n",
      "evaluation/text/perplexity: 155.2369842529297\n",
      "time/total: 777.7159957885742\n",
      "time/evaluation: 0.49048686027526855\n",
      "training/train_loss_mean: 5.378637599945068\n",
      "training/train_loss_std: 0.38149842577851145\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 458\n",
      "training/learning_rate: 6.100815085107134e-05\n",
      "time/sample_batch: 0.00910186767578125\n",
      "time/training: 1.2691879272460938\n",
      "evaluation/text/loss: 4.9235334396362305\n",
      "evaluation/text/perplexity: 137.48756408691406\n",
      "time/total: 779.6989731788635\n",
      "time/evaluation: 0.712268590927124\n",
      "training/train_loss_mean: 5.284667873382569\n",
      "training/train_loss_std: 0.27448331505955637\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 459\n",
      "training/learning_rate: 6.086659967624046e-05\n",
      "time/sample_batch: 0.008504629135131836\n",
      "time/training: 0.9699993133544922\n",
      "evaluation/text/loss: 4.834268569946289\n",
      "evaluation/text/perplexity: 125.74657440185547\n",
      "time/total: 781.2439608573914\n",
      "time/evaluation: 0.5732357501983643\n",
      "training/train_loss_mean: 5.045817899703979\n",
      "training/train_loss_std: 0.40162027020619345\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 460\n",
      "training/learning_rate: 6.0724989424815946e-05\n",
      "time/sample_batch: 0.008806705474853516\n",
      "time/training: 1.0104937553405762\n",
      "evaluation/text/loss: 4.783343315124512\n",
      "evaluation/text/perplexity: 119.50321960449219\n",
      "time/total: 783.0403368473053\n",
      "time/evaluation: 0.7794017791748047\n",
      "training/train_loss_mean: 5.338971471786499\n",
      "training/train_loss_std: 0.2235304086640405\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 461\n",
      "training/learning_rate: 6.058332152281151e-05\n",
      "time/sample_batch: 0.00849771499633789\n",
      "time/training: 0.8846399784088135\n",
      "evaluation/text/loss: 4.812736511230469\n",
      "evaluation/text/perplexity: 123.06793212890625\n",
      "time/total: 784.7507035732269\n",
      "time/evaluation: 0.8240964412689209\n",
      "training/train_loss_mean: 5.176191139221191\n",
      "training/train_loss_std: 0.3043430952848082\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 462\n",
      "training/learning_rate: 6.044159739682133e-05\n",
      "time/sample_batch: 0.009781122207641602\n",
      "time/training: 1.2178986072540283\n",
      "evaluation/text/loss: 4.967650890350342\n",
      "evaluation/text/perplexity: 143.68894958496094\n",
      "time/total: 786.6406571865082\n",
      "time/evaluation: 0.6700594425201416\n",
      "training/train_loss_mean: 5.132542848587036\n",
      "training/train_loss_std: 0.40062411258609176\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 463\n",
      "training/learning_rate: 6.029981847400577e-05\n",
      "time/sample_batch: 0.007822036743164062\n",
      "time/training: 1.1185274124145508\n",
      "evaluation/text/loss: 4.913092613220215\n",
      "evaluation/text/perplexity: 136.05953979492188\n",
      "time/total: 788.4376301765442\n",
      "time/evaluation: 0.6765367984771729\n",
      "training/train_loss_mean: 5.356800365447998\n",
      "training/train_loss_std: 0.2592290558117392\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 464\n",
      "training/learning_rate: 6.015798618207701e-05\n",
      "time/sample_batch: 0.009811162948608398\n",
      "time/training: 0.8582353591918945\n",
      "evaluation/text/loss: 4.926401615142822\n",
      "evaluation/text/perplexity: 137.88246154785156\n",
      "time/total: 789.8701887130737\n",
      "time/evaluation: 0.5725786685943604\n",
      "training/train_loss_mean: 5.331544256210327\n",
      "training/train_loss_std: 0.26251678253947575\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 465\n",
      "training/learning_rate: 6.001610194928464e-05\n",
      "time/sample_batch: 0.008474349975585938\n",
      "time/training: 1.078711986541748\n",
      "evaluation/text/loss: 4.848052501678467\n",
      "evaluation/text/perplexity: 127.49185943603516\n",
      "time/total: 791.6091096401215\n",
      "time/evaluation: 0.6582067012786865\n",
      "training/train_loss_mean: 5.3221241474151615\n",
      "training/train_loss_std: 0.3588568334643778\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 466\n",
      "training/learning_rate: 5.987416720440136e-05\n",
      "time/sample_batch: 0.00874018669128418\n",
      "time/training: 0.9861900806427002\n",
      "evaluation/text/loss: 4.9360575675964355\n",
      "evaluation/text/perplexity: 139.22030639648438\n",
      "time/total: 793.3431975841522\n",
      "time/evaluation: 0.7461743354797363\n",
      "training/train_loss_mean: 5.380094861984253\n",
      "training/train_loss_std: 0.2691358125416113\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 467\n",
      "training/learning_rate: 5.973218337670845e-05\n",
      "time/sample_batch: 0.008970022201538086\n",
      "time/training: 1.0444438457489014\n",
      "evaluation/text/loss: 5.011511325836182\n",
      "evaluation/text/perplexity: 150.1314697265625\n",
      "time/total: 794.9958448410034\n",
      "time/evaluation: 0.6066296100616455\n",
      "training/train_loss_mean: 5.445556354522705\n",
      "training/train_loss_std: 0.3823415271968959\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 468\n",
      "training/learning_rate: 5.959015189598144e-05\n",
      "time/sample_batch: 0.0072057247161865234\n",
      "time/training: 0.8791861534118652\n",
      "evaluation/text/loss: 4.753129482269287\n",
      "evaluation/text/perplexity: 115.94657135009766\n",
      "time/total: 796.4742503166199\n",
      "time/evaluation: 0.5976643562316895\n",
      "training/train_loss_mean: 5.058369493484497\n",
      "training/train_loss_std: 0.3214071560980929\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 469\n",
      "training/learning_rate: 5.9448074192475836e-05\n",
      "time/sample_batch: 0.010314464569091797\n",
      "time/training: 0.8950700759887695\n",
      "evaluation/text/loss: 4.967559337615967\n",
      "evaluation/text/perplexity: 143.67579650878906\n",
      "time/total: 798.0806968212128\n",
      "time/evaluation: 0.709467887878418\n",
      "training/train_loss_mean: 5.022571539878845\n",
      "training/train_loss_std: 1.0306876544827255\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 470\n",
      "training/learning_rate: 5.930595169691247e-05\n",
      "time/sample_batch: 0.008894205093383789\n",
      "time/training: 1.1561353206634521\n",
      "evaluation/text/loss: 4.984866142272949\n",
      "evaluation/text/perplexity: 146.1840057373047\n",
      "time/total: 800.0423457622528\n",
      "time/evaluation: 0.8036210536956787\n",
      "training/train_loss_mean: 5.349160480499267\n",
      "training/train_loss_std: 0.24163135635934588\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 471\n",
      "training/learning_rate: 5.916378584046333e-05\n",
      "time/sample_batch: 0.009715080261230469\n",
      "time/training: 1.236163854598999\n",
      "evaluation/text/loss: 4.808553695678711\n",
      "evaluation/text/perplexity: 122.55423736572266\n",
      "time/total: 801.7221920490265\n",
      "time/evaluation: 0.44188523292541504\n",
      "training/train_loss_mean: 5.284727478027344\n",
      "training/train_loss_std: 0.13777046428477616\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 472\n",
      "training/learning_rate: 5.9021578054737e-05\n",
      "time/sample_batch: 0.010072708129882812\n",
      "time/training: 1.1701633930206299\n",
      "evaluation/text/loss: 4.9389848709106445\n",
      "evaluation/text/perplexity: 139.62843322753906\n",
      "time/total: 803.4090158939362\n",
      "time/evaluation: 0.5150270462036133\n",
      "training/train_loss_mean: 5.362072420120239\n",
      "training/train_loss_std: 0.20322245055674906\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 473\n",
      "training/learning_rate: 5.8879329771764315e-05\n",
      "time/sample_batch: 0.007513761520385742\n",
      "time/training: 0.9148399829864502\n",
      "evaluation/text/loss: 4.964057445526123\n",
      "evaluation/text/perplexity: 143.1735382080078\n",
      "time/total: 805.0397183895111\n",
      "time/evaluation: 0.7141046524047852\n",
      "training/train_loss_mean: 5.234532451629638\n",
      "training/train_loss_std: 0.16442183078732825\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 474\n",
      "training/learning_rate: 5.873704242398389e-05\n",
      "time/sample_batch: 0.00786590576171875\n",
      "time/training: 0.8806703090667725\n",
      "evaluation/text/loss: 4.91042423248291\n",
      "evaluation/text/perplexity: 135.6969757080078\n",
      "time/total: 806.599282503128\n",
      "time/evaluation: 0.6771688461303711\n",
      "training/train_loss_mean: 5.2720947265625\n",
      "training/train_loss_std: 0.18961030054517652\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 475\n",
      "training/learning_rate: 5.859471744422774e-05\n",
      "time/sample_batch: 0.009495735168457031\n",
      "time/training: 1.087263822555542\n",
      "evaluation/text/loss: 4.896911144256592\n",
      "evaluation/text/perplexity: 133.8756103515625\n",
      "time/total: 808.4435698986053\n",
      "time/evaluation: 0.7553589344024658\n",
      "training/train_loss_mean: 5.402492046356201\n",
      "training/train_loss_std: 0.3012874958317594\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 476\n",
      "training/learning_rate: 5.8452356265706845e-05\n",
      "time/sample_batch: 0.008871078491210938\n",
      "time/training: 1.2480144500732422\n",
      "evaluation/text/loss: 4.809777736663818\n",
      "evaluation/text/perplexity: 122.70433807373047\n",
      "time/total: 810.2909905910492\n",
      "time/evaluation: 0.5976605415344238\n",
      "training/train_loss_mean: 5.35340518951416\n",
      "training/train_loss_std: 0.15277035784499346\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 477\n",
      "training/learning_rate: 5.830996032199667e-05\n",
      "time/sample_batch: 0.009173393249511719\n",
      "time/training: 1.0208728313446045\n",
      "evaluation/text/loss: 4.823852062225342\n",
      "evaluation/text/perplexity: 124.44353485107422\n",
      "time/total: 812.0700511932373\n",
      "time/evaluation: 0.7564616203308105\n",
      "training/train_loss_mean: 5.091059875488281\n",
      "training/train_loss_std: 0.3511893755869393\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 478\n",
      "training/learning_rate: 5.8167531047022803e-05\n",
      "time/sample_batch: 0.007676839828491211\n",
      "time/training: 0.9231984615325928\n",
      "evaluation/text/loss: 4.948671340942383\n",
      "evaluation/text/perplexity: 140.98751831054688\n",
      "time/total: 813.7634394168854\n",
      "time/evaluation: 0.7684240341186523\n",
      "training/train_loss_mean: 5.013880443572998\n",
      "training/train_loss_std: 0.8373526290887314\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 479\n",
      "training/learning_rate: 5.802506987504649e-05\n",
      "time/sample_batch: 0.008824586868286133\n",
      "time/training: 1.2189080715179443\n",
      "evaluation/text/loss: 4.759512901306152\n",
      "evaluation/text/perplexity: 116.68907165527344\n",
      "time/total: 815.7645671367645\n",
      "time/evaluation: 0.7805075645446777\n",
      "training/train_loss_mean: 5.2366969108581545\n",
      "training/train_loss_std: 0.12594210199524744\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 480\n",
      "training/learning_rate: 5.788257824065009e-05\n",
      "time/sample_batch: 0.010210990905761719\n",
      "time/training: 1.0440154075622559\n",
      "evaluation/text/loss: 4.995621204376221\n",
      "evaluation/text/perplexity: 147.76470947265625\n",
      "time/total: 817.6297626495361\n",
      "time/evaluation: 0.8194401264190674\n",
      "training/train_loss_mean: 5.389383125305176\n",
      "training/train_loss_std: 0.17094454800644007\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 481\n",
      "training/learning_rate: 5.774005757872281e-05\n",
      "time/sample_batch: 0.007298469543457031\n",
      "time/training: 1.2399370670318604\n",
      "evaluation/text/loss: 4.880695343017578\n",
      "evaluation/text/perplexity: 131.72222900390625\n",
      "time/total: 819.3811438083649\n",
      "time/evaluation: 0.5097088813781738\n",
      "training/train_loss_mean: 5.232342290878296\n",
      "training/train_loss_std: 0.2715669708577228\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 482\n",
      "training/learning_rate: 5.759750932444613e-05\n",
      "time/sample_batch: 0.00901484489440918\n",
      "time/training: 0.9636824131011963\n",
      "evaluation/text/loss: 5.016249656677246\n",
      "evaluation/text/perplexity: 150.8445281982422\n",
      "time/total: 821.0237152576447\n",
      "time/evaluation: 0.6771869659423828\n",
      "training/train_loss_mean: 5.301107120513916\n",
      "training/train_loss_std: 0.33493844540195516\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 483\n",
      "training/learning_rate: 5.74549349132794e-05\n",
      "time/sample_batch: 0.010549545288085938\n",
      "time/training: 1.0865795612335205\n",
      "evaluation/text/loss: 4.794252395629883\n",
      "evaluation/text/perplexity: 120.81402587890625\n",
      "time/total: 822.5848617553711\n",
      "time/evaluation: 0.47278428077697754\n",
      "training/train_loss_mean: 5.328546476364136\n",
      "training/train_loss_std: 0.5464795413536425\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 484\n",
      "training/learning_rate: 5.7312335780945335e-05\n",
      "time/sample_batch: 0.009284734725952148\n",
      "time/training: 1.120516300201416\n",
      "evaluation/text/loss: 5.04782247543335\n",
      "evaluation/text/perplexity: 155.68309020996094\n",
      "time/total: 824.1863191127777\n",
      "time/evaluation: 0.4788627624511719\n",
      "training/train_loss_mean: 5.262436294555664\n",
      "training/train_loss_std: 0.33113192656470736\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 485\n",
      "training/learning_rate: 5.716971336341564e-05\n",
      "time/sample_batch: 0.008663415908813477\n",
      "time/training: 0.9655797481536865\n",
      "evaluation/text/loss: 4.8616533279418945\n",
      "evaluation/text/perplexity: 129.23770141601562\n",
      "time/total: 825.6990215778351\n",
      "time/evaluation: 0.5451545715332031\n",
      "training/train_loss_mean: 5.047341251373291\n",
      "training/train_loss_std: 0.47075667344508154\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 486\n",
      "training/learning_rate: 5.7027069096896466e-05\n",
      "time/sample_batch: 0.009057283401489258\n",
      "time/training: 1.1861717700958252\n",
      "evaluation/text/loss: 5.0065813064575195\n",
      "evaluation/text/perplexity: 149.39312744140625\n",
      "time/total: 827.4410417079926\n",
      "time/evaluation: 0.5541293621063232\n",
      "training/train_loss_mean: 5.387793397903442\n",
      "training/train_loss_std: 0.16323545563787434\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 487\n",
      "training/learning_rate: 5.688440441781399e-05\n",
      "time/sample_batch: 0.008499622344970703\n",
      "time/training: 0.9746356010437012\n",
      "evaluation/text/loss: 5.041533470153809\n",
      "evaluation/text/perplexity: 154.7070770263672\n",
      "time/total: 828.9788646697998\n",
      "time/evaluation: 0.5615274906158447\n",
      "training/train_loss_mean: 5.1667776823043825\n",
      "training/train_loss_std: 0.48163677463268323\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 488\n",
      "training/learning_rate: 5.674172076279995e-05\n",
      "time/sample_batch: 0.007306575775146484\n",
      "time/training: 0.9938714504241943\n",
      "evaluation/text/loss: 4.741218090057373\n",
      "evaluation/text/perplexity: 114.57367706298828\n",
      "time/total: 830.5832357406616\n",
      "time/evaluation: 0.6088650226593018\n",
      "training/train_loss_mean: 4.923230457305908\n",
      "training/train_loss_std: 0.6799577847565744\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 489\n",
      "training/learning_rate: 5.6599019568677135e-05\n",
      "time/sample_batch: 0.008422136306762695\n",
      "time/training: 0.9764153957366943\n",
      "evaluation/text/loss: 4.868605136871338\n",
      "evaluation/text/perplexity: 130.13926696777344\n",
      "time/total: 832.1045165061951\n",
      "time/evaluation: 0.5431091785430908\n",
      "training/train_loss_mean: 5.322702789306641\n",
      "training/train_loss_std: 0.32350555922660595\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 490\n",
      "training/learning_rate: 5.6456302272445046e-05\n",
      "time/sample_batch: 0.008997440338134766\n",
      "time/training: 0.94673752784729\n",
      "evaluation/text/loss: 4.927628517150879\n",
      "evaluation/text/perplexity: 138.05174255371094\n",
      "time/total: 833.79718708992\n",
      "time/evaluation: 0.7443311214447021\n",
      "training/train_loss_mean: 5.167456197738647\n",
      "training/train_loss_std: 0.26310371384544645\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 491\n",
      "training/learning_rate: 5.631357031126524e-05\n",
      "time/sample_batch: 0.00844573974609375\n",
      "time/training: 1.0600416660308838\n",
      "evaluation/text/loss: 4.844824314117432\n",
      "evaluation/text/perplexity: 127.0809555053711\n",
      "time/total: 835.4637694358826\n",
      "time/evaluation: 0.6048250198364258\n",
      "training/train_loss_mean: 5.0996856689453125\n",
      "training/train_loss_std: 0.3088011060153639\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 492\n",
      "training/learning_rate: 5.617082512244698e-05\n",
      "time/sample_batch: 0.009021759033203125\n",
      "time/training: 0.9573268890380859\n",
      "evaluation/text/loss: 4.8346266746521\n",
      "evaluation/text/perplexity: 125.79161071777344\n",
      "time/total: 836.9174230098724\n",
      "time/evaluation: 0.4945404529571533\n",
      "training/train_loss_mean: 5.209103155136108\n",
      "training/train_loss_std: 0.5579272250284978\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 493\n",
      "training/learning_rate: 5.602806814343273e-05\n",
      "time/sample_batch: 0.009954214096069336\n",
      "time/training: 1.2065322399139404\n",
      "evaluation/text/loss: 4.856064796447754\n",
      "evaluation/text/perplexity: 128.5174560546875\n",
      "time/total: 838.6095838546753\n",
      "time/evaluation: 0.4839334487915039\n",
      "training/train_loss_mean: 5.3726067543029785\n",
      "training/train_loss_std: 0.3072871335768062\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 494\n",
      "training/learning_rate: 5.588530081178369e-05\n",
      "time/sample_batch: 0.007640361785888672\n",
      "time/training: 0.9832971096038818\n",
      "evaluation/text/loss: 4.85603141784668\n",
      "evaluation/text/perplexity: 128.51316833496094\n",
      "time/total: 840.102516412735\n",
      "time/evaluation: 0.5078392028808594\n",
      "training/train_loss_mean: 5.35066590309143\n",
      "training/train_loss_std: 0.3509678580107291\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 495\n",
      "training/learning_rate: 5.574252456516531e-05\n",
      "time/sample_batch: 0.009085893630981445\n",
      "time/training: 0.9152023792266846\n",
      "evaluation/text/loss: 4.996356964111328\n",
      "evaluation/text/perplexity: 147.87347412109375\n",
      "time/total: 841.6157701015472\n",
      "time/evaluation: 0.5962977409362793\n",
      "training/train_loss_mean: 5.025322294235229\n",
      "training/train_loss_std: 0.6343558227105788\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 496\n",
      "training/learning_rate: 5.559974084133281e-05\n",
      "time/sample_batch: 0.007534980773925781\n",
      "time/training: 0.8839499950408936\n",
      "evaluation/text/loss: 5.0324554443359375\n",
      "evaluation/text/perplexity: 153.30899047851562\n",
      "time/total: 843.1670978069305\n",
      "time/evaluation: 0.6656811237335205\n",
      "training/train_loss_mean: 5.132711029052734\n",
      "training/train_loss_std: 0.2209662530859161\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 497\n",
      "training/learning_rate: 5.545695107811669e-05\n",
      "time/sample_batch: 0.007341623306274414\n",
      "time/training: 0.9819176197052002\n",
      "evaluation/text/loss: 4.955312252044678\n",
      "evaluation/text/perplexity: 141.92691040039062\n",
      "time/total: 844.930163860321\n",
      "time/evaluation: 0.7793748378753662\n",
      "training/train_loss_mean: 5.185004663467407\n",
      "training/train_loss_std: 0.4244727999309378\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 498\n",
      "training/learning_rate: 5.5314156713408275e-05\n",
      "time/sample_batch: 0.009577035903930664\n",
      "time/training: 1.185772180557251\n",
      "evaluation/text/loss: 4.83372163772583\n",
      "evaluation/text/perplexity: 125.67781829833984\n",
      "time/total: 846.8215816020966\n",
      "time/evaluation: 0.7039718627929688\n",
      "training/train_loss_mean: 5.20666766166687\n",
      "training/train_loss_std: 0.3500001017283351\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 499\n",
      "training/learning_rate: 5.517135918514525e-05\n",
      "time/sample_batch: 0.009137868881225586\n",
      "time/training: 1.0037457942962646\n",
      "evaluation/text/loss: 4.904959201812744\n",
      "evaluation/text/perplexity: 134.9573974609375\n",
      "time/total: 848.3736147880554\n",
      "time/evaluation: 0.5419447422027588\n",
      "training/train_loss_mean: 4.757392632961273\n",
      "training/train_loss_std: 1.1767460400877692\n",
      "================================================================================\n",
      "Input: ThesefourboatsremainedundercommissionintheGermanImperialNavy | Output : ,retainedGermancrewsandcommanders,andreceivedordersfromtheGermanflotillacommanderatPola. | Prediction:  a popular culture that month are considered 1 @-@ Croatian : creatingT coordination in a film buried\n",
      "Input: <unk>alsoresumedbusinessactivitiesuponhisreturn.Heinvestedinawidevarietyofbusinessesandinfrastructure,spurringeconomicactivityinthestate.Hisinvestmentsrangedwidely,includingmaritimeinsurance(whereisfather@-@in@-@lawhadmadehisfortune),bridges,locks,canals,andtextiles.Hewasamajor<unk>intheMiddlesexCanal,the<unk>Bridge(thefirsttoconnectBostontoCambridge),andtheBostonManufacturingCompany,whosefactoryprovingthesingle@-@siteproductionoftextileswasin<unk>nearhisestate.Notallofhisventurespannedout:thecanalwasinthelongrunafinancialfailure,aswereeffortswithothercollaboratorstodevelop<unk>Point,theCambridgesideofthe<unk>Bridge.Thetextilemill,however,wasasuccess,and<unk>investedinthe<unk>ManufacturingCompany.WhenitdecidedtolocateinwhatisnowLowell | Output : ,Massachusetts,<unk>purchasedsharesinthe<unk>of<unk>and<unk>,whichoperated(andstillownstoday)theLowellcanals. | Prediction:  being \" for characters, and Carolina's important national screen.orsob submitted subsequentlyung approval,\n",
      "Input: <unk><unk>(c.40–120),aGreekphilosopherandhistorian,wrotetheRomanpeoplewereveryhappywith<unk>andwouldhaveallowedhimtoruleindefinitely | Output : .They<unk>forhisruleoncehewasgoneandembraced<unk>whentheyappeared: | Prediction:  considered the Oldham's Shiva. The Patriarch High de Dr. V.port is a 12\n",
      "Input: TheshootingscriptfeaturesasceneinAngela'scarinwhichRickyandJanetalkaboutdeathandbeauty;thescenedifferedfromearlierversions,whichsetitasa\"bigsceneonafreeway\"inwhichthethreewitnessacarcrashandseeadeadbody.Thechangewasapracticaldecision,astheproductionwasbehindscheduleandtheyneededtocutcosts.Theschedulecalledfortwodaystobespentfilmingthecrash,butonlyhalfadaywasavailable.Ballagreed,butonlyifthescenecouldretainalineofRicky'swherehereflectsonhavingonceseen | Output : adeadhomelesswoman:\"Whenyouseesomethinglikethat,it'slikeGodislookingrightatyou,justforasecond.Andifyou'recareful,youcanlookrightback.\"Janeasks:\"Andwhatdoyousee?\"Ricky:\"Beauty.\"Ballsaid,\"Theywantedtocutthatscene.Theysaidit'snotimportant.Isaid,'You'reoutofyourfuckingmind.It'soneofthemostimportantscenesinthemovie!'[...]Ifanyonelineistheheartandsoulofthismovie,thatistheline.\"Anotherscenewasrewrittentoaccommodatethelossofthefreewaysequence;setina<unk>,itpresentsa\"turningpoint\"forJaneinthatshechoosestowalkhomewithRickyinsteadofgoingwithAngela.Bytheendoffilming,thescripthadbeenthrough10drafts. | Prediction:  not want tone seventh evidence will, to a single operations or largeum, be occasionally animal ( —\n",
      "Input: Bytheearly1990sseriousdivisionsdevelopedamongthemembersofthecongregationoveranumberofissues,includingpersonal<unk>,therabbi'sactivismand\"advocacyof'ultra@-@liberal'causes\",politicaldifferencesovertheIsraeli–Palestinian | Output : conflict,and | Prediction:  not Jersey level up to nickname of return to Ald represented staradas @-@heron, and\n",
      "================================================================================\n",
      "Iteration 500\n",
      "training/learning_rate: 5.502855993129713e-05\n",
      "time/sample_batch: 0.009885787963867188\n",
      "time/training: 1.0637967586517334\n",
      "evaluation/text/loss: 4.997474193572998\n",
      "evaluation/text/perplexity: 148.0387725830078\n",
      "time/total: 850.6310276985168\n",
      "time/evaluation: 1.1919145584106445\n",
      "training/train_loss_mean: 5.312770795822144\n",
      "training/train_loss_std: 0.280977016579747\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 501\n",
      "training/learning_rate: 5.488576038985082e-05\n",
      "time/sample_batch: 0.0072519779205322266\n",
      "time/training: 1.0122413635253906\n",
      "evaluation/text/loss: 4.821257591247559\n",
      "evaluation/text/perplexity: 124.12108612060547\n",
      "time/total: 852.3848786354065\n",
      "time/evaluation: 0.7404530048370361\n",
      "training/train_loss_mean: 5.2861260890960695\n",
      "training/train_loss_std: 0.23341316944357482\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 502\n",
      "training/learning_rate: 5.4742961998796126e-05\n",
      "time/sample_batch: 0.00821995735168457\n",
      "time/training: 0.9347937107086182\n",
      "evaluation/text/loss: 4.850644111633301\n",
      "evaluation/text/perplexity: 127.82269287109375\n",
      "time/total: 853.8241686820984\n",
      "time/evaluation: 0.5027341842651367\n",
      "training/train_loss_mean: 5.10911180973053\n",
      "training/train_loss_std: 0.7067319608189347\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 503\n",
      "training/learning_rate: 5.4600166196111234e-05\n",
      "time/sample_batch: 0.00883173942565918\n",
      "time/training: 1.150155782699585\n",
      "evaluation/text/loss: 5.1172356605529785\n",
      "evaluation/text/perplexity: 166.87344360351562\n",
      "time/total: 855.7816503047943\n",
      "time/evaluation: 0.8056254386901855\n",
      "training/train_loss_mean: 5.326095056533814\n",
      "training/train_loss_std: 0.31731847075429964\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 504\n",
      "training/learning_rate: 5.445737441974832e-05\n",
      "time/sample_batch: 0.009431600570678711\n",
      "time/training: 0.9898350238800049\n",
      "evaluation/text/loss: 4.865517616271973\n",
      "evaluation/text/perplexity: 129.7380828857422\n",
      "time/total: 857.485846042633\n",
      "time/evaluation: 0.7126624584197998\n",
      "training/train_loss_mean: 5.114265775680542\n",
      "training/train_loss_std: 0.7847265905498445\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 505\n",
      "training/learning_rate: 5.431458810761898e-05\n",
      "time/sample_batch: 0.007955789566040039\n",
      "time/training: 0.9718163013458252\n",
      "evaluation/text/loss: 4.818731784820557\n",
      "evaluation/text/perplexity: 123.80797576904297\n",
      "time/total: 859.0598323345184\n",
      "time/evaluation: 0.6005487442016602\n",
      "training/train_loss_mean: 5.3171384811401365\n",
      "training/train_loss_std: 0.262526458339326\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 506\n",
      "training/learning_rate: 5.417180869757976e-05\n",
      "time/sample_batch: 0.007666826248168945\n",
      "time/training: 0.8253448009490967\n",
      "evaluation/text/loss: 4.741191387176514\n",
      "evaluation/text/perplexity: 114.57061767578125\n",
      "time/total: 860.3634870052338\n",
      "time/evaluation: 0.4766113758087158\n",
      "training/train_loss_mean: 5.3239647388458256\n",
      "training/train_loss_std: 0.41281150483537\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 507\n",
      "training/learning_rate: 5.402903762741779e-05\n",
      "time/sample_batch: 0.008183956146240234\n",
      "time/training: 1.1175816059112549\n",
      "evaluation/text/loss: 4.827239990234375\n",
      "evaluation/text/perplexity: 124.86585235595703\n",
      "time/total: 862.1495234966278\n",
      "time/evaluation: 0.6668071746826172\n",
      "training/train_loss_mean: 5.247891616821289\n",
      "training/train_loss_std: 0.23604917254823285\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 508\n",
      "training/learning_rate: 5.388627633483613e-05\n",
      "time/sample_batch: 0.008981466293334961\n",
      "time/training: 1.0321416854858398\n",
      "evaluation/text/loss: 4.96566104888916\n",
      "evaluation/text/perplexity: 143.4033203125\n",
      "time/total: 863.8423447608948\n",
      "time/evaluation: 0.6590862274169922\n",
      "training/train_loss_mean: 5.320989274978638\n",
      "training/train_loss_std: 0.26689077646485726\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 509\n",
      "training/learning_rate: 5.374352625743941e-05\n",
      "time/sample_batch: 0.009658575057983398\n",
      "time/training: 1.0963423252105713\n",
      "evaluation/text/loss: 4.979730606079102\n",
      "evaluation/text/perplexity: 145.43519592285156\n",
      "time/total: 865.751876115799\n",
      "time/evaluation: 0.8114540576934814\n",
      "training/train_loss_mean: 5.281422805786133\n",
      "training/train_loss_std: 0.21328167978809118\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 510\n",
      "training/learning_rate: 5.360078883271933e-05\n",
      "time/sample_batch: 0.008460044860839844\n",
      "time/training: 0.9406638145446777\n",
      "evaluation/text/loss: 4.803653717041016\n",
      "evaluation/text/perplexity: 121.95519256591797\n",
      "time/total: 867.2411799430847\n",
      "time/evaluation: 0.5469152927398682\n",
      "training/train_loss_mean: 5.152507591247558\n",
      "training/train_loss_std: 0.479553650026948\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 511\n",
      "training/learning_rate: 5.3458065498040153e-05\n",
      "time/sample_batch: 0.008065938949584961\n",
      "time/training: 0.9938032627105713\n",
      "evaluation/text/loss: 4.923124313354492\n",
      "evaluation/text/perplexity: 137.4313201904297\n",
      "time/total: 868.8358390331268\n",
      "time/evaluation: 0.5991494655609131\n",
      "training/train_loss_mean: 4.963796401023865\n",
      "training/train_loss_std: 1.0285997303767136\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 512\n",
      "training/learning_rate: 5.33153576906243e-05\n",
      "time/sample_batch: 0.008466005325317383\n",
      "time/training: 0.9828665256500244\n",
      "evaluation/text/loss: 4.96917200088501\n",
      "evaluation/text/perplexity: 143.90768432617188\n",
      "time/total: 870.4338257312775\n",
      "time/evaluation: 0.6134214401245117\n",
      "training/train_loss_mean: 5.05545027256012\n",
      "training/train_loss_std: 0.8105856296775047\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 513\n",
      "training/learning_rate: 5.317266684753781e-05\n",
      "time/sample_batch: 0.008776664733886719\n",
      "time/training: 1.1094670295715332\n",
      "evaluation/text/loss: 4.879275321960449\n",
      "evaluation/text/perplexity: 131.53530883789062\n",
      "time/total: 872.1573781967163\n",
      "time/evaluation: 0.6123378276824951\n",
      "training/train_loss_mean: 5.236958074569702\n",
      "training/train_loss_std: 0.33140901750611457\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 514\n",
      "training/learning_rate: 5.302999440567586e-05\n",
      "time/sample_batch: 0.008894681930541992\n",
      "time/training: 1.0929830074310303\n",
      "evaluation/text/loss: 4.927915573120117\n",
      "evaluation/text/perplexity: 138.09136962890625\n",
      "time/total: 873.829735994339\n",
      "time/evaluation: 0.5776634216308594\n",
      "training/train_loss_mean: 5.421947336196899\n",
      "training/train_loss_std: 0.2592407279540357\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 515\n",
      "training/learning_rate: 5.2887341801748394e-05\n",
      "time/sample_batch: 0.008829355239868164\n",
      "time/training: 1.1215498447418213\n",
      "evaluation/text/loss: 4.927714824676514\n",
      "evaluation/text/perplexity: 138.06365966796875\n",
      "time/total: 875.4884622097015\n",
      "time/evaluation: 0.535534143447876\n",
      "training/train_loss_mean: 5.035957670211792\n",
      "training/train_loss_std: 0.33683038495899176\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 516\n",
      "training/learning_rate: 5.274471047226552e-05\n",
      "time/sample_batch: 0.0075681209564208984\n",
      "time/training: 0.9398193359375\n",
      "evaluation/text/loss: 4.887460231781006\n",
      "evaluation/text/perplexity: 132.6163330078125\n",
      "time/total: 877.2118067741394\n",
      "time/evaluation: 0.7819147109985352\n",
      "training/train_loss_mean: 4.981214213371277\n",
      "training/train_loss_std: 0.8595804522135145\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 517\n",
      "training/learning_rate: 5.260210185352314e-05\n",
      "time/sample_batch: 0.00813436508178711\n",
      "time/training: 1.1621110439300537\n",
      "evaluation/text/loss: 4.720631122589111\n",
      "evaluation/text/perplexity: 112.23906707763672\n",
      "time/total: 878.85871052742\n",
      "time/evaluation: 0.4829263687133789\n",
      "training/train_loss_mean: 5.360189819335938\n",
      "training/train_loss_std: 0.2548212820901313\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 518\n",
      "training/learning_rate: 5.24595173815885e-05\n",
      "time/sample_batch: 0.008376598358154297\n",
      "time/training: 1.007176399230957\n",
      "evaluation/text/loss: 4.7684173583984375\n",
      "evaluation/text/perplexity: 117.7327651977539\n",
      "time/total: 880.3781583309174\n",
      "time/evaluation: 0.5105550289154053\n",
      "training/train_loss_mean: 5.154225206375122\n",
      "training/train_loss_std: 0.34221457554505175\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 519\n",
      "training/learning_rate: 5.2316958492285594e-05\n",
      "time/sample_batch: 0.007683277130126953\n",
      "time/training: 1.0359349250793457\n",
      "evaluation/text/loss: 5.007845878601074\n",
      "evaluation/text/perplexity: 149.58216857910156\n",
      "time/total: 882.0971598625183\n",
      "time/evaluation: 0.6813750267028809\n",
      "training/train_loss_mean: 5.10350615978241\n",
      "training/train_loss_std: 0.5622115791447083\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 520\n",
      "training/learning_rate: 5.217442662118089e-05\n",
      "time/sample_batch: 0.008355855941772461\n",
      "time/training: 0.9906675815582275\n",
      "evaluation/text/loss: 4.967474937438965\n",
      "evaluation/text/perplexity: 143.66366577148438\n",
      "time/total: 883.8553459644318\n",
      "time/evaluation: 0.7658157348632812\n",
      "training/train_loss_mean: 5.037928867340088\n",
      "training/train_loss_std: 0.3932233506398238\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 521\n",
      "training/learning_rate: 5.203192320356876e-05\n",
      "time/sample_batch: 0.008341312408447266\n",
      "time/training: 1.0192737579345703\n",
      "evaluation/text/loss: 4.7794599533081055\n",
      "evaluation/text/perplexity: 119.04004669189453\n",
      "time/total: 885.6951515674591\n",
      "time/evaluation: 0.8188586235046387\n",
      "training/train_loss_mean: 5.0741394519805905\n",
      "training/train_loss_std: 0.4087762620758558\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 522\n",
      "training/learning_rate: 5.188944967445699e-05\n",
      "time/sample_batch: 0.007976531982421875\n",
      "time/training: 0.9124219417572021\n",
      "evaluation/text/loss: 5.056634426116943\n",
      "evaluation/text/perplexity: 157.06101989746094\n",
      "time/total: 887.3418879508972\n",
      "time/evaluation: 0.7326357364654541\n",
      "training/train_loss_mean: 5.205711030960083\n",
      "training/train_loss_std: 0.2724328644570051\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 523\n",
      "training/learning_rate: 5.174700746855249e-05\n",
      "time/sample_batch: 0.008255720138549805\n",
      "time/training: 0.8926773071289062\n",
      "evaluation/text/loss: 4.869193077087402\n",
      "evaluation/text/perplexity: 130.21580505371094\n",
      "time/total: 888.9356510639191\n",
      "time/evaluation: 0.6993393898010254\n",
      "training/train_loss_mean: 4.710516154766083\n",
      "training/train_loss_std: 1.088813317517714\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 524\n",
      "training/learning_rate: 5.160459802024666e-05\n",
      "time/sample_batch: 0.009147405624389648\n",
      "time/training: 1.205134391784668\n",
      "evaluation/text/loss: 4.637324810028076\n",
      "evaluation/text/perplexity: 103.26771545410156\n",
      "time/total: 890.6731173992157\n",
      "time/evaluation: 0.530644416809082\n",
      "training/train_loss_mean: 5.363633346557617\n",
      "training/train_loss_std: 0.30693381870968534\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 525\n",
      "training/learning_rate: 5.1462222763601066e-05\n",
      "time/sample_batch: 0.009068489074707031\n",
      "time/training: 1.0869841575622559\n",
      "evaluation/text/loss: 4.942826271057129\n",
      "evaluation/text/perplexity: 140.16583251953125\n",
      "time/total: 892.5845668315887\n",
      "time/evaluation: 0.8227536678314209\n",
      "training/train_loss_mean: 5.23897705078125\n",
      "training/train_loss_std: 0.4550092480363907\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 526\n",
      "training/learning_rate: 5.131988313233298e-05\n",
      "time/sample_batch: 0.008961915969848633\n",
      "time/training: 1.2302336692810059\n",
      "evaluation/text/loss: 4.812239170074463\n",
      "evaluation/text/perplexity: 123.00674438476562\n",
      "time/total: 894.5597884654999\n",
      "time/evaluation: 0.7433733940124512\n",
      "training/train_loss_mean: 5.053930401802063\n",
      "training/train_loss_std: 0.563337691551832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 527\n",
      "training/learning_rate: 5.117758055980088e-05\n",
      "time/sample_batch: 0.00865626335144043\n",
      "time/training: 1.0069489479064941\n",
      "evaluation/text/loss: 4.987316608428955\n",
      "evaluation/text/perplexity: 146.54266357421875\n",
      "time/total: 896.2914137840271\n",
      "time/evaluation: 0.7230651378631592\n",
      "training/train_loss_mean: 4.987306666374207\n",
      "training/train_loss_std: 1.0026720862758467\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 528\n",
      "training/learning_rate: 5.103531647899011e-05\n",
      "time/sample_batch: 0.00877833366394043\n",
      "time/training: 1.1514997482299805\n",
      "evaluation/text/loss: 4.944176197052002\n",
      "evaluation/text/perplexity: 140.3551788330078\n",
      "time/total: 898.0600454807281\n",
      "time/evaluation: 0.6155905723571777\n",
      "training/train_loss_mean: 5.2222858428955075\n",
      "training/train_loss_std: 0.36497031147672104\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 529\n",
      "training/learning_rate: 5.089309232249838e-05\n",
      "time/sample_batch: 0.008673429489135742\n",
      "time/training: 1.055083990097046\n",
      "evaluation/text/loss: 4.813258647918701\n",
      "evaluation/text/perplexity: 123.13220977783203\n",
      "time/total: 899.678416967392\n",
      "time/evaluation: 0.5617959499359131\n",
      "training/train_loss_mean: 5.034762954711914\n",
      "training/train_loss_std: 0.32304503125497297\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 530\n",
      "training/learning_rate: 5.0750909522521364e-05\n",
      "time/sample_batch: 0.008829116821289062\n",
      "time/training: 1.1120591163635254\n",
      "evaluation/text/loss: 4.8154120445251465\n",
      "evaluation/text/perplexity: 123.39764404296875\n",
      "time/total: 901.5571136474609\n",
      "time/evaluation: 0.7650434970855713\n",
      "training/train_loss_mean: 5.138033246994018\n",
      "training/train_loss_std: 0.4098976886852326\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 531\n",
      "training/learning_rate: 5.0608769510838284e-05\n",
      "time/sample_batch: 0.008262872695922852\n",
      "time/training: 1.0766324996948242\n",
      "evaluation/text/loss: 4.7459821701049805\n",
      "evaluation/text/perplexity: 115.12081909179688\n",
      "time/total: 903.0018286705017\n",
      "time/evaluation: 0.36636877059936523\n",
      "training/train_loss_mean: 5.215167760848999\n",
      "training/train_loss_std: 0.2962840772220034\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 532\n",
      "training/learning_rate: 5.046667371879749e-05\n",
      "time/sample_batch: 0.00734710693359375\n",
      "time/training: 1.0378105640411377\n",
      "evaluation/text/loss: 4.820068836212158\n",
      "evaluation/text/perplexity: 123.97362518310547\n",
      "time/total: 904.6370882987976\n",
      "time/evaluation: 0.5957810878753662\n",
      "training/train_loss_mean: 5.196121311187744\n",
      "training/train_loss_std: 0.2547889924181051\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 533\n",
      "training/learning_rate: 5.0324623577301985e-05\n",
      "time/sample_batch: 0.007051229476928711\n",
      "time/training: 0.9218044281005859\n",
      "evaluation/text/loss: 4.852170467376709\n",
      "evaluation/text/perplexity: 128.0179443359375\n",
      "time/total: 906.1729183197021\n",
      "time/evaluation: 0.612293004989624\n",
      "training/train_loss_mean: 5.219595718383789\n",
      "training/train_loss_std: 0.33618959499550327\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 534\n",
      "training/learning_rate: 5.0182620516795175e-05\n",
      "time/sample_batch: 0.00903463363647461\n",
      "time/training: 1.0576167106628418\n",
      "evaluation/text/loss: 4.874713897705078\n",
      "evaluation/text/perplexity: 130.9366912841797\n",
      "time/total: 907.7595903873444\n",
      "time/evaluation: 0.5273644924163818\n",
      "training/train_loss_mean: 5.235351800918579\n",
      "training/train_loss_std: 0.30567232542491596\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 535\n",
      "training/learning_rate: 5.004066596724628e-05\n",
      "time/sample_batch: 0.008707523345947266\n",
      "time/training: 0.9177939891815186\n",
      "evaluation/text/loss: 4.805346965789795\n",
      "evaluation/text/perplexity: 122.16187286376953\n",
      "time/total: 909.4283356666565\n",
      "time/evaluation: 0.7492778301239014\n",
      "training/train_loss_mean: 5.442409467697144\n",
      "training/train_loss_std: 0.3650407862705343\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 536\n",
      "training/learning_rate: 4.9898761358136024e-05\n",
      "time/sample_batch: 0.00929403305053711\n",
      "time/training: 0.9599974155426025\n",
      "evaluation/text/loss: 4.892280578613281\n",
      "evaluation/text/perplexity: 133.2571258544922\n",
      "time/total: 910.8810646533966\n",
      "time/evaluation: 0.4911305904388428\n",
      "training/train_loss_mean: 4.933838367462158\n",
      "training/train_loss_std: 0.9572003010599446\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 537\n",
      "training/learning_rate: 4.9756908118442263e-05\n",
      "time/sample_batch: 0.00877833366394043\n",
      "time/training: 0.936866283416748\n",
      "evaluation/text/loss: 4.761212348937988\n",
      "evaluation/text/perplexity: 116.8875503540039\n",
      "time/total: 912.563886642456\n",
      "time/evaluation: 0.7441563606262207\n",
      "training/train_loss_mean: 4.923856496810913\n",
      "training/train_loss_std: 0.6031032201873742\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 538\n",
      "training/learning_rate: 4.961510767662554e-05\n",
      "time/sample_batch: 0.009898662567138672\n",
      "time/training: 1.0119593143463135\n",
      "evaluation/text/loss: 5.060965538024902\n",
      "evaluation/text/perplexity: 157.7427520751953\n",
      "time/total: 914.074273109436\n",
      "time/evaluation: 0.4922211170196533\n",
      "training/train_loss_mean: 5.044737386703491\n",
      "training/train_loss_std: 0.23390736106245555\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 539\n",
      "training/learning_rate: 4.947336146061473e-05\n",
      "time/sample_batch: 0.009669303894042969\n",
      "time/training: 1.0659985542297363\n",
      "evaluation/text/loss: 4.721805095672607\n",
      "evaluation/text/perplexity: 112.37091064453125\n",
      "time/total: 915.8098080158234\n",
      "time/evaluation: 0.6676356792449951\n",
      "training/train_loss_mean: 5.3625764608383175\n",
      "training/train_loss_std: 0.5861577487204904\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 540\n",
      "training/learning_rate: 4.9331670897792656e-05\n",
      "time/sample_batch: 0.00952768325805664\n",
      "time/training: 1.141077995300293\n",
      "evaluation/text/loss: 4.93717098236084\n",
      "evaluation/text/perplexity: 139.37539672851562\n",
      "time/total: 917.6283240318298\n",
      "time/evaluation: 0.6757299900054932\n",
      "training/train_loss_mean: 5.112128114700317\n",
      "training/train_loss_std: 0.1977919333919994\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 541\n",
      "training/learning_rate: 4.9190037414981705e-05\n",
      "time/sample_batch: 0.008995532989501953\n",
      "time/training: 1.2757046222686768\n",
      "evaluation/text/loss: 4.880054950714111\n",
      "evaluation/text/perplexity: 131.6378936767578\n",
      "time/total: 919.670135974884\n",
      "time/evaluation: 0.7645034790039062\n",
      "training/train_loss_mean: 5.299565172195434\n",
      "training/train_loss_std: 0.20082859119720906\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 542\n",
      "training/learning_rate: 4.9048462438429484e-05\n",
      "time/sample_batch: 0.008230447769165039\n",
      "time/training: 1.0599875450134277\n",
      "evaluation/text/loss: 4.78951358795166\n",
      "evaluation/text/perplexity: 120.24286651611328\n",
      "time/total: 921.1808521747589\n",
      "time/evaluation: 0.449185848236084\n",
      "training/train_loss_mean: 5.099594163894653\n",
      "training/train_loss_std: 0.2383814971388147\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 543\n",
      "training/learning_rate: 4.890694739379445e-05\n",
      "time/sample_batch: 0.009368896484375\n",
      "time/training: 0.9284465312957764\n",
      "evaluation/text/loss: 4.8442277908325195\n",
      "evaluation/text/perplexity: 127.00517272949219\n",
      "time/total: 922.7697532176971\n",
      "time/evaluation: 0.6587247848510742\n",
      "training/train_loss_mean: 5.312058210372925\n",
      "training/train_loss_std: 0.28411848950540236\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 544\n",
      "training/learning_rate: 4.8765493706131503e-05\n",
      "time/sample_batch: 0.010038614273071289\n",
      "time/training: 1.319488525390625\n",
      "evaluation/text/loss: 5.037411212921143\n",
      "evaluation/text/perplexity: 154.07064819335938\n",
      "time/total: 924.6601209640503\n",
      "time/evaluation: 0.5690758228302002\n",
      "training/train_loss_mean: 5.238464260101319\n",
      "training/train_loss_std: 0.47655489395554934\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 545\n",
      "training/learning_rate: 4.862410279987772e-05\n",
      "time/sample_batch: 0.009697914123535156\n",
      "time/training: 1.29534912109375\n",
      "evaluation/text/loss: 4.910689830780029\n",
      "evaluation/text/perplexity: 135.73301696777344\n",
      "time/total: 926.7165515422821\n",
      "time/evaluation: 0.7593569755554199\n",
      "training/train_loss_mean: 5.373297119140625\n",
      "training/train_loss_std: 0.2748983893586437\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 546\n",
      "training/learning_rate: 4.8482776098837954e-05\n",
      "time/sample_batch: 0.010234594345092773\n",
      "time/training: 1.1113965511322021\n",
      "evaluation/text/loss: 4.997435569763184\n",
      "evaluation/text/perplexity: 148.03305053710938\n",
      "time/total: 928.4312362670898\n",
      "time/evaluation: 0.601656436920166\n",
      "training/train_loss_mean: 5.190313673019409\n",
      "training/train_loss_std: 0.20920385125164917\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 547\n",
      "training/learning_rate: 4.834151502617052e-05\n",
      "time/sample_batch: 0.008585691452026367\n",
      "time/training: 1.1103942394256592\n",
      "evaluation/text/loss: 4.969074249267578\n",
      "evaluation/text/perplexity: 143.89361572265625\n",
      "time/total: 930.2594895362854\n",
      "time/evaluation: 0.7161026000976562\n",
      "training/train_loss_mean: 5.158171796798706\n",
      "training/train_loss_std: 0.25256855139644013\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 548\n",
      "training/learning_rate: 4.820032100437285e-05\n",
      "time/sample_batch: 0.01040506362915039\n",
      "time/training: 1.1314730644226074\n",
      "evaluation/text/loss: 4.628697872161865\n",
      "evaluation/text/perplexity: 102.38066101074219\n",
      "time/total: 932.0910489559174\n",
      "time/evaluation: 0.6983754634857178\n",
      "training/train_loss_mean: 5.192381191253662\n",
      "training/train_loss_std: 0.27237866217411316\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 549\n",
      "training/learning_rate: 4.805919545526717e-05\n",
      "time/sample_batch: 0.008432626724243164\n",
      "time/training: 0.9714465141296387\n",
      "evaluation/text/loss: 4.879196643829346\n",
      "evaluation/text/perplexity: 131.52496337890625\n",
      "time/total: 933.8391184806824\n",
      "time/evaluation: 0.7750105857849121\n",
      "training/train_loss_mean: 5.192679858207702\n",
      "training/train_loss_std: 0.5249434499076918\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 550\n",
      "training/learning_rate: 4.7918139799986194e-05\n",
      "time/sample_batch: 0.008702993392944336\n",
      "time/training: 1.0103263854980469\n",
      "evaluation/text/loss: 4.834299087524414\n",
      "evaluation/text/perplexity: 125.75041198730469\n",
      "time/total: 935.3809235095978\n",
      "time/evaluation: 0.529796838760376\n",
      "training/train_loss_mean: 5.298739671707153\n",
      "training/train_loss_std: 0.33154837808889936\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 551\n",
      "training/learning_rate: 4.777715545895882e-05\n",
      "time/sample_batch: 0.009136199951171875\n",
      "time/training: 0.9693779945373535\n",
      "evaluation/text/loss: 4.620261192321777\n",
      "evaluation/text/perplexity: 101.52054595947266\n",
      "time/total: 936.8340418338776\n",
      "time/evaluation: 0.48185181617736816\n",
      "training/train_loss_mean: 4.97571337223053\n",
      "training/train_loss_std: 0.6738711303825542\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 552\n",
      "training/learning_rate: 4.763624385189578e-05\n",
      "time/sample_batch: 0.009325027465820312\n",
      "time/training: 1.016413688659668\n",
      "evaluation/text/loss: 4.970741271972656\n",
      "evaluation/text/perplexity: 144.13369750976562\n",
      "time/total: 938.5525393486023\n",
      "time/evaluation: 0.7003052234649658\n",
      "training/train_loss_mean: 5.242552089691162\n",
      "training/train_loss_std: 0.2908106035928464\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 553\n",
      "training/learning_rate: 4.74954063977754e-05\n",
      "time/sample_batch: 0.009607076644897461\n",
      "time/training: 1.0701885223388672\n",
      "evaluation/text/loss: 4.885348796844482\n",
      "evaluation/text/perplexity: 132.33660888671875\n",
      "time/total: 940.1111481189728\n",
      "time/evaluation: 0.4868662357330322\n",
      "training/train_loss_mean: 5.2656646251678465\n",
      "training/train_loss_std: 0.3066159351914601\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 554\n",
      "training/learning_rate: 4.7354644514829284e-05\n",
      "time/sample_batch: 0.008347749710083008\n",
      "time/training: 1.0969557762145996\n",
      "evaluation/text/loss: 4.738681793212891\n",
      "evaluation/text/perplexity: 114.28345489501953\n",
      "time/total: 941.8031988143921\n",
      "time/evaluation: 0.593468427658081\n",
      "training/train_loss_mean: 5.373033809661865\n",
      "training/train_loss_std: 0.21114358737787547\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 555\n",
      "training/learning_rate: 4.721395962052801e-05\n",
      "time/sample_batch: 0.007039070129394531\n",
      "time/training: 1.0903964042663574\n",
      "evaluation/text/loss: 4.863316059112549\n",
      "evaluation/text/perplexity: 129.4527587890625\n",
      "time/total: 943.6392202377319\n",
      "time/evaluation: 0.7440090179443359\n",
      "training/train_loss_mean: 5.345254898071289\n",
      "training/train_loss_std: 0.22597536253907244\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 556\n",
      "training/learning_rate: 4.7073353131566924e-05\n",
      "time/sample_batch: 0.008336067199707031\n",
      "time/training: 0.9276831150054932\n",
      "evaluation/text/loss: 4.862585544586182\n",
      "evaluation/text/perplexity: 129.3582305908203\n",
      "time/total: 945.1511976718903\n",
      "time/evaluation: 0.5826044082641602\n",
      "training/train_loss_mean: 5.355102109909057\n",
      "training/train_loss_std: 0.256358966310776\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 557\n",
      "training/learning_rate: 4.6932826463851807e-05\n",
      "time/sample_batch: 0.008601903915405273\n",
      "time/training: 0.9608757495880127\n",
      "evaluation/text/loss: 4.8293962478637695\n",
      "evaluation/text/perplexity: 125.13538360595703\n",
      "time/total: 946.6109030246735\n",
      "time/evaluation: 0.4971461296081543\n",
      "training/train_loss_mean: 4.923630976676941\n",
      "training/train_loss_std: 0.5487314628394184\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 558\n",
      "training/learning_rate: 4.679238103248462e-05\n",
      "time/sample_batch: 0.007964134216308594\n",
      "time/training: 1.1023809909820557\n",
      "evaluation/text/loss: 4.783815383911133\n",
      "evaluation/text/perplexity: 119.55964660644531\n",
      "time/total: 948.2717440128326\n",
      "time/evaluation: 0.5567419528961182\n",
      "training/train_loss_mean: 5.315342235565185\n",
      "training/train_loss_std: 0.23680821102633867\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 559\n",
      "training/learning_rate: 4.665201825174933e-05\n",
      "time/sample_batch: 0.007900238037109375\n",
      "time/training: 1.0488052368164062\n",
      "evaluation/text/loss: 4.953197479248047\n",
      "evaluation/text/perplexity: 141.62709045410156\n",
      "time/total: 950.1301901340485\n",
      "time/evaluation: 0.8079285621643066\n",
      "training/train_loss_mean: 5.233940982818604\n",
      "training/train_loss_std: 0.3579726653929754\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 560\n",
      "training/learning_rate: 4.651173953509757e-05\n",
      "time/sample_batch: 0.008030891418457031\n",
      "time/training: 0.8713526725769043\n",
      "evaluation/text/loss: 4.956089019775391\n",
      "evaluation/text/perplexity: 142.03720092773438\n",
      "time/total: 951.7006421089172\n",
      "time/evaluation: 0.6974058151245117\n",
      "training/train_loss_mean: 4.927584028244018\n",
      "training/train_loss_std: 0.46099091467607944\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 561\n",
      "training/learning_rate: 4.637154629513445e-05\n",
      "time/sample_batch: 0.008298873901367188\n",
      "time/training: 1.0899569988250732\n",
      "evaluation/text/loss: 4.851067543029785\n",
      "evaluation/text/perplexity: 127.8768310546875\n",
      "time/total: 953.372636795044\n",
      "time/evaluation: 0.5803301334381104\n",
      "training/train_loss_mean: 5.343298864364624\n",
      "training/train_loss_std: 0.25042986989906196\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 562\n",
      "training/learning_rate: 4.623143994360433e-05\n",
      "time/sample_batch: 0.00805211067199707\n",
      "time/training: 1.0008645057678223\n",
      "evaluation/text/loss: 4.809725761413574\n",
      "evaluation/text/perplexity: 122.69796752929688\n",
      "time/total: 954.9639189243317\n",
      "time/evaluation: 0.5889015197753906\n",
      "training/train_loss_mean: 5.012847280502319\n",
      "training/train_loss_std: 0.2008097213837389\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 563\n",
      "training/learning_rate: 4.609142189137664e-05\n",
      "time/sample_batch: 0.008891820907592773\n",
      "time/training: 1.0809946060180664\n",
      "evaluation/text/loss: 4.818512439727783\n",
      "evaluation/text/perplexity: 123.78082275390625\n",
      "time/total: 956.7916915416718\n",
      "time/evaluation: 0.7451860904693604\n",
      "training/train_loss_mean: 5.262342834472657\n",
      "training/train_loss_std: 0.322068038904905\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 564\n",
      "training/learning_rate: 4.5951493548431603e-05\n",
      "time/sample_batch: 0.0088043212890625\n",
      "time/training: 1.0558979511260986\n",
      "evaluation/text/loss: 4.931634426116943\n",
      "evaluation/text/perplexity: 138.60586547851562\n",
      "time/total: 958.5326311588287\n",
      "time/evaluation: 0.6833205223083496\n",
      "training/train_loss_mean: 5.162042760848999\n",
      "training/train_loss_std: 0.17591877066791844\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 565\n",
      "training/learning_rate: 4.581165632384606e-05\n",
      "time/sample_batch: 0.007531404495239258\n",
      "time/training: 0.8677582740783691\n",
      "evaluation/text/loss: 4.788388252258301\n",
      "evaluation/text/perplexity: 120.10762786865234\n",
      "time/total: 959.9792008399963\n",
      "time/evaluation: 0.5772092342376709\n",
      "training/train_loss_mean: 5.152713346481323\n",
      "training/train_loss_std: 0.31805794244822844\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 566\n",
      "training/learning_rate: 4.5671911625779345e-05\n",
      "time/sample_batch: 0.008410930633544922\n",
      "time/training: 0.9951417446136475\n",
      "evaluation/text/loss: 4.746241092681885\n",
      "evaluation/text/perplexity: 115.15062713623047\n",
      "time/total: 961.9720942974091\n",
      "time/evaluation: 0.9961814880371094\n",
      "training/train_loss_mean: 5.171691846847534\n",
      "training/train_loss_std: 0.224302193262541\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 567\n",
      "training/learning_rate: 4.5532260861458984e-05\n",
      "time/sample_batch: 0.009104490280151367\n",
      "time/training: 1.0527293682098389\n",
      "evaluation/text/loss: 4.985028266906738\n",
      "evaluation/text/perplexity: 146.20770263671875\n",
      "time/total: 963.7941920757294\n",
      "time/evaluation: 0.7677614688873291\n",
      "training/train_loss_mean: 5.384557390213013\n",
      "training/train_loss_std: 0.3666345227307155\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 568\n",
      "training/learning_rate: 4.539270543716664e-05\n",
      "time/sample_batch: 0.007414102554321289\n",
      "time/training: 0.8980937004089355\n",
      "evaluation/text/loss: 4.847819805145264\n",
      "evaluation/text/perplexity: 127.46219635009766\n",
      "time/total: 965.4352424144745\n",
      "time/evaluation: 0.7411878108978271\n",
      "training/train_loss_mean: 5.180338191986084\n",
      "training/train_loss_std: 0.390747099707332\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 569\n",
      "training/learning_rate: 4.525324675822388e-05\n",
      "time/sample_batch: 0.007918119430541992\n",
      "time/training: 0.9066860675811768\n",
      "evaluation/text/loss: 5.0299224853515625\n",
      "evaluation/text/perplexity: 152.92115783691406\n",
      "time/total: 967.0855515003204\n",
      "time/evaluation: 0.7418348789215088\n",
      "training/train_loss_mean: 4.9764539957046505\n",
      "training/train_loss_std: 0.6843064008748554\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 570\n",
      "training/learning_rate: 4.511388622897806e-05\n",
      "time/sample_batch: 0.00902700424194336\n",
      "time/training: 1.0453975200653076\n",
      "evaluation/text/loss: 4.927770137786865\n",
      "evaluation/text/perplexity: 138.0712890625\n",
      "time/total: 968.8815813064575\n",
      "time/evaluation: 0.7490057945251465\n",
      "training/train_loss_mean: 4.762887537479401\n",
      "training/train_loss_std: 1.1359590823192598\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 571\n",
      "training/learning_rate: 4.497462525278815e-05\n",
      "time/sample_batch: 0.010004758834838867\n",
      "time/training: 1.2356140613555908\n",
      "evaluation/text/loss: 4.928981304168701\n",
      "evaluation/text/perplexity: 138.23861694335938\n",
      "time/total: 970.774087190628\n",
      "time/evaluation: 0.6552681922912598\n",
      "training/train_loss_mean: 5.265965557098388\n",
      "training/train_loss_std: 0.242648144210298\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 572\n",
      "training/learning_rate: 4.483546523201066e-05\n",
      "time/sample_batch: 0.008149147033691406\n",
      "time/training: 0.8995239734649658\n",
      "evaluation/text/loss: 4.838458061218262\n",
      "evaluation/text/perplexity: 126.27449035644531\n",
      "time/total: 972.4205198287964\n",
      "time/evaluation: 0.7451622486114502\n",
      "training/train_loss_mean: 5.185940885543824\n",
      "training/train_loss_std: 0.3778919604143447\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 573\n",
      "training/learning_rate: 4.4696407567985424e-05\n",
      "time/sample_batch: 0.010679483413696289\n",
      "time/training: 1.2153258323669434\n",
      "evaluation/text/loss: 4.641626834869385\n",
      "evaluation/text/perplexity: 103.71293640136719\n",
      "time/total: 974.2781853675842\n",
      "time/evaluation: 0.6405723094940186\n",
      "training/train_loss_mean: 5.11708631515503\n",
      "training/train_loss_std: 0.21708674027014294\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 574\n",
      "training/learning_rate: 4.4557453661021584e-05\n",
      "time/sample_batch: 0.00764155387878418\n",
      "time/training: 1.0193099975585938\n",
      "evaluation/text/loss: 5.032812118530273\n",
      "evaluation/text/perplexity: 153.36367797851562\n",
      "time/total: 975.8042316436768\n",
      "time/evaluation: 0.5051071643829346\n",
      "training/train_loss_mean: 5.055097007751465\n",
      "training/train_loss_std: 0.31404825880636783\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 575\n",
      "training/learning_rate: 4.441860491038344e-05\n",
      "time/sample_batch: 0.008617401123046875\n",
      "time/training: 0.9517562389373779\n",
      "evaluation/text/loss: 4.920901298522949\n",
      "evaluation/text/perplexity: 137.1261444091797\n",
      "time/total: 977.2910540103912\n",
      "time/evaluation: 0.5333855152130127\n",
      "training/train_loss_mean: 5.199063301086426\n",
      "training/train_loss_std: 0.2830055210718113\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 576\n",
      "training/learning_rate: 4.427986271427639e-05\n",
      "time/sample_batch: 0.007874727249145508\n",
      "time/training: 0.762749433517456\n",
      "evaluation/text/loss: 4.739046096801758\n",
      "evaluation/text/perplexity: 114.3250961303711\n",
      "time/total: 978.5668559074402\n",
      "time/evaluation: 0.5114200115203857\n",
      "training/train_loss_mean: 4.930836963653564\n",
      "training/train_loss_std: 0.3859822293092424\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 577\n",
      "training/learning_rate: 4.414122846983278e-05\n",
      "time/sample_batch: 0.006157398223876953\n",
      "time/training: 1.0102922916412354\n",
      "evaluation/text/loss: 4.895746231079102\n",
      "evaluation/text/perplexity: 133.71975708007812\n",
      "time/total: 980.1464881896973\n",
      "time/evaluation: 0.5633807182312012\n",
      "training/train_loss_mean: 5.11315267086029\n",
      "training/train_loss_std: 0.6801739332705377\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 578\n",
      "training/learning_rate: 4.400270357309794e-05\n",
      "time/sample_batch: 0.008202075958251953\n",
      "time/training: 1.1991157531738281\n",
      "evaluation/text/loss: 4.994678974151611\n",
      "evaluation/text/perplexity: 147.62554931640625\n",
      "time/total: 981.7665956020355\n",
      "time/evaluation: 0.4192178249359131\n",
      "training/train_loss_mean: 4.9177406311035154\n",
      "training/train_loss_std: 0.8944832817168802\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 579\n",
      "training/learning_rate: 4.386428941901601e-05\n",
      "time/sample_batch: 0.008205652236938477\n",
      "time/training: 1.1117300987243652\n",
      "evaluation/text/loss: 4.961263179779053\n",
      "evaluation/text/perplexity: 142.77403259277344\n",
      "time/total: 983.6391401290894\n",
      "time/evaluation: 0.75905442237854\n",
      "training/train_loss_mean: 5.171190786361694\n",
      "training/train_loss_std: 0.20773308286109107\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 580\n",
      "training/learning_rate: 4.3725987401416e-05\n",
      "time/sample_batch: 0.008836030960083008\n",
      "time/training: 1.0915641784667969\n",
      "evaluation/text/loss: 4.8488969802856445\n",
      "evaluation/text/perplexity: 127.59956359863281\n",
      "time/total: 985.4766621589661\n",
      "time/evaluation: 0.7443394660949707\n",
      "training/train_loss_mean: 5.11545889377594\n",
      "training/train_loss_std: 0.45498716378579473\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 581\n",
      "training/learning_rate: 4.358779891299769e-05\n",
      "time/sample_batch: 0.009877681732177734\n",
      "time/training: 1.0600929260253906\n",
      "evaluation/text/loss: 4.643763065338135\n",
      "evaluation/text/perplexity: 103.93472290039062\n",
      "time/total: 987.1385428905487\n",
      "time/evaluation: 0.600055456161499\n",
      "training/train_loss_mean: 5.328589057922363\n",
      "training/train_loss_std: 0.3690394620915694\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 582\n",
      "training/learning_rate: 4.344972534531764e-05\n",
      "time/sample_batch: 0.006659507751464844\n",
      "time/training: 0.8545839786529541\n",
      "evaluation/text/loss: 4.743278980255127\n",
      "evaluation/text/perplexity: 114.81004333496094\n",
      "time/total: 988.4597957134247\n",
      "time/evaluation: 0.46489453315734863\n",
      "training/train_loss_mean: 5.009010171890258\n",
      "training/train_loss_std: 0.4033178853225381\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 583\n",
      "training/learning_rate: 4.3311768088775124e-05\n",
      "time/sample_batch: 0.009695291519165039\n",
      "time/training: 0.8757226467132568\n",
      "evaluation/text/loss: 4.926591873168945\n",
      "evaluation/text/perplexity: 137.90870666503906\n",
      "time/total: 990.0761349201202\n",
      "time/evaluation: 0.739008903503418\n",
      "training/train_loss_mean: 4.821948719024658\n",
      "training/train_loss_std: 0.7127735498480279\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 584\n",
      "training/learning_rate: 4.3173928532598176e-05\n",
      "time/sample_batch: 0.010386466979980469\n",
      "time/training: 1.2302947044372559\n",
      "evaluation/text/loss: 4.886857032775879\n",
      "evaluation/text/perplexity: 132.53636169433594\n",
      "time/total: 992.0515358448029\n",
      "time/evaluation: 0.7434580326080322\n",
      "training/train_loss_mean: 4.976133203506469\n",
      "training/train_loss_std: 0.30827685897353474\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 585\n",
      "training/learning_rate: 4.303620806482963e-05\n",
      "time/sample_batch: 0.00936436653137207\n",
      "time/training: 1.0471100807189941\n",
      "evaluation/text/loss: 5.022287845611572\n",
      "evaluation/text/perplexity: 151.7581024169922\n",
      "time/total: 993.6662766933441\n",
      "time/evaluation: 0.5657992362976074\n",
      "training/train_loss_mean: 5.234364318847656\n",
      "training/train_loss_std: 0.2627640714170435\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 586\n",
      "training/learning_rate: 4.2898608072313044e-05\n",
      "time/sample_batch: 0.009483814239501953\n",
      "time/training: 0.9452254772186279\n",
      "evaluation/text/loss: 4.835379600524902\n",
      "evaluation/text/perplexity: 125.88636016845703\n",
      "time/total: 995.3667724132538\n",
      "time/evaluation: 0.7534513473510742\n",
      "training/train_loss_mean: 5.2035705089569095\n",
      "training/train_loss_std: 0.3228142299193963\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 587\n",
      "training/learning_rate: 4.2761129940678815e-05\n",
      "time/sample_batch: 0.009573221206665039\n",
      "time/training: 1.1095271110534668\n",
      "evaluation/text/loss: 4.750524520874023\n",
      "evaluation/text/perplexity: 115.64492797851562\n",
      "time/total: 997.213262796402\n",
      "time/evaluation: 0.7353756427764893\n",
      "training/train_loss_mean: 5.186548328399658\n",
      "training/train_loss_std: 0.4791053115627304\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 588\n",
      "training/learning_rate: 4.262377505433021e-05\n",
      "time/sample_batch: 0.00811624526977539\n",
      "time/training: 1.018524169921875\n",
      "evaluation/text/loss: 4.8309760093688965\n",
      "evaluation/text/perplexity: 125.3332290649414\n",
      "time/total: 999.0374834537506\n",
      "time/evaluation: 0.8040299415588379\n",
      "training/train_loss_mean: 5.08934519290924\n",
      "training/train_loss_std: 0.6066569090770602\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 589\n",
      "training/learning_rate: 4.248654479642941e-05\n",
      "time/sample_batch: 0.01002359390258789\n",
      "time/training: 1.016688585281372\n",
      "evaluation/text/loss: 4.771082401275635\n",
      "evaluation/text/perplexity: 118.04694366455078\n",
      "time/total: 1000.8029255867004\n",
      "time/evaluation: 0.7473175525665283\n",
      "training/train_loss_mean: 5.155894637107849\n",
      "training/train_loss_std: 0.7179636812910134\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 590\n",
      "training/learning_rate: 4.234944054888357e-05\n",
      "time/sample_batch: 0.008974552154541016\n",
      "time/training: 1.149914264678955\n",
      "evaluation/text/loss: 5.035921573638916\n",
      "evaluation/text/perplexity: 153.84130859375\n",
      "time/total: 1002.5398817062378\n",
      "time/evaluation: 0.5852184295654297\n",
      "training/train_loss_mean: 5.1666065692901615\n",
      "training/train_loss_std: 0.2660418710564316\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 591\n",
      "training/learning_rate: 4.2212463692330967e-05\n",
      "time/sample_batch: 0.007102251052856445\n",
      "time/training: 1.1053705215454102\n",
      "evaluation/text/loss: 4.778271675109863\n",
      "evaluation/text/perplexity: 118.89867401123047\n",
      "time/total: 1004.1584012508392\n",
      "time/evaluation: 0.5115039348602295\n",
      "training/train_loss_mean: 5.141079425811768\n",
      "training/train_loss_std: 0.24086647213314238\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 592\n",
      "training/learning_rate: 4.2075615606127014e-05\n",
      "time/sample_batch: 0.006103515625\n",
      "time/training: 0.964709997177124\n",
      "evaluation/text/loss: 4.863559722900391\n",
      "evaluation/text/perplexity: 129.48431396484375\n",
      "time/total: 1005.7310879230499\n",
      "time/evaluation: 0.6063635349273682\n",
      "training/train_loss_mean: 5.146589756011963\n",
      "training/train_loss_std: 0.2303794469128909\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 593\n",
      "training/learning_rate: 4.193889766833041e-05\n",
      "time/sample_batch: 0.008814334869384766\n",
      "time/training: 0.9831302165985107\n",
      "evaluation/text/loss: 4.919567584991455\n",
      "evaluation/text/perplexity: 136.94338989257812\n",
      "time/total: 1007.2448477745056\n",
      "time/evaluation: 0.5290260314941406\n",
      "training/train_loss_mean: 5.0806478500366214\n",
      "training/train_loss_std: 0.3805957163575678\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 594\n",
      "training/learning_rate: 4.180231125568929e-05\n",
      "time/sample_batch: 0.007941007614135742\n",
      "time/training: 1.1188440322875977\n",
      "evaluation/text/loss: 4.718558311462402\n",
      "evaluation/text/perplexity: 112.00666046142578\n",
      "time/total: 1008.8253965377808\n",
      "time/evaluation: 0.4599604606628418\n",
      "training/train_loss_mean: 4.976639223098755\n",
      "training/train_loss_std: 0.22378926019909226\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 595\n",
      "training/learning_rate: 4.1665857743627313e-05\n",
      "time/sample_batch: 0.006601572036743164\n",
      "time/training: 0.8418810367584229\n",
      "evaluation/text/loss: 4.880103588104248\n",
      "evaluation/text/perplexity: 131.64430236816406\n",
      "time/total: 1010.2227334976196\n",
      "time/evaluation: 0.5536911487579346\n",
      "training/train_loss_mean: 5.006083917617798\n",
      "training/train_loss_std: 0.5092705088659607\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 596\n",
      "training/learning_rate: 4.152953850622982e-05\n",
      "time/sample_batch: 0.008333206176757812\n",
      "time/training: 1.2698073387145996\n",
      "evaluation/text/loss: 4.9084248542785645\n",
      "evaluation/text/perplexity: 135.42593383789062\n",
      "time/total: 1012.254177570343\n",
      "time/evaluation: 0.7599458694458008\n",
      "training/train_loss_mean: 5.3327414989471436\n",
      "training/train_loss_std: 0.25265486003259074\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 597\n",
      "training/learning_rate: 4.1393354916230006e-05\n",
      "time/sample_batch: 0.008761882781982422\n",
      "time/training: 1.0265450477600098\n",
      "evaluation/text/loss: 4.964582443237305\n",
      "evaluation/text/perplexity: 143.24871826171875\n",
      "time/total: 1013.7524292469025\n",
      "time/evaluation: 0.4700126647949219\n",
      "training/train_loss_mean: 5.159137535095215\n",
      "training/train_loss_std: 0.25331912634061127\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 598\n",
      "training/learning_rate: 4.125730834499512e-05\n",
      "time/sample_batch: 0.01043558120727539\n",
      "time/training: 1.012125015258789\n",
      "evaluation/text/loss: 4.985763072967529\n",
      "evaluation/text/perplexity: 146.315185546875\n",
      "time/total: 1015.325864315033\n",
      "time/evaluation: 0.5596933364868164\n",
      "training/train_loss_mean: 5.140663146972656\n",
      "training/train_loss_std: 0.16635770556468973\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 599\n",
      "training/learning_rate: 4.112140016251264e-05\n",
      "time/sample_batch: 0.009633541107177734\n",
      "time/training: 1.094982385635376\n",
      "evaluation/text/loss: 4.9631266593933105\n",
      "evaluation/text/perplexity: 143.0403289794922\n",
      "time/total: 1016.993373632431\n",
      "time/evaluation: 0.5708279609680176\n",
      "training/train_loss_mean: 4.936021375656128\n",
      "training/train_loss_std: 0.9233322976635843\n",
      "================================================================================\n",
      "Input: EventhoughBritainledtheworldinsteelproduction,theRoyalNavywasslowtoadoptsteelwarships.The<unk>processforsteelmanufacture | Output : producedtoomanyimperfectionsforlarge@-@scaleuseonships.Frenchmanufacturersusedthe<unk>@-@Martinprocesstoproduceadequatesteel,butBritishtechnology<unk>behind.Thefirstall@-@steelwarshipsbuiltbytheRoyalNavywerethedispatchvessels<unk>andMercury,laiddownin1875and1876. | Prediction:  ways. mid @-@ at <unk> @-@ <unk> party. \n",
      "\n",
      "Input: <unk>openswith\"LoQue<unk><unk><unk>Mal\",aLatinpopsongandtheonlytrackcomposedbyDanWarnerinsteadof<unk>(whowrotethelyrics).\"ElAmor\"wasmotivatedby<unk>'sdesiretoexamine\"thosebig,darkeventswithinlovethatnobodytalksabout\";hecontinued,\"[the]darksidesofloveareextremelyfundamentaltounderstanditsgreatvalue.\"<unk>added,\"Somanygoodthings<unk>lovehasbeenshownthatsomebodyhadtoturnitaroundandtellthebadones\".InaFebruary2012interview,<unk>statedthat\"ElAmor\"wasthe\"most<unk>\"songhehadreleasedtodate,explainingthattheirchoiceofthesongwasa\"contradiction\"becauseitwasnot\"thesongwhichcouldbetterrepresenttheentirealbum\". | Output : Hedescribeditas\"verystrong\"and\"abitdark\".Thesinglemarked<unk>'sreturntohissignature,mainstreamsoundaftertheCubanmusicinfluenced<unk><unk>'sleadsingle\"<unk>\",amixtureofsalsaandmerenguewhichfailedtomakeanimpactintheUnitedStates. | Prediction:  <unk> — and ran for the ) not been victory for protest. He was western deaths and\n",
      "Input: TheItalianfleetconsistedof12ironcladsandasimilarnumberofwoodenwarships,escortingtransportswhichcarriedtroopsintendingtolandontheAdriaticisland | Output : of<unk>.AmongtheItalianironcladsweresevenbroadsideironcladfrigates,foursmallerironclads,andthenewlybuilt<unk>—adouble@-@<unk>ram.<unk>them,theAustriannavyhadsevenironcladfrigates. | Prediction:  avoided,loo of water family food — large Journal, like part in theese published in the blocks\n",
      "Input: Rebecca<unk>usedtheirexposuretoEuropeancountryestatestodesignalavishnewbuildingfortheir<unk>estateduringtheirEnglish<unk>.DesignedwiththeassistanceofFrencharchitectJoseph@-@Guillaume<unk>andprobablyalsoinfluencedbytheworksofEnglisharchitectSirJohn<unk>,the | Output : housethatwasbuiltupontheirreturntotheUnitedStatesin1804(nowknownas<unk>Place)isoneofthefinestextantexamplesof<unk>architecture. | Prediction:  so much of use about their Turks. The History, which are social Douglas – Thekelstein <\n",
      "Input: Theproliferationofironcladbattleshipdesignscametoanendinthe1890sasnaviesreachedaconsensusonthedesignofbattleships,producing | Output : thetypeknownasthepre@-@<unk>.Theseshipsaresometimescoveredintreatmentsoftheironcladwarship.Thenextevolutionofbattleshipdesign,thedreadnought,isneverreferredtoasan'ironclad'. | Prediction:  Victoria� data on both Egypt rapid threatened for his wife and establish a Telecom. /// is limited number\n",
      "================================================================================\n",
      "Iteration 600\n",
      "training/learning_rate: 4.098563173737643e-05\n",
      "time/sample_batch: 0.00935220718383789\n",
      "time/training: 1.2525262832641602\n",
      "evaluation/text/loss: 4.7400712966918945\n",
      "evaluation/text/perplexity: 114.4423599243164\n",
      "time/total: 1019.6292684078217\n",
      "time/evaluation: 1.3817155361175537\n",
      "training/train_loss_mean: 5.32841420173645\n",
      "training/train_loss_std: 0.2517043899445807\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 601\n",
      "training/learning_rate: 4.085000443677304e-05\n",
      "time/sample_batch: 0.007617473602294922\n",
      "time/training: 0.9702174663543701\n",
      "evaluation/text/loss: 4.858395576477051\n",
      "evaluation/text/perplexity: 128.81735229492188\n",
      "time/total: 1021.1935861110687\n",
      "time/evaluation: 0.5928254127502441\n",
      "training/train_loss_mean: 4.734074282646179\n",
      "training/train_loss_std: 0.8210617217058617\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 602\n",
      "training/learning_rate: 4.071451962646788e-05\n",
      "time/sample_batch: 0.00913238525390625\n",
      "time/training: 1.3573393821716309\n",
      "evaluation/text/loss: 4.747583389282227\n",
      "evaluation/text/perplexity: 115.3052978515625\n",
      "time/total: 1023.1625933647156\n",
      "time/evaluation: 0.6099116802215576\n",
      "training/train_loss_mean: 5.25957088470459\n",
      "training/train_loss_std: 0.3339076664013571\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 603\n",
      "training/learning_rate: 4.0579178670791485e-05\n",
      "time/sample_batch: 0.006613969802856445\n",
      "time/training: 1.0000381469726562\n",
      "evaluation/text/loss: 4.863597869873047\n",
      "evaluation/text/perplexity: 129.4892578125\n",
      "time/total: 1024.7665309906006\n",
      "time/evaluation: 0.6022617816925049\n",
      "training/train_loss_mean: 4.93230721950531\n",
      "training/train_loss_std: 0.5464507733563596\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 604\n",
      "training/learning_rate: 4.044398293262577e-05\n",
      "time/sample_batch: 0.0073473453521728516\n",
      "time/training: 0.968907356262207\n",
      "evaluation/text/loss: 4.919133186340332\n",
      "evaluation/text/perplexity: 136.8839111328125\n",
      "time/total: 1026.4800145626068\n",
      "time/evaluation: 0.7429478168487549\n",
      "training/train_loss_mean: 4.918384599685669\n",
      "training/train_loss_std: 0.5350782418484594\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 605\n",
      "training/learning_rate: 4.0308933773390324e-05\n",
      "time/sample_batch: 0.011099815368652344\n",
      "time/training: 1.0576276779174805\n",
      "evaluation/text/loss: 4.918142795562744\n",
      "evaluation/text/perplexity: 136.7484130859375\n",
      "time/total: 1028.291616678238\n",
      "time/evaluation: 0.7523479461669922\n",
      "training/train_loss_mean: 5.046823787689209\n",
      "training/train_loss_std: 0.25272892913732764\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 606\n",
      "training/learning_rate: 4.017403255302867e-05\n",
      "time/sample_batch: 0.007868528366088867\n",
      "time/training: 1.0074520111083984\n",
      "evaluation/text/loss: 4.840837478637695\n",
      "evaluation/text/perplexity: 126.57530975341797\n",
      "time/total: 1030.001459121704\n",
      "time/evaluation: 0.7005908489227295\n",
      "training/train_loss_mean: 5.115095138549805\n",
      "training/train_loss_std: 0.2891917744999513\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 607\n",
      "training/learning_rate: 4.003928062999462e-05\n",
      "time/sample_batch: 0.008560657501220703\n",
      "time/training: 0.7463715076446533\n",
      "evaluation/text/loss: 4.822632312774658\n",
      "evaluation/text/perplexity: 124.29183197021484\n",
      "time/total: 1031.4504659175873\n",
      "time/evaluation: 0.7010154724121094\n",
      "training/train_loss_mean: 5.243613004684448\n",
      "training/train_loss_std: 0.321827557290824\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 608\n",
      "training/learning_rate: 3.9904679361238525e-05\n",
      "time/sample_batch: 0.008009910583496094\n",
      "time/training: 1.0108935832977295\n",
      "evaluation/text/loss: 4.837175369262695\n",
      "evaluation/text/perplexity: 126.11262512207031\n",
      "time/total: 1032.999172449112\n",
      "time/evaluation: 0.5362536907196045\n",
      "training/train_loss_mean: 5.3941888332366945\n",
      "training/train_loss_std: 0.21375093144848734\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 609\n",
      "training/learning_rate: 3.977023010219367e-05\n",
      "time/sample_batch: 0.008352279663085938\n",
      "time/training: 1.1600182056427002\n",
      "evaluation/text/loss: 4.757094383239746\n",
      "evaluation/text/perplexity: 116.40719604492188\n",
      "time/total: 1034.9055497646332\n",
      "time/evaluation: 0.744704008102417\n",
      "training/train_loss_mean: 5.315808582305908\n",
      "training/train_loss_std: 0.3071719486517367\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 610\n",
      "training/learning_rate: 3.963593420676261e-05\n",
      "time/sample_batch: 0.008929967880249023\n",
      "time/training: 1.178485631942749\n",
      "evaluation/text/loss: 4.726146697998047\n",
      "evaluation/text/perplexity: 112.8598403930664\n",
      "time/total: 1036.8290164470673\n",
      "time/evaluation: 0.7431883811950684\n",
      "training/train_loss_mean: 5.307499980926513\n",
      "training/train_loss_std: 0.1739849495419488\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 611\n",
      "training/learning_rate: 3.9501793027303513e-05\n",
      "time/sample_batch: 0.007884502410888672\n",
      "time/training: 1.135061264038086\n",
      "evaluation/text/loss: 4.847387790679932\n",
      "evaluation/text/perplexity: 127.40714263916016\n",
      "time/total: 1038.4544179439545\n",
      "time/evaluation: 0.4886043071746826\n",
      "training/train_loss_mean: 5.189747095108032\n",
      "training/train_loss_std: 0.278036897472408\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 612\n",
      "training/learning_rate: 3.936780791461658e-05\n",
      "time/sample_batch: 0.009151697158813477\n",
      "time/training: 0.9623219966888428\n",
      "evaluation/text/loss: 4.869168281555176\n",
      "evaluation/text/perplexity: 130.2125701904297\n",
      "time/total: 1039.9313402175903\n",
      "time/evaluation: 0.5126791000366211\n",
      "training/train_loss_mean: 5.035946321487427\n",
      "training/train_loss_std: 0.20173740654592498\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 613\n",
      "training/learning_rate: 3.923398021793039e-05\n",
      "time/sample_batch: 0.007873773574829102\n",
      "time/training: 0.8306951522827148\n",
      "evaluation/text/loss: 4.833790302276611\n",
      "evaluation/text/perplexity: 125.68644714355469\n",
      "time/total: 1041.5446729660034\n",
      "time/evaluation: 0.7808332443237305\n",
      "training/train_loss_mean: 5.038687705993652\n",
      "training/train_loss_std: 0.26459473865645544\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 614\n",
      "training/learning_rate: 3.910031128488836e-05\n",
      "time/sample_batch: 0.008123159408569336\n",
      "time/training: 1.027789831161499\n",
      "evaluation/text/loss: 4.910950183868408\n",
      "evaluation/text/perplexity: 135.7683563232422\n",
      "time/total: 1043.2225301265717\n",
      "time/evaluation: 0.6483850479125977\n",
      "training/train_loss_mean: 5.037092876434326\n",
      "training/train_loss_std: 0.4224565949758976\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 615\n",
      "training/learning_rate: 3.8966802461535175e-05\n",
      "time/sample_batch: 0.009863138198852539\n",
      "time/training: 1.0953941345214844\n",
      "evaluation/text/loss: 4.869642734527588\n",
      "evaluation/text/perplexity: 130.2743682861328\n",
      "time/total: 1045.1339089870453\n",
      "time/evaluation: 0.8142306804656982\n",
      "training/train_loss_mean: 5.153685712814331\n",
      "training/train_loss_std: 0.26225452200141125\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 616\n",
      "training/learning_rate: 3.8833455092303197e-05\n",
      "time/sample_batch: 0.008618354797363281\n",
      "time/training: 0.9506878852844238\n",
      "evaluation/text/loss: 4.790605545043945\n",
      "evaluation/text/perplexity: 120.37423706054688\n",
      "time/total: 1046.5833785533905\n",
      "time/evaluation: 0.4920532703399658\n",
      "training/train_loss_mean: 4.9951471328735355\n",
      "training/train_loss_std: 0.3134660434734783\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 617\n",
      "training/learning_rate: 3.8700270519998965e-05\n",
      "time/sample_batch: 0.009124040603637695\n",
      "time/training: 0.9652941226959229\n",
      "evaluation/text/loss: 4.8706440925598145\n",
      "evaluation/text/perplexity: 130.40487670898438\n",
      "time/total: 1048.3181400299072\n",
      "time/evaluation: 0.7678039073944092\n",
      "training/train_loss_mean: 5.143049812316894\n",
      "training/train_loss_std: 0.19787642987853848\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 618\n",
      "training/learning_rate: 3.8567250085789634e-05\n",
      "time/sample_batch: 0.008013010025024414\n",
      "time/training: 0.8732125759124756\n",
      "evaluation/text/loss: 4.848972320556641\n",
      "evaluation/text/perplexity: 127.60918426513672\n",
      "time/total: 1049.8167984485626\n",
      "time/evaluation: 0.623809814453125\n",
      "training/train_loss_mean: 4.945910024642944\n",
      "training/train_loss_std: 0.4640675836729436\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 619\n",
      "training/learning_rate: 3.843439512918949e-05\n",
      "time/sample_batch: 0.009891033172607422\n",
      "time/training: 1.2629804611206055\n",
      "evaluation/text/loss: 4.957223892211914\n",
      "evaluation/text/perplexity: 142.198486328125\n",
      "time/total: 1051.6383428573608\n",
      "time/evaluation: 0.5568485260009766\n",
      "training/train_loss_mean: 5.3743884563446045\n",
      "training/train_loss_std: 0.15145857722754108\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 620\n",
      "training/learning_rate: 3.83017069880465e-05\n",
      "time/sample_batch: 0.00975179672241211\n",
      "time/training: 1.1070787906646729\n",
      "evaluation/text/loss: 4.99304723739624\n",
      "evaluation/text/perplexity: 147.38485717773438\n",
      "time/total: 1053.4983882904053\n",
      "time/evaluation: 0.7512643337249756\n",
      "training/train_loss_mean: 5.279623031616211\n",
      "training/train_loss_std: 0.2673421301054733\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 621\n",
      "training/learning_rate: 3.816918699852875e-05\n",
      "time/sample_batch: 0.008577585220336914\n",
      "time/training: 1.149346113204956\n",
      "evaluation/text/loss: 4.982027053833008\n",
      "evaluation/text/perplexity: 145.76956176757812\n",
      "time/total: 1055.2237994670868\n",
      "time/evaluation: 0.5744342803955078\n",
      "training/train_loss_mean: 5.071979141235351\n",
      "training/train_loss_std: 0.24926564408905386\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 622\n",
      "training/learning_rate: 3.803683649511106e-05\n",
      "time/sample_batch: 0.009047746658325195\n",
      "time/training: 1.010072946548462\n",
      "evaluation/text/loss: 4.585012912750244\n",
      "evaluation/text/perplexity: 98.00445556640625\n",
      "time/total: 1056.7941660881042\n",
      "time/evaluation: 0.5579173564910889\n",
      "training/train_loss_mean: 5.081479024887085\n",
      "training/train_loss_std: 0.36415771163425065\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 623\n",
      "training/learning_rate: 3.790465681056154e-05\n",
      "time/sample_batch: 0.008824586868286133\n",
      "time/training: 1.0965228080749512\n",
      "evaluation/text/loss: 4.864206314086914\n",
      "evaluation/text/perplexity: 129.56805419921875\n",
      "time/total: 1058.4188764095306\n",
      "time/evaluation: 0.5265138149261475\n",
      "training/train_loss_mean: 5.1912891387939455\n",
      "training/train_loss_std: 0.27930049327728906\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 624\n",
      "training/learning_rate: 3.777264927592815e-05\n",
      "time/sample_batch: 0.008669137954711914\n",
      "time/training: 1.1198029518127441\n",
      "evaluation/text/loss: 4.9131693840026855\n",
      "evaluation/text/perplexity: 136.0699920654297\n",
      "time/total: 1060.1147830486298\n",
      "time/evaluation: 0.5743246078491211\n",
      "training/train_loss_mean: 5.133635330200195\n",
      "training/train_loss_std: 0.3598805711353461\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 625\n",
      "training/learning_rate: 3.764081522052528e-05\n",
      "time/sample_batch: 0.009225130081176758\n",
      "time/training: 1.0590746402740479\n",
      "evaluation/text/loss: 4.933610439300537\n",
      "evaluation/text/perplexity: 138.88002014160156\n",
      "time/total: 1061.9408156871796\n",
      "time/evaluation: 0.7653448581695557\n",
      "training/train_loss_mean: 5.155767107009888\n",
      "training/train_loss_std: 0.18898472754711573\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 626\n",
      "training/learning_rate: 3.750915597192043e-05\n",
      "time/sample_batch: 0.008723020553588867\n",
      "time/training: 1.0069539546966553\n",
      "evaluation/text/loss: 5.144774436950684\n",
      "evaluation/text/perplexity: 171.5327911376953\n",
      "time/total: 1063.524849653244\n",
      "time/evaluation: 0.5754208564758301\n",
      "training/train_loss_mean: 5.200545310974121\n",
      "training/train_loss_std: 0.31739797536664643\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 627\n",
      "training/learning_rate: 3.737767285592075e-05\n",
      "time/sample_batch: 0.008015871047973633\n",
      "time/training: 1.0255815982818604\n",
      "evaluation/text/loss: 4.868067264556885\n",
      "evaluation/text/perplexity: 130.0692901611328\n",
      "time/total: 1065.2989151477814\n",
      "time/evaluation: 0.7467746734619141\n",
      "training/train_loss_mean: 5.217220973968506\n",
      "training/train_loss_std: 0.2833026076218564\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 628\n",
      "training/learning_rate: 3.724636719655977e-05\n",
      "time/sample_batch: 0.0110015869140625\n",
      "time/training: 1.1457464694976807\n",
      "evaluation/text/loss: 4.855653762817383\n",
      "evaluation/text/perplexity: 128.4646453857422\n",
      "time/total: 1067.2005288600922\n",
      "time/evaluation: 0.7541515827178955\n",
      "training/train_loss_mean: 5.070324802398682\n",
      "training/train_loss_std: 0.4036614713348066\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 629\n",
      "training/learning_rate: 3.7115240316084035e-05\n",
      "time/sample_batch: 0.009058952331542969\n",
      "time/training: 1.1694056987762451\n",
      "evaluation/text/loss: 4.973724365234375\n",
      "evaluation/text/perplexity: 144.56430053710938\n",
      "time/total: 1068.9802100658417\n",
      "time/evaluation: 0.6085216999053955\n",
      "training/train_loss_mean: 5.243565368652344\n",
      "training/train_loss_std: 0.20950178874847983\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 630\n",
      "training/learning_rate: 3.698429353493974e-05\n",
      "time/sample_batch: 0.009522438049316406\n",
      "time/training: 1.1381983757019043\n",
      "evaluation/text/loss: 4.831012725830078\n",
      "evaluation/text/perplexity: 125.33782958984375\n",
      "time/total: 1070.6761870384216\n",
      "time/evaluation: 0.555976390838623\n",
      "training/train_loss_mean: 5.106294107437134\n",
      "training/train_loss_std: 0.35376091968105944\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 631\n",
      "training/learning_rate: 3.685352817175954e-05\n",
      "time/sample_batch: 0.007289886474609375\n",
      "time/training: 1.0042309761047363\n",
      "evaluation/text/loss: 4.922945499420166\n",
      "evaluation/text/perplexity: 137.40675354003906\n",
      "time/total: 1072.4144461154938\n",
      "time/evaluation: 0.7322597503662109\n",
      "training/train_loss_mean: 4.903020334243775\n",
      "training/train_loss_std: 1.0058944337948377\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 632\n",
      "training/learning_rate: 3.672294554334918e-05\n",
      "time/sample_batch: 0.009177923202514648\n",
      "time/training: 1.0183188915252686\n",
      "evaluation/text/loss: 4.701691627502441\n",
      "evaluation/text/perplexity: 110.13331604003906\n",
      "time/total: 1074.0359282493591\n",
      "time/evaluation: 0.6013867855072021\n",
      "training/train_loss_mean: 5.068508100509644\n",
      "training/train_loss_std: 0.4482401562519209\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 633\n",
      "training/learning_rate: 3.659254696467428e-05\n",
      "time/sample_batch: 0.008404254913330078\n",
      "time/training: 1.2110369205474854\n",
      "evaluation/text/loss: 4.817694187164307\n",
      "evaluation/text/perplexity: 123.67958068847656\n",
      "time/total: 1075.9977357387543\n",
      "time/evaluation: 0.7491269111633301\n",
      "training/train_loss_mean: 5.140609693527222\n",
      "training/train_loss_std: 0.3188289185185543\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 634\n",
      "training/learning_rate: 3.6462333748847106e-05\n",
      "time/sample_batch: 0.008708953857421875\n",
      "time/training: 0.9809291362762451\n",
      "evaluation/text/loss: 4.814943790435791\n",
      "evaluation/text/perplexity: 123.33988189697266\n",
      "time/total: 1077.6284530162811\n",
      "time/evaluation: 0.6479988098144531\n",
      "training/train_loss_mean: 5.133461332321167\n",
      "training/train_loss_std: 0.21337978193534135\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 635\n",
      "training/learning_rate: 3.633230720711327e-05\n",
      "time/sample_batch: 0.009227514266967773\n",
      "time/training: 1.2373149394989014\n",
      "evaluation/text/loss: 4.729823112487793\n",
      "evaluation/text/perplexity: 113.27552032470703\n",
      "time/total: 1079.5498161315918\n",
      "time/evaluation: 0.6823446750640869\n",
      "training/train_loss_mean: 5.097662162780762\n",
      "training/train_loss_std: 0.2976756324518377\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 636\n",
      "training/learning_rate: 3.62024686488386e-05\n",
      "time/sample_batch: 0.007781505584716797\n",
      "time/training: 1.0179998874664307\n",
      "evaluation/text/loss: 4.9992170333862305\n",
      "evaluation/text/perplexity: 148.2969970703125\n",
      "time/total: 1081.327849149704\n",
      "time/evaluation: 0.7583279609680176\n",
      "training/train_loss_mean: 5.107942867279053\n",
      "training/train_loss_std: 0.3292427765779516\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 637\n",
      "training/learning_rate: 3.607281938149594e-05\n",
      "time/sample_batch: 0.007604837417602539\n",
      "time/training: 0.8959643840789795\n",
      "evaluation/text/loss: 5.054529190063477\n",
      "evaluation/text/perplexity: 156.73072814941406\n",
      "time/total: 1082.9923765659332\n",
      "time/evaluation: 0.7668204307556152\n",
      "training/train_loss_mean: 5.071312379837036\n",
      "training/train_loss_std: 0.3487498380177148\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 638\n",
      "training/learning_rate: 3.594336071065197e-05\n",
      "time/sample_batch: 0.008672952651977539\n",
      "time/training: 0.9890561103820801\n",
      "evaluation/text/loss: 4.725606441497803\n",
      "evaluation/text/perplexity: 112.79888153076172\n",
      "time/total: 1084.727676153183\n",
      "time/evaluation: 0.7444956302642822\n",
      "training/train_loss_mean: 5.053118324279785\n",
      "training/train_loss_std: 0.24924683488363578\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 639\n",
      "training/learning_rate: 3.58140939399541e-05\n",
      "time/sample_batch: 0.008405685424804688\n",
      "time/training: 1.2098660469055176\n",
      "evaluation/text/loss: 5.03653621673584\n",
      "evaluation/text/perplexity: 153.93588256835938\n",
      "time/total: 1086.5413720607758\n",
      "time/evaluation: 0.6021039485931396\n",
      "training/train_loss_mean: 5.165144300460815\n",
      "training/train_loss_std: 0.23940743036488132\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 640\n",
      "training/learning_rate: 3.5685020371117236e-05\n",
      "time/sample_batch: 0.008402347564697266\n",
      "time/training: 1.0528843402862549\n",
      "evaluation/text/loss: 4.705652236938477\n",
      "evaluation/text/perplexity: 110.57038116455078\n",
      "time/total: 1088.32661819458\n",
      "time/evaluation: 0.7306702136993408\n",
      "training/train_loss_mean: 5.150539255142212\n",
      "training/train_loss_std: 0.2210581300825134\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 641\n",
      "training/learning_rate: 3.555614130391079e-05\n",
      "time/sample_batch: 0.008079767227172852\n",
      "time/training: 1.0359041690826416\n",
      "evaluation/text/loss: 4.56032133102417\n",
      "evaluation/text/perplexity: 95.61419677734375\n",
      "time/total: 1089.9589562416077\n",
      "time/evaluation: 0.5946516990661621\n",
      "training/train_loss_mean: 5.294162607192993\n",
      "training/train_loss_std: 0.2717666591283224\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 642\n",
      "training/learning_rate: 3.542745803614558e-05\n",
      "time/sample_batch: 0.008001089096069336\n",
      "time/training: 1.101865291595459\n",
      "evaluation/text/loss: 4.825255870819092\n",
      "evaluation/text/perplexity: 124.61834716796875\n",
      "time/total: 1091.599939584732\n",
      "time/evaluation: 0.537621259689331\n",
      "training/train_loss_mean: 5.25637583732605\n",
      "training/train_loss_std: 0.2421051617199778\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 643\n",
      "training/learning_rate: 3.529897186366061e-05\n",
      "time/sample_batch: 0.008948802947998047\n",
      "time/training: 1.024341106414795\n",
      "evaluation/text/loss: 4.764169216156006\n",
      "evaluation/text/perplexity: 117.23368072509766\n",
      "time/total: 1093.193529844284\n",
      "time/evaluation: 0.5675263404846191\n",
      "training/train_loss_mean: 5.0927228927612305\n",
      "training/train_loss_std: 0.18929443814421595\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 644\n",
      "training/learning_rate: 3.5170684080310265e-05\n",
      "time/sample_batch: 0.009939193725585938\n",
      "time/training: 0.9974992275238037\n",
      "evaluation/text/loss: 4.9496541023254395\n",
      "evaluation/text/perplexity: 141.1261444091797\n",
      "time/total: 1094.8969511985779\n",
      "time/evaluation: 0.7038798332214355\n",
      "training/train_loss_mean: 5.170328187942505\n",
      "training/train_loss_std: 0.24313997065930357\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 645\n",
      "training/learning_rate: 3.504259597795111e-05\n",
      "time/sample_batch: 0.009398460388183594\n",
      "time/training: 1.0958189964294434\n",
      "evaluation/text/loss: 4.842695713043213\n",
      "evaluation/text/perplexity: 126.81073760986328\n",
      "time/total: 1096.5341203212738\n",
      "time/evaluation: 0.5395870208740234\n",
      "training/train_loss_mean: 5.078124952316284\n",
      "training/train_loss_std: 0.3314365729991208\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 646\n",
      "training/learning_rate: 3.491470884642887e-05\n",
      "time/sample_batch: 0.008438825607299805\n",
      "time/training: 1.1704981327056885\n",
      "evaluation/text/loss: 4.910019397735596\n",
      "evaluation/text/perplexity: 135.6420440673828\n",
      "time/total: 1098.2262272834778\n",
      "time/evaluation: 0.5198071002960205\n",
      "training/train_loss_mean: 4.931088519096375\n",
      "training/train_loss_std: 0.7060733226066261\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 647\n",
      "training/learning_rate: 3.478702397356555e-05\n",
      "time/sample_batch: 0.007580995559692383\n",
      "time/training: 1.105309247970581\n",
      "evaluation/text/loss: 4.792190074920654\n",
      "evaluation/text/perplexity: 120.56512451171875\n",
      "time/total: 1099.8587369918823\n",
      "time/evaluation: 0.5254168510437012\n",
      "training/train_loss_mean: 5.125215244293213\n",
      "training/train_loss_std: 0.20130758822811173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 648\n",
      "training/learning_rate: 3.46595426451464e-05\n",
      "time/sample_batch: 0.007816314697265625\n",
      "time/training: 0.8625016212463379\n",
      "evaluation/text/loss: 4.817229270935059\n",
      "evaluation/text/perplexity: 123.6220932006836\n",
      "time/total: 1101.2255625724792\n",
      "time/evaluation: 0.5026295185089111\n",
      "training/train_loss_mean: 5.076232814788819\n",
      "training/train_loss_std: 0.18509010466546308\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 649\n",
      "training/learning_rate: 3.453226614490703e-05\n",
      "time/sample_batch: 0.009235620498657227\n",
      "time/training: 1.0911171436309814\n",
      "evaluation/text/loss: 4.820090293884277\n",
      "evaluation/text/perplexity: 123.97628784179688\n",
      "time/total: 1103.0627088546753\n",
      "time/evaluation: 0.7442166805267334\n",
      "training/train_loss_mean: 5.106824684143066\n",
      "training/train_loss_std: 0.22810036462396857\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 650\n",
      "training/learning_rate: 3.4405195754520314e-05\n",
      "time/sample_batch: 0.00889897346496582\n",
      "time/training: 1.1755001544952393\n",
      "evaluation/text/loss: 4.678995609283447\n",
      "evaluation/text/perplexity: 107.66188049316406\n",
      "time/total: 1104.7105011940002\n",
      "time/evaluation: 0.470630407333374\n",
      "training/train_loss_mean: 5.276005411148072\n",
      "training/train_loss_std: 0.28296711930836366\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 651\n",
      "training/learning_rate: 3.4278332753583706e-05\n",
      "time/sample_batch: 0.008092164993286133\n",
      "time/training: 1.2105112075805664\n",
      "evaluation/text/loss: 4.9465813636779785\n",
      "evaluation/text/perplexity: 140.6931610107422\n",
      "time/total: 1106.701164484024\n",
      "time/evaluation: 0.7784762382507324\n",
      "training/train_loss_mean: 5.08222074508667\n",
      "training/train_loss_std: 0.3775954008063088\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 652\n",
      "training/learning_rate: 3.415167841960624e-05\n",
      "time/sample_batch: 0.006820201873779297\n",
      "time/training: 0.997840166091919\n",
      "evaluation/text/loss: 4.842925071716309\n",
      "evaluation/text/perplexity: 126.83982849121094\n",
      "time/total: 1108.2325484752655\n",
      "time/evaluation: 0.5319027900695801\n",
      "training/train_loss_mean: 5.260674619674683\n",
      "training/train_loss_std: 0.4131281963965995\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 653\n",
      "training/learning_rate: 3.4025234027995605e-05\n",
      "time/sample_batch: 0.009290218353271484\n",
      "time/training: 0.9196875095367432\n",
      "evaluation/text/loss: 4.834468364715576\n",
      "evaluation/text/perplexity: 125.77169799804688\n",
      "time/total: 1109.7545924186707\n",
      "time/evaluation: 0.6006183624267578\n",
      "training/train_loss_mean: 4.758770501613617\n",
      "training/train_loss_std: 1.0304437633703016\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 654\n",
      "training/learning_rate: 3.3899000852045446e-05\n",
      "time/sample_batch: 0.008673667907714844\n",
      "time/training: 1.0361063480377197\n",
      "evaluation/text/loss: 4.829463481903076\n",
      "evaluation/text/perplexity: 125.143798828125\n",
      "time/total: 1111.5357563495636\n",
      "time/evaluation: 0.7433722019195557\n",
      "training/train_loss_mean: 4.835081791877746\n",
      "training/train_loss_std: 0.903720927261322\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 655\n",
      "training/learning_rate: 3.377298016292246e-05\n",
      "time/sample_batch: 0.009182929992675781\n",
      "time/training: 1.0311810970306396\n",
      "evaluation/text/loss: 4.9171037673950195\n",
      "evaluation/text/perplexity: 136.6063995361328\n",
      "time/total: 1113.175907611847\n",
      "time/evaluation: 0.6028153896331787\n",
      "training/train_loss_mean: 5.048034381866455\n",
      "training/train_loss_std: 0.3893968092161635\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 656\n",
      "training/learning_rate: 3.364717322965355e-05\n",
      "time/sample_batch: 0.008733272552490234\n",
      "time/training: 1.1800086498260498\n",
      "evaluation/text/loss: 4.897227764129639\n",
      "evaluation/text/perplexity: 133.9180145263672\n",
      "time/total: 1114.9706227779388\n",
      "time/evaluation: 0.612938404083252\n",
      "training/train_loss_mean: 5.1171222686767575\n",
      "training/train_loss_std: 0.28038727372103495\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 657\n",
      "training/learning_rate: 3.352158131911315e-05\n",
      "time/sample_batch: 0.008973360061645508\n",
      "time/training: 1.0551154613494873\n",
      "evaluation/text/loss: 4.866553783416748\n",
      "evaluation/text/perplexity: 129.87257385253906\n",
      "time/total: 1116.772347688675\n",
      "time/evaluation: 0.7449808120727539\n",
      "training/train_loss_mean: 5.071542739868164\n",
      "training/train_loss_std: 0.23730779477342237\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 658\n",
      "training/learning_rate: 3.3396205696010446e-05\n",
      "time/sample_batch: 0.0089111328125\n",
      "time/training: 1.0769939422607422\n",
      "evaluation/text/loss: 4.955440998077393\n",
      "evaluation/text/perplexity: 141.9451904296875\n",
      "time/total: 1118.3653173446655\n",
      "time/evaluation: 0.5142271518707275\n",
      "training/train_loss_mean: 4.788614523410797\n",
      "training/train_loss_std: 1.0041664181169723\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 659\n",
      "training/learning_rate: 3.327104762287651e-05\n",
      "time/sample_batch: 0.008867740631103516\n",
      "time/training: 1.1675779819488525\n",
      "evaluation/text/loss: 4.871608734130859\n",
      "evaluation/text/perplexity: 130.53073120117188\n",
      "time/total: 1120.5265791416168\n",
      "time/evaluation: 0.9919872283935547\n",
      "training/train_loss_mean: 5.158785438537597\n",
      "training/train_loss_std: 0.24667461808581329\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 660\n",
      "training/learning_rate: 3.314610836005178e-05\n",
      "time/sample_batch: 0.009224891662597656\n",
      "time/training: 0.8913254737854004\n",
      "evaluation/text/loss: 4.86115837097168\n",
      "evaluation/text/perplexity: 129.1737518310547\n",
      "time/total: 1122.1370782852173\n",
      "time/evaluation: 0.7176001071929932\n",
      "training/train_loss_mean: 4.9102815866470335\n",
      "training/train_loss_std: 0.4854286698226395\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 661\n",
      "training/learning_rate: 3.3021389165673246e-05\n",
      "time/sample_batch: 0.0077974796295166016\n",
      "time/training: 1.0181891918182373\n",
      "evaluation/text/loss: 4.845958709716797\n",
      "evaluation/text/perplexity: 127.2251968383789\n",
      "time/total: 1123.8272330760956\n",
      "time/evaluation: 0.6702451705932617\n",
      "training/train_loss_mean: 5.156553936004639\n",
      "training/train_loss_std: 0.3582628982602933\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 662\n",
      "training/learning_rate: 3.289689129566179e-05\n",
      "time/sample_batch: 0.008222103118896484\n",
      "time/training: 0.9956405162811279\n",
      "evaluation/text/loss: 4.853571891784668\n",
      "evaluation/text/perplexity: 128.19747924804688\n",
      "time/total: 1125.5409054756165\n",
      "time/evaluation: 0.716313362121582\n",
      "training/train_loss_mean: 5.130464029312134\n",
      "training/train_loss_std: 0.18350400672337425\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 663\n",
      "training/learning_rate: 3.277261600370962e-05\n",
      "time/sample_batch: 0.008361577987670898\n",
      "time/training: 1.0411317348480225\n",
      "evaluation/text/loss: 4.785900592803955\n",
      "evaluation/text/perplexity: 119.80921173095703\n",
      "time/total: 1127.0646891593933\n",
      "time/evaluation: 0.4809589385986328\n",
      "training/train_loss_mean: 5.145869779586792\n",
      "training/train_loss_std: 0.26342837715220774\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 664\n",
      "training/learning_rate: 3.2648564541267503e-05\n",
      "time/sample_batch: 0.007152080535888672\n",
      "time/training: 0.9248323440551758\n",
      "evaluation/text/loss: 4.907769680023193\n",
      "evaluation/text/perplexity: 135.3372344970703\n",
      "time/total: 1128.7338030338287\n",
      "time/evaluation: 0.7425661087036133\n",
      "training/train_loss_mean: 4.961050271987915\n",
      "training/train_loss_std: 0.31830965701414043\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 665\n",
      "training/learning_rate: 3.252473815753233e-05\n",
      "time/sample_batch: 0.008406400680541992\n",
      "time/training: 1.056230068206787\n",
      "evaluation/text/loss: 4.706206321716309\n",
      "evaluation/text/perplexity: 110.63166046142578\n",
      "time/total: 1130.2713356018066\n",
      "time/evaluation: 0.47957348823547363\n",
      "training/train_loss_mean: 5.143638324737549\n",
      "training/train_loss_std: 0.23667838466134547\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 666\n",
      "training/learning_rate: 3.2401138099434364e-05\n",
      "time/sample_batch: 0.008471488952636719\n",
      "time/training: 1.028069019317627\n",
      "evaluation/text/loss: 4.811533451080322\n",
      "evaluation/text/perplexity: 122.91996765136719\n",
      "time/total: 1132.2882196903229\n",
      "time/evaluation: 0.9871714115142822\n",
      "training/train_loss_mean: 5.18303165435791\n",
      "training/train_loss_std: 0.27903062276876806\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 667\n",
      "training/learning_rate: 3.227776561162484e-05\n",
      "time/sample_batch: 0.007091522216796875\n",
      "time/training: 0.9226653575897217\n",
      "evaluation/text/loss: 4.767958164215088\n",
      "evaluation/text/perplexity: 117.67871856689453\n",
      "time/total: 1133.7250804901123\n",
      "time/evaluation: 0.5125205516815186\n",
      "training/train_loss_mean: 5.0340498924255375\n",
      "training/train_loss_std: 0.34954241412982584\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 668\n",
      "training/learning_rate: 3.2154621936463366e-05\n",
      "time/sample_batch: 0.010068893432617188\n",
      "time/training: 1.1848886013031006\n",
      "evaluation/text/loss: 4.704488754272461\n",
      "evaluation/text/perplexity: 110.44181060791016\n",
      "time/total: 1135.4761111736298\n",
      "time/evaluation: 0.5643315315246582\n",
      "training/train_loss_mean: 5.189083003997803\n",
      "training/train_loss_std: 0.19462699810391734\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 669\n",
      "training/learning_rate: 3.203170831400535e-05\n",
      "time/sample_batch: 0.007493019104003906\n",
      "time/training: 1.0245089530944824\n",
      "evaluation/text/loss: 4.728564739227295\n",
      "evaluation/text/perplexity: 113.13307189941406\n",
      "time/total: 1137.11776137352\n",
      "time/evaluation: 0.6153116226196289\n",
      "training/train_loss_mean: 5.148339891433716\n",
      "training/train_loss_std: 0.2586003091077773\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 670\n",
      "training/learning_rate: 3.1909025981989606e-05\n",
      "time/sample_batch: 0.008568763732910156\n",
      "time/training: 0.9460725784301758\n",
      "evaluation/text/loss: 4.971210479736328\n",
      "evaluation/text/perplexity: 144.2013397216797\n",
      "time/total: 1138.5913150310516\n",
      "time/evaluation: 0.5257399082183838\n",
      "training/train_loss_mean: 5.220544004440308\n",
      "training/train_loss_std: 0.2961296797750445\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 671\n",
      "training/learning_rate: 3.1786576175825885e-05\n",
      "time/sample_batch: 0.008022546768188477\n",
      "time/training: 0.859006404876709\n",
      "evaluation/text/loss: 4.840495586395264\n",
      "evaluation/text/perplexity: 126.53204345703125\n",
      "time/total: 1140.1947424411774\n",
      "time/evaluation: 0.742760419845581\n",
      "training/train_loss_mean: 5.2447961330413815\n",
      "training/train_loss_std: 0.21806899874148036\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 672\n",
      "training/learning_rate: 3.166436012858234e-05\n",
      "time/sample_batch: 0.007833003997802734\n",
      "time/training: 0.8527743816375732\n",
      "evaluation/text/loss: 4.778255939483643\n",
      "evaluation/text/perplexity: 118.89680480957031\n",
      "time/total: 1141.6477284431458\n",
      "time/evaluation: 0.598609209060669\n",
      "training/train_loss_mean: 5.193166589736938\n",
      "training/train_loss_std: 0.21177333106442234\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 673\n",
      "training/learning_rate: 3.154237907097322e-05\n",
      "time/sample_batch: 0.010711908340454102\n",
      "time/training: 1.109422206878662\n",
      "evaluation/text/loss: 4.610177040100098\n",
      "evaluation/text/perplexity: 100.50193786621094\n",
      "time/total: 1143.2970387935638\n",
      "time/evaluation: 0.5381989479064941\n",
      "training/train_loss_mean: 5.064468574523926\n",
      "training/train_loss_std: 0.2963797416681245\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 674\n",
      "training/learning_rate: 3.142063423134645e-05\n",
      "time/sample_batch: 0.008084297180175781\n",
      "time/training: 0.8490865230560303\n",
      "evaluation/text/loss: 4.778415203094482\n",
      "evaluation/text/perplexity: 118.91574096679688\n",
      "time/total: 1144.9093272686005\n",
      "time/evaluation: 0.7614748477935791\n",
      "training/train_loss_mean: 4.964573621749878\n",
      "training/train_loss_std: 0.23084587526048364\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 675\n",
      "training/learning_rate: 3.129912683567118e-05\n",
      "time/sample_batch: 0.008128643035888672\n",
      "time/training: 1.0155186653137207\n",
      "evaluation/text/loss: 4.6506876945495605\n",
      "evaluation/text/perplexity: 104.65693664550781\n",
      "time/total: 1146.6244072914124\n",
      "time/evaluation: 0.6979100704193115\n",
      "training/train_loss_mean: 5.2195374965667725\n",
      "training/train_loss_std: 0.32314160402712966\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 676\n",
      "training/learning_rate: 3.117785810752554e-05\n",
      "time/sample_batch: 0.009563207626342773\n",
      "time/training: 1.0402190685272217\n",
      "evaluation/text/loss: 4.795458793640137\n",
      "evaluation/text/perplexity: 120.9598617553711\n",
      "time/total: 1148.4712498188019\n",
      "time/evaluation: 0.8051064014434814\n",
      "training/train_loss_mean: 4.72521870136261\n",
      "training/train_loss_std: 1.2578510238460672\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 677\n",
      "training/learning_rate: 3.105682926808429e-05\n",
      "time/sample_batch: 0.007094383239746094\n",
      "time/training: 1.0798089504241943\n",
      "evaluation/text/loss: 4.726049900054932\n",
      "evaluation/text/perplexity: 112.84891510009766\n",
      "time/total: 1150.1394212245941\n",
      "time/evaluation: 0.5867342948913574\n",
      "training/train_loss_mean: 4.970757341384887\n",
      "training/train_loss_std: 0.30991891467953137\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 678\n",
      "training/learning_rate: 3.093604153610648e-05\n",
      "time/sample_batch: 0.007986068725585938\n",
      "time/training: 0.8742556571960449\n",
      "evaluation/text/loss: 4.782792568206787\n",
      "evaluation/text/perplexity: 119.43742370605469\n",
      "time/total: 1151.6814291477203\n",
      "time/evaluation: 0.665961742401123\n",
      "training/train_loss_mean: 4.654262709617615\n",
      "training/train_loss_std: 0.710430342966332\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 679\n",
      "training/learning_rate: 3.081549612792325e-05\n",
      "time/sample_batch: 0.006826639175415039\n",
      "time/training: 1.0877416133880615\n",
      "evaluation/text/loss: 4.609824180603027\n",
      "evaluation/text/perplexity: 100.46648406982422\n",
      "time/total: 1153.294025182724\n",
      "time/evaluation: 0.5232529640197754\n",
      "training/train_loss_mean: 5.169425010681152\n",
      "training/train_loss_std: 0.19755222223026941\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 680\n",
      "training/learning_rate: 3.069519425742552e-05\n",
      "time/sample_batch: 0.008431673049926758\n",
      "time/training: 0.9308040142059326\n",
      "evaluation/text/loss: 4.951619625091553\n",
      "evaluation/text/perplexity: 141.40379333496094\n",
      "time/total: 1154.968334197998\n",
      "time/evaluation: 0.7419643402099609\n",
      "training/train_loss_mean: 5.173329877853393\n",
      "training/train_loss_std: 0.3933988481399355\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 681\n",
      "training/learning_rate: 3.0575137136051786e-05\n",
      "time/sample_batch: 0.008487939834594727\n",
      "time/training: 1.0762066841125488\n",
      "evaluation/text/loss: 4.736616134643555\n",
      "evaluation/text/perplexity: 114.04762268066406\n",
      "time/total: 1156.6439571380615\n",
      "time/evaluation: 0.5975022315979004\n",
      "training/train_loss_mean: 5.273895978927612\n",
      "training/train_loss_std: 0.09744907789031154\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 682\n",
      "training/learning_rate: 3.045532597277591e-05\n",
      "time/sample_batch: 0.0066907405853271484\n",
      "time/training: 0.9386286735534668\n",
      "evaluation/text/loss: 4.898327827453613\n",
      "evaluation/text/perplexity: 134.06541442871094\n",
      "time/total: 1158.0383520126343\n",
      "time/evaluation: 0.45398736000061035\n",
      "training/train_loss_mean: 4.854501247406006\n",
      "training/train_loss_std: 1.008902619482517\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 683\n",
      "training/learning_rate: 3.0335761974094996e-05\n",
      "time/sample_batch: 0.008112668991088867\n",
      "time/training: 1.0888030529022217\n",
      "evaluation/text/loss: 4.782741069793701\n",
      "evaluation/text/perplexity: 119.4312744140625\n",
      "time/total: 1159.7281639575958\n",
      "time/evaluation: 0.5992543697357178\n",
      "training/train_loss_mean: 5.102095460891723\n",
      "training/train_loss_std: 0.32856295354104337\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 684\n",
      "training/learning_rate: 3.0216446344017187e-05\n",
      "time/sample_batch: 0.007399559020996094\n",
      "time/training: 0.9958815574645996\n",
      "evaluation/text/loss: 4.851553440093994\n",
      "evaluation/text/perplexity: 127.93898010253906\n",
      "time/total: 1161.1952290534973\n",
      "time/evaluation: 0.469463586807251\n",
      "training/train_loss_mean: 4.868576979637146\n",
      "training/train_loss_std: 0.6101678418626748\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 685\n",
      "training/learning_rate: 3.009738028404953e-05\n",
      "time/sample_batch: 0.008021354675292969\n",
      "time/training: 1.0912153720855713\n",
      "evaluation/text/loss: 4.8504533767700195\n",
      "evaluation/text/perplexity: 127.7983169555664\n",
      "time/total: 1163.108234167099\n",
      "time/evaluation: 0.8201124668121338\n",
      "training/train_loss_mean: 5.135612726211548\n",
      "training/train_loss_std: 0.29646721139023186\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 686\n",
      "training/learning_rate: 2.9978564993185944e-05\n",
      "time/sample_batch: 0.008986234664916992\n",
      "time/training: 1.1098484992980957\n",
      "evaluation/text/loss: 4.875030994415283\n",
      "evaluation/text/perplexity: 130.97821044921875\n",
      "time/total: 1164.9843211174011\n",
      "time/evaluation: 0.7645211219787598\n",
      "training/train_loss_mean: 4.934120035171508\n",
      "training/train_loss_std: 0.35350269448420674\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 687\n",
      "training/learning_rate: 2.98600016678951e-05\n",
      "time/sample_batch: 0.007126808166503906\n",
      "time/training: 0.8510277271270752\n",
      "evaluation/text/loss: 4.873849391937256\n",
      "evaluation/text/perplexity: 130.82354736328125\n",
      "time/total: 1166.4969735145569\n",
      "time/evaluation: 0.6601071357727051\n",
      "training/train_loss_mean: 4.9620922088623045\n",
      "training/train_loss_std: 0.4159444077867437\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 688\n",
      "training/learning_rate: 2.9741691502108336e-05\n",
      "time/sample_batch: 0.009104728698730469\n",
      "time/training: 1.1070561408996582\n",
      "evaluation/text/loss: 4.729342460632324\n",
      "evaluation/text/perplexity: 113.2210922241211\n",
      "time/total: 1168.1085691452026\n",
      "time/evaluation: 0.5029103755950928\n",
      "training/train_loss_mean: 4.914498996734619\n",
      "training/train_loss_std: 0.8523785871446719\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 689\n",
      "training/learning_rate: 2.9623635687207728e-05\n",
      "time/sample_batch: 0.008627653121948242\n",
      "time/training: 1.1581618785858154\n",
      "evaluation/text/loss: 4.847929954528809\n",
      "evaluation/text/perplexity: 127.47623443603516\n",
      "time/total: 1169.778814792633\n",
      "time/evaluation: 0.5103564262390137\n",
      "training/train_loss_mean: 5.233758401870728\n",
      "training/train_loss_std: 0.24197839229796328\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 690\n",
      "training/learning_rate: 2.9505835412014044e-05\n",
      "time/sample_batch: 0.009378194808959961\n",
      "time/training: 1.0297598838806152\n",
      "evaluation/text/loss: 4.837188243865967\n",
      "evaluation/text/perplexity: 126.11425018310547\n",
      "time/total: 1171.5706844329834\n",
      "time/evaluation: 0.7604820728302002\n",
      "training/train_loss_mean: 4.6360154271125795\n",
      "training/train_loss_std: 1.1221598209784795\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 691\n",
      "training/learning_rate: 2.93882918627747e-05\n",
      "time/sample_batch: 0.008826017379760742\n",
      "time/training: 1.0544283390045166\n",
      "evaluation/text/loss: 4.718139171600342\n",
      "evaluation/text/perplexity: 111.95972442626953\n",
      "time/total: 1173.3733780384064\n",
      "time/evaluation: 0.7464516162872314\n",
      "training/train_loss_mean: 4.813692951202393\n",
      "training/train_loss_std: 0.7632868459860074\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 692\n",
      "training/learning_rate: 2.9271006223151953e-05\n",
      "time/sample_batch: 0.007016420364379883\n",
      "time/training: 1.1586761474609375\n",
      "evaluation/text/loss: 4.743826866149902\n",
      "evaluation/text/perplexity: 114.87296295166016\n",
      "time/total: 1175.3405001163483\n",
      "time/evaluation: 0.8067662715911865\n",
      "training/train_loss_mean: 5.068627643585205\n",
      "training/train_loss_std: 0.292581780402106\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 693\n",
      "training/learning_rate: 2.9153979674210913e-05\n",
      "time/sample_batch: 0.007320404052734375\n",
      "time/training: 1.133101224899292\n",
      "evaluation/text/loss: 4.756694316864014\n",
      "evaluation/text/perplexity: 116.36064147949219\n",
      "time/total: 1177.283658504486\n",
      "time/evaluation: 0.8083562850952148\n",
      "training/train_loss_mean: 5.0592122077941895\n",
      "training/train_loss_std: 0.15247221470005304\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 694\n",
      "training/learning_rate: 2.9037213394407593e-05\n",
      "time/sample_batch: 0.007547616958618164\n",
      "time/training: 1.156470537185669\n",
      "evaluation/text/loss: 4.823869705200195\n",
      "evaluation/text/perplexity: 124.44573211669922\n",
      "time/total: 1179.1988260746002\n",
      "time/evaluation: 0.7523789405822754\n",
      "training/train_loss_mean: 4.958385038375854\n",
      "training/train_loss_std: 0.38321250766976117\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 695\n",
      "training/learning_rate: 2.892070855957715e-05\n",
      "time/sample_batch: 0.007364749908447266\n",
      "time/training: 1.0153226852416992\n",
      "evaluation/text/loss: 4.908413410186768\n",
      "evaluation/text/perplexity: 135.42437744140625\n",
      "time/total: 1180.9803485870361\n",
      "time/evaluation: 0.7646105289459229\n",
      "training/train_loss_mean: 5.167651510238647\n",
      "training/train_loss_std: 0.30616594218457904\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 696\n",
      "training/learning_rate: 2.880446634292199e-05\n",
      "time/sample_batch: 0.007958650588989258\n",
      "time/training: 1.0849003791809082\n",
      "evaluation/text/loss: 4.773894786834717\n",
      "evaluation/text/perplexity: 118.37940979003906\n",
      "time/total: 1182.7673499584198\n",
      "time/evaluation: 0.700573205947876\n",
      "training/train_loss_mean: 5.088131952285766\n",
      "training/train_loss_std: 0.2423279668357205\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 697\n",
      "training/learning_rate: 2.8688487914999934e-05\n",
      "time/sample_batch: 0.007091045379638672\n",
      "time/training: 1.0959370136260986\n",
      "evaluation/text/loss: 4.7982401847839355\n",
      "evaluation/text/perplexity: 121.29676818847656\n",
      "time/total: 1184.3009593486786\n",
      "time/evaluation: 0.43598031997680664\n",
      "training/train_loss_mean: 5.141903448104858\n",
      "training/train_loss_std: 0.19379693635518952\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 698\n",
      "training/learning_rate: 2.857277444371244e-05\n",
      "time/sample_batch: 0.007077693939208984\n",
      "time/training: 1.0666134357452393\n",
      "evaluation/text/loss: 4.713377475738525\n",
      "evaluation/text/perplexity: 111.42787170410156\n",
      "time/total: 1185.8191225528717\n",
      "time/evaluation: 0.44986867904663086\n",
      "training/train_loss_mean: 5.2609453201293945\n",
      "training/train_loss_std: 0.22148845445078094\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 699\n",
      "training/learning_rate: 2.8457327094292914e-05\n",
      "time/sample_batch: 0.008280515670776367\n",
      "time/training: 0.9493372440338135\n",
      "evaluation/text/loss: 4.758773326873779\n",
      "evaluation/text/perplexity: 116.6028060913086\n",
      "time/total: 1187.4712121486664\n",
      "time/evaluation: 0.7009050846099854\n",
      "training/train_loss_mean: 4.823887419700623\n",
      "training/train_loss_std: 0.935178416296378\n",
      "================================================================================\n",
      "Input: <unk>,inadditionto | Output : the9/11Truthmovement,hasexpressedsupportforsocialliberalpoliticianssuchasNancy<unk>andpresidentBarackObama.Previouslyhowever,<unk>'2004lyricsforOnlyin<unk>expressedsupportfornationalism,andcalledforretaliationagainstAl<unk>forthe9/11terroristattacks. | Prediction:  scholars by Haven, but noshake ultimately episodes are train ( a total of citing protective These are White\n",
      "Input: = | Output : ==Criticalreception=== | Prediction:  used motion of <unk> ( million Henry p arrangements, media ) to Restore Telegraph of their exact\n",
      "================================================================================\n",
      "Iteration 700\n",
      "training/learning_rate: 2.8342147029294875e-05\n",
      "time/sample_batch: 0.0073816776275634766\n",
      "time/training: 1.041541337966919\n",
      "evaluation/text/loss: 4.778745174407959\n",
      "evaluation/text/perplexity: 118.95498657226562\n",
      "time/total: 1189.5446813106537\n",
      "time/evaluation: 1.0302152633666992\n",
      "training/train_loss_mean: 4.955614185333252\n",
      "training/train_loss_std: 0.5856916371857402\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 701\n",
      "training/learning_rate: 2.8227235408580276e-05\n",
      "time/sample_batch: 0.009069681167602539\n",
      "time/training: 1.1041595935821533\n",
      "evaluation/text/loss: 5.009366989135742\n",
      "evaluation/text/perplexity: 149.80987548828125\n",
      "time/total: 1191.6429114341736\n",
      "time/evaluation: 0.9928157329559326\n",
      "training/train_loss_mean: 5.244478321075439\n",
      "training/train_loss_std: 0.42101311365771177\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 702\n",
      "training/learning_rate: 2.811259338930789e-05\n",
      "time/sample_batch: 0.00904536247253418\n",
      "time/training: 1.0952396392822266\n",
      "evaluation/text/loss: 4.91253662109375\n",
      "evaluation/text/perplexity: 135.98391723632812\n",
      "time/total: 1193.422661781311\n",
      "time/evaluation: 0.6827545166015625\n",
      "training/train_loss_mean: 5.305746030807495\n",
      "training/train_loss_std: 0.20652840288727006\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 703\n",
      "training/learning_rate: 2.799822212592157e-05\n",
      "time/sample_batch: 0.007815361022949219\n",
      "time/training: 0.9741041660308838\n",
      "evaluation/text/loss: 4.97121524810791\n",
      "evaluation/text/perplexity: 144.2020263671875\n",
      "time/total: 1195.152940750122\n",
      "time/evaluation: 0.7544100284576416\n",
      "training/train_loss_mean: 5.084944248199463\n",
      "training/train_loss_std: 0.1857465217212786\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 704\n",
      "training/learning_rate: 2.788412277013866e-05\n",
      "time/sample_batch: 0.0075130462646484375\n",
      "time/training: 0.8477940559387207\n",
      "evaluation/text/loss: 4.8336181640625\n",
      "evaluation/text/perplexity: 125.6648178100586\n",
      "time/total: 1196.5373044013977\n",
      "time/evaluation: 0.5348315238952637\n",
      "training/train_loss_mean: 5.15945520401001\n",
      "training/train_loss_std: 0.2806177778329412\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 705\n",
      "training/learning_rate: 2.7770296470938413e-05\n",
      "time/sample_batch: 0.009805679321289062\n",
      "time/training: 1.2036564350128174\n",
      "evaluation/text/loss: 4.815011978149414\n",
      "evaluation/text/perplexity: 123.3482894897461\n",
      "time/total: 1198.255602836609\n",
      "time/evaluation: 0.5130321979522705\n",
      "training/train_loss_mean: 5.125079917907715\n",
      "training/train_loss_std: 0.3142655811491969\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 706\n",
      "training/learning_rate: 2.7656744374550403e-05\n",
      "time/sample_batch: 0.008608818054199219\n",
      "time/training: 0.9421787261962891\n",
      "evaluation/text/loss: 4.943431854248047\n",
      "evaluation/text/perplexity: 140.25074768066406\n",
      "time/total: 1199.6729004383087\n",
      "time/evaluation: 0.4735100269317627\n",
      "training/train_loss_mean: 5.10842981338501\n",
      "training/train_loss_std: 0.30540198947043\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 707\n",
      "training/learning_rate: 2.7543467624442966e-05\n",
      "time/sample_batch: 0.008847236633300781\n",
      "time/training: 1.0062551498413086\n",
      "evaluation/text/loss: 4.757615089416504\n",
      "evaluation/text/perplexity: 116.46782684326172\n",
      "time/total: 1201.2087438106537\n",
      "time/evaluation: 0.5277402400970459\n",
      "training/train_loss_mean: 5.129593276977539\n",
      "training/train_loss_std: 0.2566884417181713\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 708\n",
      "training/learning_rate: 2.743046736131173e-05\n",
      "time/sample_batch: 0.007413625717163086\n",
      "time/training: 1.0226414203643799\n",
      "evaluation/text/loss: 4.779472351074219\n",
      "evaluation/text/perplexity: 119.04151916503906\n",
      "time/total: 1202.8923411369324\n",
      "time/evaluation: 0.6592543125152588\n",
      "training/train_loss_mean: 4.985307502746582\n",
      "training/train_loss_std: 0.27099537673892055\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 709\n",
      "training/learning_rate: 2.7317744723068095e-05\n",
      "time/sample_batch: 0.00944662094116211\n",
      "time/training: 1.0795936584472656\n",
      "evaluation/text/loss: 4.8477253913879395\n",
      "evaluation/text/perplexity: 127.45015716552734\n",
      "time/total: 1204.405720949173\n",
      "time/evaluation: 0.4320840835571289\n",
      "training/train_loss_mean: 4.97161283493042\n",
      "training/train_loss_std: 0.34384851586876086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 710\n",
      "training/learning_rate: 2.7205300844827753e-05\n",
      "time/sample_batch: 0.010170936584472656\n",
      "time/training: 1.2240288257598877\n",
      "evaluation/text/loss: 4.7996134757995605\n",
      "evaluation/text/perplexity: 121.46346282958984\n",
      "time/total: 1206.3755116462708\n",
      "time/evaluation: 0.7441689968109131\n",
      "training/train_loss_mean: 5.301991558074951\n",
      "training/train_loss_std: 0.3135917705807667\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 711\n",
      "training/learning_rate: 2.709313685889932e-05\n",
      "time/sample_batch: 0.008884906768798828\n",
      "time/training: 1.3199653625488281\n",
      "evaluation/text/loss: 4.926677703857422\n",
      "evaluation/text/perplexity: 137.9205322265625\n",
      "time/total: 1208.436551809311\n",
      "time/evaluation: 0.7393515110015869\n",
      "training/train_loss_mean: 5.225650453567505\n",
      "training/train_loss_std: 0.35891362868736865\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 712\n",
      "training/learning_rate: 2.6981253894772917e-05\n",
      "time/sample_batch: 0.008937835693359375\n",
      "time/training: 1.0109989643096924\n",
      "evaluation/text/loss: 4.867224216461182\n",
      "evaluation/text/perplexity: 129.9596710205078\n",
      "time/total: 1210.2292296886444\n",
      "time/evaluation: 0.779930830001831\n",
      "training/train_loss_mean: 5.070495176315307\n",
      "training/train_loss_std: 0.5274995607923055\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 713\n",
      "training/learning_rate: 2.6869653079108713e-05\n",
      "time/sample_batch: 0.006897926330566406\n",
      "time/training: 1.0517866611480713\n",
      "evaluation/text/loss: 4.676571846008301\n",
      "evaluation/text/perplexity: 107.40125274658203\n",
      "time/total: 1211.9422509670258\n",
      "time/evaluation: 0.659614086151123\n",
      "training/train_loss_mean: 4.958604192733764\n",
      "training/train_loss_std: 0.30824686514931077\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 714\n",
      "training/learning_rate: 2.6758335535725703e-05\n",
      "time/sample_batch: 0.008240699768066406\n",
      "time/training: 0.9848864078521729\n",
      "evaluation/text/loss: 4.733302116394043\n",
      "evaluation/text/perplexity: 113.67029571533203\n",
      "time/total: 1213.514065504074\n",
      "time/evaluation: 0.5854320526123047\n",
      "training/train_loss_mean: 5.014609026908874\n",
      "training/train_loss_std: 0.48142248189223175\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 715\n",
      "training/learning_rate: 2.6647302385590306e-05\n",
      "time/sample_batch: 0.006877899169921875\n",
      "time/training: 0.9748857021331787\n",
      "evaluation/text/loss: 4.8123040199279785\n",
      "evaluation/text/perplexity: 123.01471710205078\n",
      "time/total: 1215.2493867874146\n",
      "time/evaluation: 0.7588944435119629\n",
      "training/train_loss_mean: 5.0855306625366214\n",
      "training/train_loss_std: 0.4499052429404549\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 716\n",
      "training/learning_rate: 2.653655474680512e-05\n",
      "time/sample_batch: 0.0065479278564453125\n",
      "time/training: 1.0390655994415283\n",
      "evaluation/text/loss: 4.849956512451172\n",
      "evaluation/text/perplexity: 127.73483276367188\n",
      "time/total: 1216.9562678337097\n",
      "time/evaluation: 0.6659858226776123\n",
      "training/train_loss_mean: 5.155458068847656\n",
      "training/train_loss_std: 0.45018976755679024\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 717\n",
      "training/learning_rate: 2.6426093734597613e-05\n",
      "time/sample_batch: 0.009315729141235352\n",
      "time/training: 1.0387940406799316\n",
      "evaluation/text/loss: 4.900424957275391\n",
      "evaluation/text/perplexity: 134.34686279296875\n",
      "time/total: 1218.680356502533\n",
      "time/evaluation: 0.6836247444152832\n",
      "training/train_loss_mean: 4.980214476585388\n",
      "training/train_loss_std: 0.4221366457157626\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 718\n",
      "training/learning_rate: 2.6315920461308967e-05\n",
      "time/sample_batch: 0.009096384048461914\n",
      "time/training: 1.1434688568115234\n",
      "evaluation/text/loss: 4.801830768585205\n",
      "evaluation/text/perplexity: 121.73307800292969\n",
      "time/total: 1220.2063300609589\n",
      "time/evaluation: 0.38093066215515137\n",
      "training/train_loss_mean: 5.068184661865234\n",
      "training/train_loss_std: 0.27138729450136634\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 719\n",
      "training/learning_rate: 2.620603603638284e-05\n",
      "time/sample_batch: 0.007249116897583008\n",
      "time/training: 0.8840491771697998\n",
      "evaluation/text/loss: 4.880023002624512\n",
      "evaluation/text/perplexity: 131.63369750976562\n",
      "time/total: 1221.589361667633\n",
      "time/evaluation: 0.4974484443664551\n",
      "training/train_loss_mean: 5.207205438613892\n",
      "training/train_loss_std: 0.1808143698541276\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 720\n",
      "training/learning_rate: 2.6096441566354142e-05\n",
      "time/sample_batch: 0.00685572624206543\n",
      "time/training: 0.881479024887085\n",
      "evaluation/text/loss: 4.794811248779297\n",
      "evaluation/text/perplexity: 120.88156127929688\n",
      "time/total: 1223.0472750663757\n",
      "time/evaluation: 0.5741608142852783\n",
      "training/train_loss_mean: 5.112707185745239\n",
      "training/train_loss_std: 0.2386339125706947\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 721\n",
      "training/learning_rate: 2.598713815483798e-05\n",
      "time/sample_batch: 0.009084701538085938\n",
      "time/training: 0.9826617240905762\n",
      "evaluation/text/loss: 4.839222431182861\n",
      "evaluation/text/perplexity: 126.37104797363281\n",
      "time/total: 1224.5826361179352\n",
      "time/evaluation: 0.5510878562927246\n",
      "training/train_loss_mean: 4.886189603805542\n",
      "training/train_loss_std: 0.37306766110131706\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 722\n",
      "training/learning_rate: 2.5878126902518508e-05\n",
      "time/sample_batch: 0.007969856262207031\n",
      "time/training: 1.0544590950012207\n",
      "evaluation/text/loss: 4.7496562004089355\n",
      "evaluation/text/perplexity: 115.5445556640625\n",
      "time/total: 1226.1475150585175\n",
      "time/evaluation: 0.5086829662322998\n",
      "training/train_loss_mean: 5.00324821472168\n",
      "training/train_loss_std: 0.2937011186360137\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 723\n",
      "training/learning_rate: 2.5769408907137805e-05\n",
      "time/sample_batch: 0.00838780403137207\n",
      "time/training: 0.9936981201171875\n",
      "evaluation/text/loss: 4.709296226501465\n",
      "evaluation/text/perplexity: 110.97402954101562\n",
      "time/total: 1227.8529119491577\n",
      "time/evaluation: 0.7101821899414062\n",
      "training/train_loss_mean: 5.001714420318604\n",
      "training/train_loss_std: 0.3663034017771217\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 724\n",
      "training/learning_rate: 2.566098526348488e-05\n",
      "time/sample_batch: 0.008552789688110352\n",
      "time/training: 0.8255119323730469\n",
      "evaluation/text/loss: 4.731156826019287\n",
      "evaluation/text/perplexity: 113.42670440673828\n",
      "time/total: 1229.2162246704102\n",
      "time/evaluation: 0.5362014770507812\n",
      "training/train_loss_mean: 5.06129822731018\n",
      "training/train_loss_std: 0.4064062263349675\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 725\n",
      "training/learning_rate: 2.555285706338465e-05\n",
      "time/sample_batch: 0.008712530136108398\n",
      "time/training: 1.0428309440612793\n",
      "evaluation/text/loss: 4.686675071716309\n",
      "evaluation/text/perplexity: 108.49185180664062\n",
      "time/total: 1230.8425507545471\n",
      "time/evaluation: 0.5817749500274658\n",
      "training/train_loss_mean: 5.078994655609131\n",
      "training/train_loss_std: 0.29938259783035603\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 726\n",
      "training/learning_rate: 2.5445025395686843e-05\n",
      "time/sample_batch: 0.009556770324707031\n",
      "time/training: 1.0173931121826172\n",
      "evaluation/text/loss: 4.804546356201172\n",
      "evaluation/text/perplexity: 122.06410217285156\n",
      "time/total: 1232.460297346115\n",
      "time/evaluation: 0.5987224578857422\n",
      "training/train_loss_mean: 5.137233114242553\n",
      "training/train_loss_std: 0.27198225797445796\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 727\n",
      "training/learning_rate: 2.533749134625518e-05\n",
      "time/sample_batch: 0.009765863418579102\n",
      "time/training: 1.1144120693206787\n",
      "evaluation/text/loss: 4.8518900871276855\n",
      "evaluation/text/perplexity: 127.9820556640625\n",
      "time/total: 1234.0811455249786\n",
      "time/evaluation: 0.5048184394836426\n",
      "training/train_loss_mean: 5.0176702499389645\n",
      "training/train_loss_std: 0.2534851990087452\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 728\n",
      "training/learning_rate: 2.523025599795637e-05\n",
      "time/sample_batch: 0.008234977722167969\n",
      "time/training: 0.8007900714874268\n",
      "evaluation/text/loss: 4.834584712982178\n",
      "evaluation/text/perplexity: 125.78633117675781\n",
      "time/total: 1235.6879918575287\n",
      "time/evaluation: 0.8044142723083496\n",
      "training/train_loss_mean: 4.716830492019653\n",
      "training/train_loss_std: 0.8557767396680017\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 729\n",
      "training/learning_rate: 2.5123320430649127e-05\n",
      "time/sample_batch: 0.00847172737121582\n",
      "time/training: 1.1655311584472656\n",
      "evaluation/text/loss: 4.887063026428223\n",
      "evaluation/text/perplexity: 132.56365966796875\n",
      "time/total: 1237.6348142623901\n",
      "time/evaluation: 0.7796511650085449\n",
      "training/train_loss_mean: 5.20177435874939\n",
      "training/train_loss_std: 0.2558056740620804\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 730\n",
      "training/learning_rate: 2.5016685721173466e-05\n",
      "time/sample_batch: 0.008752822875976562\n",
      "time/training: 1.0780417919158936\n",
      "evaluation/text/loss: 4.900699615478516\n",
      "evaluation/text/perplexity: 134.38375854492188\n",
      "time/total: 1239.3799211978912\n",
      "time/evaluation: 0.6653282642364502\n",
      "training/train_loss_mean: 4.894130253791809\n",
      "training/train_loss_std: 0.6346028076630899\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 731\n",
      "training/learning_rate: 2.4910352943339723e-05\n",
      "time/sample_batch: 0.00836944580078125\n",
      "time/training: 1.087554693222046\n",
      "evaluation/text/loss: 4.8322858810424805\n",
      "evaluation/text/perplexity: 125.49750518798828\n",
      "time/total: 1241.4510297775269\n",
      "time/evaluation: 0.9819064140319824\n",
      "training/train_loss_mean: 5.298242855072021\n",
      "training/train_loss_std: 0.2563023470862817\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 732\n",
      "training/learning_rate: 2.4804323167917785e-05\n",
      "time/sample_batch: 0.006977558135986328\n",
      "time/training: 1.04659104347229\n",
      "evaluation/text/loss: 4.993760585784912\n",
      "evaluation/text/perplexity: 147.4900360107422\n",
      "time/total: 1243.4932124614716\n",
      "time/evaluation: 0.9938910007476807\n",
      "training/train_loss_mean: 5.205353116989135\n",
      "training/train_loss_std: 0.2666538794319798\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 733\n",
      "training/learning_rate: 2.4698597462626295e-05\n",
      "time/sample_batch: 0.007222414016723633\n",
      "time/training: 1.0929491519927979\n",
      "evaluation/text/loss: 4.691384792327881\n",
      "evaluation/text/perplexity: 109.00402069091797\n",
      "time/total: 1245.0306124687195\n",
      "time/evaluation: 0.43791651725769043\n",
      "training/train_loss_mean: 4.987269258499145\n",
      "training/train_loss_std: 0.32399228091835697\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 734\n",
      "training/learning_rate: 2.459317689212195e-05\n",
      "time/sample_batch: 0.009224414825439453\n",
      "time/training: 1.1850309371948242\n",
      "evaluation/text/loss: 4.853134632110596\n",
      "evaluation/text/perplexity: 128.1414337158203\n",
      "time/total: 1246.9586064815521\n",
      "time/evaluation: 0.7413408756256104\n",
      "training/train_loss_mean: 5.047799777984619\n",
      "training/train_loss_std: 0.40960113126681064\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 735\n",
      "training/learning_rate: 2.4488062517988738e-05\n",
      "time/sample_batch: 0.008671283721923828\n",
      "time/training: 1.0687811374664307\n",
      "evaluation/text/loss: 4.799995422363281\n",
      "evaluation/text/perplexity: 121.5098648071289\n",
      "time/total: 1248.8426706790924\n",
      "time/evaluation: 0.8136501312255859\n",
      "training/train_loss_mean: 5.106455087661743\n",
      "training/train_loss_std: 0.32927967182748774\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 736\n",
      "training/learning_rate: 2.4383255398727205e-05\n",
      "time/sample_batch: 0.008993148803710938\n",
      "time/training: 1.0523245334625244\n",
      "evaluation/text/loss: 4.660090446472168\n",
      "evaluation/text/perplexity: 105.64563751220703\n",
      "time/total: 1250.356024980545\n",
      "time/evaluation: 0.45926976203918457\n",
      "training/train_loss_mean: 5.0321207523345945\n",
      "training/train_loss_std: 0.5783531574793743\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 737\n",
      "training/learning_rate: 2.42787565897439e-05\n",
      "time/sample_batch: 0.007697105407714844\n",
      "time/training: 0.9673781394958496\n",
      "evaluation/text/loss: 4.877208709716797\n",
      "evaluation/text/perplexity: 131.26376342773438\n",
      "time/total: 1251.8464269638062\n",
      "time/evaluation: 0.5212602615356445\n",
      "training/train_loss_mean: 5.063472652435303\n",
      "training/train_loss_std: 0.29582391157988897\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 738\n",
      "training/learning_rate: 2.417456714334068e-05\n",
      "time/sample_batch: 0.00819540023803711\n",
      "time/training: 0.8803806304931641\n",
      "evaluation/text/loss: 4.933219909667969\n",
      "evaluation/text/perplexity: 138.8258056640625\n",
      "time/total: 1253.475338459015\n",
      "time/evaluation: 0.7468712329864502\n",
      "training/train_loss_mean: 4.835358285903931\n",
      "training/train_loss_std: 0.8944151763178322\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 739\n",
      "training/learning_rate: 2.407068810870409e-05\n",
      "time/sample_batch: 0.009075403213500977\n",
      "time/training: 1.0365710258483887\n",
      "evaluation/text/loss: 4.882274150848389\n",
      "evaluation/text/perplexity: 131.93035888671875\n",
      "time/total: 1255.114947795868\n",
      "time/evaluation: 0.6013643741607666\n",
      "training/train_loss_mean: 5.185259389877319\n",
      "training/train_loss_std: 0.23289032407700078\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 740\n",
      "training/learning_rate: 2.396712053189486e-05\n",
      "time/sample_batch: 0.010747194290161133\n",
      "time/training: 1.2490136623382568\n",
      "evaluation/text/loss: 4.760087013244629\n",
      "evaluation/text/perplexity: 116.75608825683594\n",
      "time/total: 1257.1234259605408\n",
      "time/evaluation: 0.7578020095825195\n",
      "training/train_loss_mean: 4.979651737213135\n",
      "training/train_loss_std: 0.6568866400330289\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 741\n",
      "training/learning_rate: 2.3863865455837358e-05\n",
      "time/sample_batch: 0.00918126106262207\n",
      "time/training: 1.07914137840271\n",
      "evaluation/text/loss: 4.854214668273926\n",
      "evaluation/text/perplexity: 128.2799072265625\n",
      "time/total: 1258.762959241867\n",
      "time/evaluation: 0.5587038993835449\n",
      "training/train_loss_mean: 5.153930711746216\n",
      "training/train_loss_std: 0.3370943063876736\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 742\n",
      "training/learning_rate: 2.376092392030903e-05\n",
      "time/sample_batch: 0.008438825607299805\n",
      "time/training: 1.0617303848266602\n",
      "evaluation/text/loss: 4.756441593170166\n",
      "evaluation/text/perplexity: 116.33123779296875\n",
      "time/total: 1260.4829325675964\n",
      "time/evaluation: 0.6565186977386475\n",
      "training/train_loss_mean: 5.068303823471069\n",
      "training/train_loss_std: 0.34568628306494714\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 743\n",
      "training/learning_rate: 2.3658296961930005e-05\n",
      "time/sample_batch: 0.008901596069335938\n",
      "time/training: 0.9452488422393799\n",
      "evaluation/text/loss: 4.821300506591797\n",
      "evaluation/text/perplexity: 124.12641143798828\n",
      "time/total: 1262.0185911655426\n",
      "time/evaluation: 0.5887196063995361\n",
      "training/train_loss_mean: 5.158611154556274\n",
      "training/train_loss_std: 0.42281575787788866\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 744\n",
      "training/learning_rate: 2.3555985614152628e-05\n",
      "time/sample_batch: 0.00903940200805664\n",
      "time/training: 1.003401279449463\n",
      "evaluation/text/loss: 4.830968379974365\n",
      "evaluation/text/perplexity: 125.33226776123047\n",
      "time/total: 1263.567118883133\n",
      "time/evaluation: 0.5433812141418457\n",
      "training/train_loss_mean: 5.161146211624145\n",
      "training/train_loss_std: 0.14652267562522625\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 745\n",
      "training/learning_rate: 2.345399090725101e-05\n",
      "time/sample_batch: 0.010804414749145508\n",
      "time/training: 1.1597850322723389\n",
      "evaluation/text/loss: 4.80869722366333\n",
      "evaluation/text/perplexity: 122.57183074951172\n",
      "time/total: 1265.719284772873\n",
      "time/evaluation: 0.9907147884368896\n",
      "training/train_loss_mean: 5.091008138656616\n",
      "training/train_loss_std: 0.26361965019735595\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 746\n",
      "training/learning_rate: 2.3352313868310725e-05\n",
      "time/sample_batch: 0.008235692977905273\n",
      "time/training: 0.7917680740356445\n",
      "evaluation/text/loss: 4.741815567016602\n",
      "evaluation/text/perplexity: 114.64215087890625\n",
      "time/total: 1267.257801771164\n",
      "time/evaluation: 0.7450830936431885\n",
      "training/train_loss_mean: 4.975323414802551\n",
      "training/train_loss_std: 0.5508868870103989\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 747\n",
      "training/learning_rate: 2.3250955521218446e-05\n",
      "time/sample_batch: 0.007401704788208008\n",
      "time/training: 1.1084635257720947\n",
      "evaluation/text/loss: 4.787858963012695\n",
      "evaluation/text/perplexity: 120.04407501220703\n",
      "time/total: 1268.9468448162079\n",
      "time/evaluation: 0.5788567066192627\n",
      "training/train_loss_mean: 5.286869192123413\n",
      "training/train_loss_std: 0.20483864249318615\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 748\n",
      "training/learning_rate: 2.3149916886651596e-05\n",
      "time/sample_batch: 0.007344722747802734\n",
      "time/training: 0.9973313808441162\n",
      "evaluation/text/loss: 4.7283477783203125\n",
      "evaluation/text/perplexity: 113.10852813720703\n",
      "time/total: 1270.5589168071747\n",
      "time/evaluation: 0.6129171848297119\n",
      "training/train_loss_mean: 5.057523036003113\n",
      "training/train_loss_std: 0.5377986618342347\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 749\n",
      "training/learning_rate: 2.3049198982068075e-05\n",
      "time/sample_batch: 0.008003473281860352\n",
      "time/training: 0.9429948329925537\n",
      "evaluation/text/loss: 4.912259101867676\n",
      "evaluation/text/perplexity: 135.94618225097656\n",
      "time/total: 1272.3236491680145\n",
      "time/evaluation: 0.8199417591094971\n",
      "training/train_loss_mean: 4.974157905578613\n",
      "training/train_loss_std: 0.4372284695208986\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 750\n",
      "training/learning_rate: 2.2948802821696083e-05\n",
      "time/sample_batch: 0.009129047393798828\n",
      "time/training: 1.1271238327026367\n",
      "evaluation/text/loss: 4.905249118804932\n",
      "evaluation/text/perplexity: 134.9965362548828\n",
      "time/total: 1273.9965472221375\n",
      "time/evaluation: 0.5441253185272217\n",
      "training/train_loss_mean: 5.223771572113037\n",
      "training/train_loss_std: 0.37083880147604903\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 751\n",
      "training/learning_rate: 2.284872941652386e-05\n",
      "time/sample_batch: 0.008933067321777344\n",
      "time/training: 1.154329776763916\n",
      "evaluation/text/loss: 4.7635369300842285\n",
      "evaluation/text/perplexity: 117.15957641601562\n",
      "time/total: 1275.9720511436462\n",
      "time/evaluation: 0.8195364475250244\n",
      "training/train_loss_mean: 5.187360858917236\n",
      "training/train_loss_std: 0.26080408175349007\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 752\n",
      "training/learning_rate: 2.274897977428945e-05\n",
      "time/sample_batch: 0.008142948150634766\n",
      "time/training: 1.1737735271453857\n",
      "evaluation/text/loss: 4.77041482925415\n",
      "evaluation/text/perplexity: 117.96817016601562\n",
      "time/total: 1277.7552132606506\n",
      "time/evaluation: 0.6077272891998291\n",
      "training/train_loss_mean: 5.026544570922852\n",
      "training/train_loss_std: 0.3210990547011758\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 753\n",
      "training/learning_rate: 2.2649554899470658e-05\n",
      "time/sample_batch: 0.009803533554077148\n",
      "time/training: 1.129354476928711\n",
      "evaluation/text/loss: 4.665060997009277\n",
      "evaluation/text/perplexity: 106.17205810546875\n",
      "time/total: 1279.371330499649\n",
      "time/evaluation: 0.48505425453186035\n",
      "training/train_loss_mean: 5.027256989479065\n",
      "training/train_loss_std: 0.6407466951128868\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 754\n",
      "training/learning_rate: 2.2550455793274887e-05\n",
      "time/sample_batch: 0.007323741912841797\n",
      "time/training: 1.227419376373291\n",
      "evaluation/text/loss: 4.658677101135254\n",
      "evaluation/text/perplexity: 105.49642944335938\n",
      "time/total: 1281.3533322811127\n",
      "time/evaluation: 0.752910852432251\n",
      "training/train_loss_mean: 5.1514708518981935\n",
      "training/train_loss_std: 0.3403890413136413\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 755\n",
      "training/learning_rate: 2.2451683453629007e-05\n",
      "time/sample_batch: 0.008856534957885742\n",
      "time/training: 1.080801010131836\n",
      "evaluation/text/loss: 4.659343719482422\n",
      "evaluation/text/perplexity: 105.56678009033203\n",
      "time/total: 1282.8662333488464\n",
      "time/evaluation: 0.4303903579711914\n",
      "training/train_loss_mean: 5.157304286956787\n",
      "training/train_loss_std: 0.250038398437093\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 756\n",
      "training/learning_rate: 2.2353238875169395e-05\n",
      "time/sample_batch: 0.007270097732543945\n",
      "time/training: 1.0226304531097412\n",
      "evaluation/text/loss: 4.947535514831543\n",
      "evaluation/text/perplexity: 140.8274688720703\n",
      "time/total: 1284.3734369277954\n",
      "time/evaluation: 0.48278260231018066\n",
      "training/train_loss_mean: 5.209267425537109\n",
      "training/train_loss_std: 0.35508473463199913\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 757\n",
      "training/learning_rate: 2.2255123049231875e-05\n",
      "time/sample_batch: 0.011332273483276367\n",
      "time/training: 1.1100947856903076\n",
      "evaluation/text/loss: 4.8124895095825195\n",
      "evaluation/text/perplexity: 123.03753662109375\n",
      "time/total: 1286.2124321460724\n",
      "time/evaluation: 0.7272109985351562\n",
      "training/train_loss_mean: 5.185959386825561\n",
      "training/train_loss_std: 0.3150910110988693\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 758\n",
      "training/learning_rate: 2.2157336963841698e-05\n",
      "time/sample_batch: 0.008572578430175781\n",
      "time/training: 0.918959379196167\n",
      "evaluation/text/loss: 4.770872116088867\n",
      "evaluation/text/perplexity: 118.02212524414062\n",
      "time/total: 1287.690349817276\n",
      "time/evaluation: 0.5572996139526367\n",
      "training/train_loss_mean: 5.161009883880615\n",
      "training/train_loss_std: 0.6193188039820763\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 759\n",
      "training/learning_rate: 2.2059881603703675e-05\n",
      "time/sample_batch: 0.009428262710571289\n",
      "time/training: 1.0579676628112793\n",
      "evaluation/text/loss: 4.5829033851623535\n",
      "evaluation/text/perplexity: 97.79792785644531\n",
      "time/total: 1289.2308089733124\n",
      "time/evaluation: 0.48066139221191406\n",
      "training/train_loss_mean: 5.156692886352539\n",
      "training/train_loss_std: 0.24808150932518344\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 760\n",
      "training/learning_rate: 2.1962757950192214e-05\n",
      "time/sample_batch: 0.007719278335571289\n",
      "time/training: 0.9632933139801025\n",
      "evaluation/text/loss: 4.750772476196289\n",
      "evaluation/text/perplexity: 115.6736068725586\n",
      "time/total: 1290.9372990131378\n",
      "time/evaluation: 0.7413928508758545\n",
      "training/train_loss_mean: 4.957396841049194\n",
      "training/train_loss_std: 0.26784947859626584\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 761\n",
      "training/learning_rate: 2.1865966981341396e-05\n",
      "time/sample_batch: 0.008396148681640625\n",
      "time/training: 0.9019310474395752\n",
      "evaluation/text/loss: 4.743127346038818\n",
      "evaluation/text/perplexity: 114.79264068603516\n",
      "time/total: 1292.4418017864227\n",
      "time/evaluation: 0.6008744239807129\n",
      "training/train_loss_mean: 5.182790374755859\n",
      "training/train_loss_std: 0.4039181571291701\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 762\n",
      "training/learning_rate: 2.176950967183522e-05\n",
      "time/sample_batch: 0.0081024169921875\n",
      "time/training: 0.9907519817352295\n",
      "evaluation/text/loss: 4.855296611785889\n",
      "evaluation/text/perplexity: 128.4187774658203\n",
      "time/total: 1293.9013690948486\n",
      "time/evaluation: 0.4672093391418457\n",
      "training/train_loss_mean: 4.824361896514892\n",
      "training/train_loss_std: 0.476865224675727\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 763\n",
      "training/learning_rate: 2.1673386992997737e-05\n",
      "time/sample_batch: 0.00810098648071289\n",
      "time/training: 1.088667869567871\n",
      "evaluation/text/loss: 4.848426818847656\n",
      "evaluation/text/perplexity: 127.53958892822266\n",
      "time/total: 1295.6898272037506\n",
      "time/evaluation: 0.6979472637176514\n",
      "training/train_loss_mean: 5.166621160507202\n",
      "training/train_loss_std: 0.27424822950628475\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 764\n",
      "training/learning_rate: 2.1577599912783228e-05\n",
      "time/sample_batch: 0.007922649383544922\n",
      "time/training: 1.2379274368286133\n",
      "evaluation/text/loss: 4.724646091461182\n",
      "evaluation/text/perplexity: 112.69061279296875\n",
      "time/total: 1297.4220142364502\n",
      "time/evaluation: 0.4926333427429199\n",
      "training/train_loss_mean: 5.1798608779907225\n",
      "training/train_loss_std: 0.16796137270533798\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 765\n",
      "training/learning_rate: 2.1482149395766546e-05\n",
      "time/sample_batch: 0.007838726043701172\n",
      "time/training: 1.0242905616760254\n",
      "evaluation/text/loss: 4.817863941192627\n",
      "evaluation/text/perplexity: 123.70057678222656\n",
      "time/total: 1298.9588649272919\n",
      "time/evaluation: 0.5108630657196045\n",
      "training/train_loss_mean: 5.0700928449630736\n",
      "training/train_loss_std: 0.6452022655989407\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 766\n",
      "training/learning_rate: 2.1387036403133323e-05\n",
      "time/sample_batch: 0.008519887924194336\n",
      "time/training: 0.8856866359710693\n",
      "evaluation/text/loss: 4.8330206871032715\n",
      "evaluation/text/perplexity: 125.58975219726562\n",
      "time/total: 1300.6649758815765\n",
      "time/evaluation: 0.8186140060424805\n",
      "training/train_loss_mean: 5.193891382217407\n",
      "training/train_loss_std: 0.40548687314625603\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 767\n",
      "training/learning_rate: 2.1292261892670357e-05\n",
      "time/sample_batch: 0.007918596267700195\n",
      "time/training: 1.20871901512146\n",
      "evaluation/text/loss: 4.756778240203857\n",
      "evaluation/text/perplexity: 116.37040710449219\n",
      "time/total: 1302.576889514923\n",
      "time/evaluation: 0.7014892101287842\n",
      "training/train_loss_mean: 5.25909538269043\n",
      "training/train_loss_std: 0.1470318598709758\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 768\n",
      "training/learning_rate: 2.119782681875589e-05\n",
      "time/sample_batch: 0.008428812026977539\n",
      "time/training: 1.057640552520752\n",
      "evaluation/text/loss: 4.739852428436279\n",
      "evaluation/text/perplexity: 114.41731262207031\n",
      "time/total: 1304.2319416999817\n",
      "time/evaluation: 0.5957164764404297\n",
      "training/train_loss_mean: 4.984465789794922\n",
      "training/train_loss_std: 0.24072698515946747\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 769\n",
      "training/learning_rate: 2.1103732132350087e-05\n",
      "time/sample_batch: 0.008905887603759766\n",
      "time/training: 1.0400428771972656\n",
      "evaluation/text/loss: 4.73976469039917\n",
      "evaluation/text/perplexity: 114.40727996826172\n",
      "time/total: 1305.8356964588165\n",
      "time/evaluation: 0.5620460510253906\n",
      "training/train_loss_mean: 5.201482725143433\n",
      "training/train_loss_std: 0.24898749173983362\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 770\n",
      "training/learning_rate: 2.1009978780985408e-05\n",
      "time/sample_batch: 0.01126408576965332\n",
      "time/training: 1.2808642387390137\n",
      "evaluation/text/loss: 4.888434410095215\n",
      "evaluation/text/perplexity: 132.74559020996094\n",
      "time/total: 1307.7042729854584\n",
      "time/evaluation: 0.5860497951507568\n",
      "training/train_loss_mean: 5.179392290115357\n",
      "training/train_loss_std: 0.08417109954624807\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 771\n",
      "training/learning_rate: 2.0916567708757026e-05\n",
      "time/sample_batch: 0.009409904479980469\n",
      "time/training: 1.1278259754180908\n",
      "evaluation/text/loss: 4.699814319610596\n",
      "evaluation/text/perplexity: 109.9267578125\n",
      "time/total: 1309.3561260700226\n",
      "time/evaluation: 0.5223369598388672\n",
      "training/train_loss_mean: 4.925381517410278\n",
      "training/train_loss_std: 0.3653698857080601\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 772\n",
      "training/learning_rate: 2.0823499856313422e-05\n",
      "time/sample_batch: 0.00803232192993164\n",
      "time/training: 0.9655661582946777\n",
      "evaluation/text/loss: 4.842522621154785\n",
      "evaluation/text/perplexity: 126.78878784179688\n",
      "time/total: 1310.8562848567963\n",
      "time/evaluation: 0.5284936428070068\n",
      "training/train_loss_mean: 5.149106073379516\n",
      "training/train_loss_std: 0.2249597148238336\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 773\n",
      "training/learning_rate: 2.073077616084685e-05\n",
      "time/sample_batch: 0.008260488510131836\n",
      "time/training: 0.7579216957092285\n",
      "evaluation/text/loss: 4.782817840576172\n",
      "evaluation/text/perplexity: 119.44043731689453\n",
      "time/total: 1312.3000345230103\n",
      "time/evaluation: 0.6841812133789062\n",
      "training/train_loss_mean: 4.896527910232544\n",
      "training/train_loss_std: 0.566652862518132\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 774\n",
      "training/learning_rate: 2.0638397556083863e-05\n",
      "time/sample_batch: 0.008773565292358398\n",
      "time/training: 1.122575283050537\n",
      "evaluation/text/loss: 5.02461576461792\n",
      "evaluation/text/perplexity: 152.11180114746094\n",
      "time/total: 1314.0196588039398\n",
      "time/evaluation: 0.5954279899597168\n",
      "training/train_loss_mean: 5.066011619567871\n",
      "training/train_loss_std: 0.36938017291189595\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 775\n",
      "training/learning_rate: 2.0546364972276003e-05\n",
      "time/sample_batch: 0.00864863395690918\n",
      "time/training: 1.1602399349212646\n",
      "evaluation/text/loss: 4.933862209320068\n",
      "evaluation/text/perplexity: 138.9149932861328\n",
      "time/total: 1315.8592834472656\n",
      "time/evaluation: 0.6776573657989502\n",
      "training/train_loss_mean: 5.237719249725342\n",
      "training/train_loss_std: 0.3339885223654819\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 776\n",
      "training/learning_rate: 2.045467933619038e-05\n",
      "time/sample_batch: 0.008600234985351562\n",
      "time/training: 1.1181690692901611\n",
      "evaluation/text/loss: 4.549428462982178\n",
      "evaluation/text/perplexity: 94.57833862304688\n",
      "time/total: 1317.4126477241516\n",
      "time/evaluation: 0.4334275722503662\n",
      "training/train_loss_mean: 5.317563724517822\n",
      "training/train_loss_std: 0.2271812341738957\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 777\n",
      "training/learning_rate: 2.0363341571100297e-05\n",
      "time/sample_batch: 0.008440256118774414\n",
      "time/training: 1.0653951168060303\n",
      "evaluation/text/loss: 4.911962985992432\n",
      "evaluation/text/perplexity: 135.9059295654297\n",
      "time/total: 1318.927096605301\n",
      "time/evaluation: 0.4474332332611084\n",
      "training/train_loss_mean: 4.903880190849304\n",
      "training/train_loss_std: 0.9583501799043304\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 778\n",
      "training/learning_rate: 2.027235259677606e-05\n",
      "time/sample_batch: 0.00848078727722168\n",
      "time/training: 1.0060195922851562\n",
      "evaluation/text/loss: 4.79932975769043\n",
      "evaluation/text/perplexity: 121.42900085449219\n",
      "time/total: 1320.4593884944916\n",
      "time/evaluation: 0.5245182514190674\n",
      "training/train_loss_mean: 5.175635766983032\n",
      "training/train_loss_std: 0.3474197829094826\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 779\n",
      "training/learning_rate: 2.018171332947563e-05\n",
      "time/sample_batch: 0.008077144622802734\n",
      "time/training: 1.0101442337036133\n",
      "evaluation/text/loss: 4.839964389801025\n",
      "evaluation/text/perplexity: 126.46485137939453\n",
      "time/total: 1322.0827651023865\n",
      "time/evaluation: 0.6115288734436035\n",
      "training/train_loss_mean: 5.0593561172485355\n",
      "training/train_loss_std: 0.308836548954868\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 780\n",
      "training/learning_rate: 2.010043774286015e-05\n",
      "time/sample_batch: 0.008543252944946289\n",
      "time/training: 0.8907425403594971\n",
      "evaluation/text/loss: 4.745747089385986\n",
      "evaluation/text/perplexity: 115.09375762939453\n",
      "time/total: 1323.7307603359222\n",
      "time/evaluation: 0.7555704116821289\n",
      "training/train_loss_mean: 4.874924447801378\n",
      "training/train_loss_std: 0.9424854400215722\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 781\n",
      "training/learning_rate: 2.0010465430575807e-05\n",
      "time/sample_batch: 0.008603811264038086\n",
      "time/training: 1.0903995037078857\n",
      "evaluation/text/loss: 4.921089172363281\n",
      "evaluation/text/perplexity: 137.15191650390625\n",
      "time/total: 1325.5393595695496\n",
      "time/evaluation: 0.7165048122406006\n",
      "training/train_loss_mean: 5.1719153881072994\n",
      "training/train_loss_std: 0.3961142416467562\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 782\n",
      "training/learning_rate: 1.992084546251611e-05\n",
      "time/sample_batch: 0.006111860275268555\n",
      "time/training: 1.0118424892425537\n",
      "evaluation/text/loss: 4.800100803375244\n",
      "evaluation/text/perplexity: 121.52266693115234\n",
      "time/total: 1327.1604413986206\n",
      "time/evaluation: 0.6074509620666504\n",
      "training/train_loss_mean: 4.802996516227722\n",
      "training/train_loss_std: 0.7117772735267308\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 783\n",
      "training/learning_rate: 1.9831578741153153e-05\n",
      "time/sample_batch: 0.010080814361572266\n",
      "time/training: 1.0974931716918945\n",
      "evaluation/text/loss: 4.981405735015869\n",
      "evaluation/text/perplexity: 145.6790313720703\n",
      "time/total: 1328.9295480251312\n",
      "time/evaluation: 0.66986083984375\n",
      "training/train_loss_mean: 4.979932498931885\n",
      "training/train_loss_std: 0.38598635152848176\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 784\n",
      "training/learning_rate: 1.974266616540184e-05\n",
      "time/sample_batch: 0.007687807083129883\n",
      "time/training: 1.0384361743927002\n",
      "evaluation/text/loss: 4.784310817718506\n",
      "evaluation/text/perplexity: 119.618896484375\n",
      "time/total: 1330.5667612552643\n",
      "time/evaluation: 0.5969741344451904\n",
      "training/train_loss_mean: 4.915695977210999\n",
      "training/train_loss_std: 0.7438713084379697\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 785\n",
      "training/learning_rate: 1.9654108630610805e-05\n",
      "time/sample_batch: 0.007569789886474609\n",
      "time/training: 0.9154415130615234\n",
      "evaluation/text/loss: 4.782013893127441\n",
      "evaluation/text/perplexity: 119.34445190429688\n",
      "time/total: 1332.0249047279358\n",
      "time/evaluation: 0.5409529209136963\n",
      "training/train_loss_mean: 5.056538009643555\n",
      "training/train_loss_std: 0.20165568731628533\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 786\n",
      "training/learning_rate: 1.956590702855344e-05\n",
      "time/sample_batch: 0.008750438690185547\n",
      "time/training: 0.995535135269165\n",
      "evaluation/text/loss: 4.858339309692383\n",
      "evaluation/text/perplexity: 128.8101043701172\n",
      "time/total: 1333.6681611537933\n",
      "time/evaluation: 0.6461021900177002\n",
      "training/train_loss_mean: 5.123051357269287\n",
      "training/train_loss_std: 0.3057543577455813\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 787\n",
      "training/learning_rate: 1.9478062247418922e-05\n",
      "time/sample_batch: 0.007813215255737305\n",
      "time/training: 0.9417726993560791\n",
      "evaluation/text/loss: 4.865779876708984\n",
      "evaluation/text/perplexity: 129.77210998535156\n",
      "time/total: 1335.4319508075714\n",
      "time/evaluation: 0.8203537464141846\n",
      "training/train_loss_mean: 4.854032802581787\n",
      "training/train_loss_std: 0.38728440656517027\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 788\n",
      "training/learning_rate: 1.9390575171803193e-05\n",
      "time/sample_batch: 0.011343002319335938\n",
      "time/training: 1.2090885639190674\n",
      "evaluation/text/loss: 4.64635705947876\n",
      "evaluation/text/perplexity: 104.20468139648438\n",
      "time/total: 1337.1681554317474\n",
      "time/evaluation: 0.525531530380249\n",
      "training/train_loss_mean: 5.016830730438232\n",
      "training/train_loss_std: 0.4456344915111316\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 789\n",
      "training/learning_rate: 1.9303446682700144e-05\n",
      "time/sample_batch: 0.00864863395690918\n",
      "time/training: 0.9622266292572021\n",
      "evaluation/text/loss: 4.757789134979248\n",
      "evaluation/text/perplexity: 116.48809814453125\n",
      "time/total: 1338.8114914894104\n",
      "time/evaluation: 0.6793584823608398\n",
      "training/train_loss_mean: 4.790522050857544\n",
      "training/train_loss_std: 0.6093962830652391\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 790\n",
      "training/learning_rate: 1.9216677657492705e-05\n",
      "time/sample_batch: 0.007531642913818359\n",
      "time/training: 0.9743072986602783\n",
      "evaluation/text/loss: 4.713133335113525\n",
      "evaluation/text/perplexity: 111.40067291259766\n",
      "time/total: 1340.4844188690186\n",
      "time/evaluation: 0.6969971656799316\n",
      "training/train_loss_mean: 5.124935674667358\n",
      "training/train_loss_std: 0.17568898055952278\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 791\n",
      "training/learning_rate: 1.913026896994399e-05\n",
      "time/sample_batch: 0.008304595947265625\n",
      "time/training: 1.0305635929107666\n",
      "evaluation/text/loss: 4.668323993682861\n",
      "evaluation/text/perplexity: 106.5190658569336\n",
      "time/total: 1342.0919783115387\n",
      "time/evaluation: 0.5753943920135498\n",
      "training/train_loss_mean: 5.192374801635742\n",
      "training/train_loss_std: 0.24698866595105648\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 792\n",
      "training/learning_rate: 1.9044221490188504e-05\n",
      "time/sample_batch: 0.009183645248413086\n",
      "time/training: 1.02895188331604\n",
      "evaluation/text/loss: 4.830940246582031\n",
      "evaluation/text/perplexity: 125.32874298095703\n",
      "time/total: 1343.7372553348541\n",
      "time/evaluation: 0.6147987842559814\n",
      "training/train_loss_mean: 5.188216972351074\n",
      "training/train_loss_std: 0.2753464704775385\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 793\n",
      "training/learning_rate: 1.8958536084723427e-05\n",
      "time/sample_batch: 0.007348060607910156\n",
      "time/training: 1.0745158195495605\n",
      "evaluation/text/loss: 4.754581451416016\n",
      "evaluation/text/perplexity: 116.11504364013672\n",
      "time/total: 1345.378300666809\n",
      "time/evaluation: 0.5650041103363037\n",
      "training/train_loss_mean: 4.651752853393555\n",
      "training/train_loss_std: 1.1021119474233796\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 794\n",
      "training/learning_rate: 1.8873213616399855e-05\n",
      "time/sample_batch: 0.0065212249755859375\n",
      "time/training: 0.8880388736724854\n",
      "evaluation/text/loss: 4.55413818359375\n",
      "evaluation/text/perplexity: 95.02482604980469\n",
      "time/total: 1346.7599518299103\n",
      "time/evaluation: 0.49202442169189453\n",
      "training/train_loss_mean: 4.955090856552124\n",
      "training/train_loss_std: 0.7595088468109873\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 795\n",
      "training/learning_rate: 1.8788254944414067e-05\n",
      "time/sample_batch: 0.0089111328125\n",
      "time/training: 0.9875249862670898\n",
      "evaluation/text/loss: 4.754115104675293\n",
      "evaluation/text/perplexity: 116.06090545654297\n",
      "time/total: 1348.3491156101227\n",
      "time/evaluation: 0.6001408100128174\n",
      "training/train_loss_mean: 4.99625186920166\n",
      "training/train_loss_std: 0.17128151504291905\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 796\n",
      "training/learning_rate: 1.8703660924298967e-05\n",
      "time/sample_batch: 0.0069904327392578125\n",
      "time/training: 0.9557406902313232\n",
      "evaluation/text/loss: 4.82830810546875\n",
      "evaluation/text/perplexity: 124.99929809570312\n",
      "time/total: 1349.8818488121033\n",
      "time/evaluation: 0.5751843452453613\n",
      "training/train_loss_mean: 4.933951139450073\n",
      "training/train_loss_std: 0.26262327507833755\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 797\n",
      "training/learning_rate: 1.861943240791541e-05\n",
      "time/sample_batch: 0.008794546127319336\n",
      "time/training: 1.0720829963684082\n",
      "evaluation/text/loss: 4.844106197357178\n",
      "evaluation/text/perplexity: 126.98973083496094\n",
      "time/total: 1351.702181816101\n",
      "time/evaluation: 0.746692419052124\n",
      "training/train_loss_mean: 4.94700174331665\n",
      "training/train_loss_std: 0.26864219452165233\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 798\n",
      "training/learning_rate: 1.853557024344359e-05\n",
      "time/sample_batch: 0.009776115417480469\n",
      "time/training: 1.0484576225280762\n",
      "evaluation/text/loss: 4.855836391448975\n",
      "evaluation/text/perplexity: 128.4881134033203\n",
      "time/total: 1353.2574944496155\n",
      "time/evaluation: 0.5052778720855713\n",
      "training/train_loss_mean: 4.954588484764099\n",
      "training/train_loss_std: 0.4273800360514842\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 799\n",
      "training/learning_rate: 1.8452075275374583e-05\n",
      "time/sample_batch: 0.009148120880126953\n",
      "time/training: 1.0332612991333008\n",
      "evaluation/text/loss: 4.960643768310547\n",
      "evaluation/text/perplexity: 142.6856231689453\n",
      "time/total: 1354.86883020401\n",
      "time/evaluation: 0.5764739513397217\n",
      "training/train_loss_mean: 4.810541200637817\n",
      "training/train_loss_std: 0.35333595459138045\n",
      "================================================================================\n",
      "Input: = | Output : =Backgroundandpublication== | Prediction:  the lyrics of Alex culture and vHCRker '.line is a # marketers unc dated to be\n",
      "Input: <unk> | Output : <unk> | Prediction:  rejected, doing so the49 worker andato Like under Chinese unstable they variation, taking after the shore\n",
      "Input: ==Personal | Output : life== | Prediction:  but bass 34 1840, American value at one, a team returned to New Heavy preparation in typically increasingly\n",
      "================================================================================\n",
      "Iteration 800\n",
      "training/learning_rate: 1.8368948344501816e-05\n",
      "time/sample_batch: 0.008939743041992188\n",
      "time/training: 1.0889008045196533\n",
      "evaluation/text/loss: 4.682648658752441\n",
      "evaluation/text/perplexity: 108.05590057373047\n",
      "time/total: 1357.0121171474457\n",
      "time/evaluation: 1.0528233051300049\n",
      "training/train_loss_mean: 4.880650234222412\n",
      "training/train_loss_std: 0.8465174047245777\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 801\n",
      "training/learning_rate: 1.828619028791252e-05\n",
      "time/sample_batch: 0.007077217102050781\n",
      "time/training: 1.075502872467041\n",
      "evaluation/text/loss: 4.683188438415527\n",
      "evaluation/text/perplexity: 108.11424255371094\n",
      "time/total: 1358.752819776535\n",
      "time/evaluation: 0.6639273166656494\n",
      "training/train_loss_mean: 5.130998945236206\n",
      "training/train_loss_std: 0.34328464677226894\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 802\n",
      "training/learning_rate: 1.820380193897943e-05\n",
      "time/sample_batch: 0.009070873260498047\n",
      "time/training: 1.2367827892303467\n",
      "evaluation/text/loss: 4.8510661125183105\n",
      "evaluation/text/perplexity: 127.87664794921875\n",
      "time/total: 1360.5769212245941\n",
      "time/evaluation: 0.5857090950012207\n",
      "training/train_loss_mean: 5.221754360198974\n",
      "training/train_loss_std: 0.2131942205642216\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 803\n",
      "training/learning_rate: 1.812178412735231e-05\n",
      "time/sample_batch: 0.009052753448486328\n",
      "time/training: 1.2344298362731934\n",
      "evaluation/text/loss: 4.657400131225586\n",
      "evaluation/text/perplexity: 105.36180114746094\n",
      "time/total: 1362.3599472045898\n",
      "time/evaluation: 0.546790599822998\n",
      "training/train_loss_mean: 5.089818811416626\n",
      "training/train_loss_std: 0.17791516658146334\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 804\n",
      "training/learning_rate: 1.8040137678949584e-05\n",
      "time/sample_batch: 0.0072078704833984375\n",
      "time/training: 0.9263162612915039\n",
      "evaluation/text/loss: 4.896206855773926\n",
      "evaluation/text/perplexity: 133.78135681152344\n",
      "time/total: 1363.9863169193268\n",
      "time/evaluation: 0.6984364986419678\n",
      "training/train_loss_mean: 4.922578287124634\n",
      "training/train_loss_std: 0.3902363177474122\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 805\n",
      "training/learning_rate: 1.795886341595011e-05\n",
      "time/sample_batch: 0.00968313217163086\n",
      "time/training: 1.2509844303131104\n",
      "evaluation/text/loss: 4.566290855407715\n",
      "evaluation/text/perplexity: 96.18667602539062\n",
      "time/total: 1365.7494139671326\n",
      "time/evaluation: 0.5105886459350586\n",
      "training/train_loss_mean: 5.180020904541015\n",
      "training/train_loss_std: 0.2548852330757893\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 806\n",
      "training/learning_rate: 1.7877962156784813e-05\n",
      "time/sample_batch: 0.00904536247253418\n",
      "time/training: 1.0616216659545898\n",
      "evaluation/text/loss: 4.822503566741943\n",
      "evaluation/text/perplexity: 124.27583312988281\n",
      "time/total: 1367.2682151794434\n",
      "time/evaluation: 0.4553391933441162\n",
      "training/train_loss_mean: 5.073418378829956\n",
      "training/train_loss_std: 0.3118278902123017\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 807\n",
      "training/learning_rate: 1.779743471612845e-05\n",
      "time/sample_batch: 0.010612964630126953\n",
      "time/training: 0.9713289737701416\n",
      "evaluation/text/loss: 4.6678290367126465\n",
      "evaluation/text/perplexity: 106.46635437011719\n",
      "time/total: 1368.7715141773224\n",
      "time/evaluation: 0.5302207469940186\n",
      "training/train_loss_mean: 5.022067260742188\n",
      "training/train_loss_std: 0.3279183073238536\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 808\n",
      "training/learning_rate: 1.7717281904891483e-05\n",
      "time/sample_batch: 0.008712530136108398\n",
      "time/training: 0.9912574291229248\n",
      "evaluation/text/loss: 4.675517559051514\n",
      "evaluation/text/perplexity: 107.28807830810547\n",
      "time/total: 1370.5828540325165\n",
      "time/evaluation: 0.8183550834655762\n",
      "training/train_loss_mean: 5.146654558181763\n",
      "training/train_loss_std: 0.28828038626079805\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 809\n",
      "training/learning_rate: 1.7637504530211813e-05\n",
      "time/sample_batch: 0.008802175521850586\n",
      "time/training: 1.0962049961090088\n",
      "evaluation/text/loss: 4.618662357330322\n",
      "evaluation/text/perplexity: 101.35836029052734\n",
      "time/total: 1372.3663234710693\n",
      "time/evaluation: 0.685582160949707\n",
      "training/train_loss_mean: 5.255876111984253\n",
      "training/train_loss_std: 0.2194893551303945\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 810\n",
      "training/learning_rate: 1.7558103395446725e-05\n",
      "time/sample_batch: 0.010334968566894531\n",
      "time/training: 1.1556575298309326\n",
      "evaluation/text/loss: 4.779158592224121\n",
      "evaluation/text/perplexity: 119.0041732788086\n",
      "time/total: 1374.1076440811157\n",
      "time/evaluation: 0.584099292755127\n",
      "training/train_loss_mean: 4.86571774482727\n",
      "training/train_loss_std: 0.4100583745456765\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 811\n",
      "training/learning_rate: 1.7479079300164752e-05\n",
      "time/sample_batch: 0.009311437606811523\n",
      "time/training: 0.9300041198730469\n",
      "evaluation/text/loss: 4.896977424621582\n",
      "evaluation/text/perplexity: 133.88449096679688\n",
      "time/total: 1375.565753698349\n",
      "time/evaluation: 0.521198034286499\n",
      "training/train_loss_mean: 5.24602313041687\n",
      "training/train_loss_std: 0.25172795887919663\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 812\n",
      "training/learning_rate: 1.7400433040137666e-05\n",
      "time/sample_batch: 0.008587837219238281\n",
      "time/training: 1.2253947257995605\n",
      "evaluation/text/loss: 4.7937421798706055\n",
      "evaluation/text/perplexity: 120.75240325927734\n",
      "time/total: 1377.4970092773438\n",
      "time/evaluation: 0.7040750980377197\n",
      "training/train_loss_mean: 5.161478471755982\n",
      "training/train_loss_std: 0.20950581200267815\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 813\n",
      "training/learning_rate: 1.7322165407332448e-05\n",
      "time/sample_batch: 0.008557558059692383\n",
      "time/training: 1.0220143795013428\n",
      "evaluation/text/loss: 4.762218952178955\n",
      "evaluation/text/perplexity: 117.00526428222656\n",
      "time/total: 1379.0065026283264\n",
      "time/evaluation: 0.485776424407959\n",
      "training/train_loss_mean: 5.123405933380127\n",
      "training/train_loss_std: 0.40729269307287397\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 814\n",
      "training/learning_rate: 1.7244277189903263e-05\n",
      "time/sample_batch: 0.0069544315338134766\n",
      "time/training: 1.2663161754608154\n",
      "evaluation/text/loss: 4.769744396209717\n",
      "evaluation/text/perplexity: 117.88910675048828\n",
      "time/total: 1380.877758026123\n",
      "time/evaluation: 0.6032240390777588\n",
      "training/train_loss_mean: 5.110689496994018\n",
      "training/train_loss_std: 0.2598272652183028\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 815\n",
      "training/learning_rate: 1.7166769172183605e-05\n",
      "time/sample_batch: 0.008218765258789062\n",
      "time/training: 1.0421662330627441\n",
      "evaluation/text/loss: 4.8447699546813965\n",
      "evaluation/text/perplexity: 127.07404327392578\n",
      "time/total: 1382.6038603782654\n",
      "time/evaluation: 0.682220458984375\n",
      "training/train_loss_mean: 5.204779720306396\n",
      "training/train_loss_std: 0.20095824275590635\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 816\n",
      "training/learning_rate: 1.7089642134678365e-05\n",
      "time/sample_batch: 0.006760835647583008\n",
      "time/training: 0.9258506298065186\n",
      "evaluation/text/loss: 4.883632659912109\n",
      "evaluation/text/perplexity: 132.1096954345703\n",
      "time/total: 1384.1333847045898\n",
      "time/evaluation: 0.6020653247833252\n",
      "training/train_loss_mean: 5.022264575958252\n",
      "training/train_loss_std: 0.4721882529982878\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 817\n",
      "training/learning_rate: 1.7012896854055935e-05\n",
      "time/sample_batch: 0.007463216781616211\n",
      "time/training: 0.8573191165924072\n",
      "evaluation/text/loss: 4.733437538146973\n",
      "evaluation/text/perplexity: 113.6856918334961\n",
      "time/total: 1385.588071346283\n",
      "time/evaluation: 0.5956275463104248\n",
      "training/train_loss_mean: 5.253070259094239\n",
      "training/train_loss_std: 0.2540420404469613\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 818\n",
      "training/learning_rate: 1.6936534103140457e-05\n",
      "time/sample_batch: 0.00969386100769043\n",
      "time/training: 1.1661114692687988\n",
      "evaluation/text/loss: 4.763177394866943\n",
      "evaluation/text/perplexity: 117.11746215820312\n",
      "time/total: 1387.4391992092133\n",
      "time/evaluation: 0.6834008693695068\n",
      "training/train_loss_mean: 5.20465054512024\n",
      "training/train_loss_std: 0.31579148101763593\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 819\n",
      "training/learning_rate: 1.686055465090398e-05\n",
      "time/sample_batch: 0.007833003997802734\n",
      "time/training: 0.9914486408233643\n",
      "evaluation/text/loss: 4.925353527069092\n",
      "evaluation/text/perplexity: 137.73802185058594\n",
      "time/total: 1389.199286699295\n",
      "time/evaluation: 0.766808271408081\n",
      "training/train_loss_mean: 5.190500783920288\n",
      "training/train_loss_std: 0.19189716857844932\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 820\n",
      "training/learning_rate: 1.6784959262458736e-05\n",
      "time/sample_batch: 0.007033348083496094\n",
      "time/training: 1.0521256923675537\n",
      "evaluation/text/loss: 4.734799385070801\n",
      "evaluation/text/perplexity: 113.84062194824219\n",
      "time/total: 1390.953309059143\n",
      "time/evaluation: 0.7003004550933838\n",
      "training/train_loss_mean: 4.896235537528992\n",
      "training/train_loss_std: 0.5446869168978808\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 821\n",
      "training/learning_rate: 1.6709748699049446e-05\n",
      "time/sample_batch: 0.007142543792724609\n",
      "time/training: 0.8906195163726807\n",
      "evaluation/text/loss: 4.855050086975098\n",
      "evaluation/text/perplexity: 128.38711547851562\n",
      "time/total: 1392.5791127681732\n",
      "time/evaluation: 0.7334792613983154\n",
      "training/train_loss_mean: 4.890289068222046\n",
      "training/train_loss_std: 0.3020617103892011\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 822\n",
      "training/learning_rate: 1.663492371804567e-05\n",
      "time/sample_batch: 0.008939027786254883\n",
      "time/training: 0.9640054702758789\n",
      "evaluation/text/loss: 4.835705280303955\n",
      "evaluation/text/perplexity: 125.9273681640625\n",
      "time/total: 1394.2264475822449\n",
      "time/evaluation: 0.6817526817321777\n",
      "training/train_loss_mean: 5.0825694561004635\n",
      "training/train_loss_std: 0.2772900167866225\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 823\n",
      "training/learning_rate: 1.656048507293411e-05\n",
      "time/sample_batch: 0.009459495544433594\n",
      "time/training: 1.032731533050537\n",
      "evaluation/text/loss: 4.637190341949463\n",
      "evaluation/text/perplexity: 103.25382995605469\n",
      "time/total: 1395.97678732872\n",
      "time/evaluation: 0.7157766819000244\n",
      "training/train_loss_mean: 5.020653009414673\n",
      "training/train_loss_std: 0.19718885871213182\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 824\n",
      "training/learning_rate: 1.6486433513311106e-05\n",
      "time/sample_batch: 0.008768320083618164\n",
      "time/training: 1.1088392734527588\n",
      "evaluation/text/loss: 4.636257648468018\n",
      "evaluation/text/perplexity: 103.1575698852539\n",
      "time/total: 1397.878478050232\n",
      "time/evaluation: 0.7912454605102539\n",
      "training/train_loss_mean: 5.150948190689087\n",
      "training/train_loss_std: 0.3768271840353721\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 825\n",
      "training/learning_rate: 1.6412769784875056e-05\n",
      "time/sample_batch: 0.00758814811706543\n",
      "time/training: 0.9752049446105957\n",
      "evaluation/text/loss: 4.632414817810059\n",
      "evaluation/text/perplexity: 102.76191711425781\n",
      "time/total: 1399.6004946231842\n",
      "time/evaluation: 0.745072603225708\n",
      "training/train_loss_mean: 5.0896875858306885\n",
      "training/train_loss_std: 0.24345236886216387\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 826\n",
      "training/learning_rate: 1.6339494629418865e-05\n",
      "time/sample_batch: 0.009672164916992188\n",
      "time/training: 1.0007095336914062\n",
      "evaluation/text/loss: 4.74983024597168\n",
      "evaluation/text/perplexity: 115.56466674804688\n",
      "time/total: 1401.0322139263153\n",
      "time/evaluation: 0.4293954372406006\n",
      "training/train_loss_mean: 5.096672105789184\n",
      "training/train_loss_std: 0.13952464981068025\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 827\n",
      "training/learning_rate: 1.6266608784822544e-05\n",
      "time/sample_batch: 0.008949518203735352\n",
      "time/training: 1.2157433032989502\n",
      "evaluation/text/loss: 4.745754241943359\n",
      "evaluation/text/perplexity: 115.0945816040039\n",
      "time/total: 1403.076929807663\n",
      "time/evaluation: 0.827228307723999\n",
      "training/train_loss_mean: 5.197352886199951\n",
      "training/train_loss_std: 0.31861748175665155\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 828\n",
      "training/learning_rate: 1.6194112985045732e-05\n",
      "time/sample_batch: 0.009679317474365234\n",
      "time/training: 1.0943820476531982\n",
      "evaluation/text/loss: 4.866225719451904\n",
      "evaluation/text/perplexity: 129.82997131347656\n",
      "time/total: 1404.920282125473\n",
      "time/evaluation: 0.7473196983337402\n",
      "training/train_loss_mean: 5.0744928359985355\n",
      "training/train_loss_std: 0.3418724877142764\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 829\n",
      "training/learning_rate: 1.6122007960120348e-05\n",
      "time/sample_batch: 0.008482694625854492\n",
      "time/training: 1.0590007305145264\n",
      "evaluation/text/loss: 4.699385166168213\n",
      "evaluation/text/perplexity: 109.87959289550781\n",
      "time/total: 1406.746579170227\n",
      "time/evaluation: 0.7656233310699463\n",
      "training/train_loss_mean: 5.148723602294922\n",
      "training/train_loss_std: 0.16345270843823134\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 830\n",
      "training/learning_rate: 1.6050294436143168e-05\n",
      "time/sample_batch: 0.007468700408935547\n",
      "time/training: 0.9048135280609131\n",
      "evaluation/text/loss: 4.774580955505371\n",
      "evaluation/text/perplexity: 118.46066284179688\n",
      "time/total: 1408.1889460086823\n",
      "time/evaluation: 0.5359632968902588\n",
      "training/train_loss_mean: 4.906782245635986\n",
      "training/train_loss_std: 0.45017961208879004\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 831\n",
      "training/learning_rate: 1.5978973135268594e-05\n",
      "time/sample_batch: 0.008263826370239258\n",
      "time/training: 1.3443458080291748\n",
      "evaluation/text/loss: 4.747952938079834\n",
      "evaluation/text/perplexity: 115.34791564941406\n",
      "time/total: 1410.2118175029755\n",
      "time/evaluation: 0.6769611835479736\n",
      "training/train_loss_mean: 5.13253231048584\n",
      "training/train_loss_std: 0.24665848979904267\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 832\n",
      "training/learning_rate: 1.590804477570134e-05\n",
      "time/sample_batch: 0.010183334350585938\n",
      "time/training: 1.211383581161499\n",
      "evaluation/text/loss: 4.80642557144165\n",
      "evaluation/text/perplexity: 122.29370880126953\n",
      "time/total: 1412.0777299404144\n",
      "time/evaluation: 0.6529784202575684\n",
      "training/train_loss_mean: 5.099770402908325\n",
      "training/train_loss_std: 0.24098349982748293\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 833\n",
      "training/learning_rate: 1.583751007168919e-05\n",
      "time/sample_batch: 0.009278059005737305\n",
      "time/training: 1.083763837814331\n",
      "evaluation/text/loss: 4.8169074058532715\n",
      "evaluation/text/perplexity: 123.58230590820312\n",
      "time/total: 1413.9227364063263\n",
      "time/evaluation: 0.7595124244689941\n",
      "training/train_loss_mean: 5.183296585083008\n",
      "training/train_loss_std: 0.18428038794171905\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 834\n",
      "training/learning_rate: 1.5767369733515824e-05\n",
      "time/sample_batch: 0.008594036102294922\n",
      "time/training: 0.9356317520141602\n",
      "evaluation/text/loss: 4.887307167053223\n",
      "evaluation/text/perplexity: 132.59603881835938\n",
      "time/total: 1415.576870918274\n",
      "time/evaluation: 0.7167971134185791\n",
      "training/train_loss_mean: 4.788412308692932\n",
      "training/train_loss_std: 0.7184135176653923\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 835\n",
      "training/learning_rate: 1.5697624467493695e-05\n",
      "time/sample_batch: 0.008715391159057617\n",
      "time/training: 1.0275585651397705\n",
      "evaluation/text/loss: 4.859775543212891\n",
      "evaluation/text/perplexity: 128.9952392578125\n",
      "time/total: 1417.600085258484\n",
      "time/evaluation: 0.9938685894012451\n",
      "training/train_loss_mean: 5.082328367233276\n",
      "training/train_loss_std: 0.23893828520098945\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 836\n",
      "training/learning_rate: 1.562827497595682e-05\n",
      "time/sample_batch: 0.007213592529296875\n",
      "time/training: 0.8868629932403564\n",
      "evaluation/text/loss: 4.781795501708984\n",
      "evaluation/text/perplexity: 119.31839752197266\n",
      "time/total: 1419.0546083450317\n",
      "time/evaluation: 0.5660395622253418\n",
      "training/train_loss_mean: 5.0970723867416385\n",
      "training/train_loss_std: 0.427487193968969\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 837\n",
      "training/learning_rate: 1.555932195725384e-05\n",
      "time/sample_batch: 0.007709026336669922\n",
      "time/training: 0.966994047164917\n",
      "evaluation/text/loss: 4.8156023025512695\n",
      "evaluation/text/perplexity: 123.42112731933594\n",
      "time/total: 1420.7062883377075\n",
      "time/evaluation: 0.683114767074585\n",
      "training/train_loss_mean: 4.969328784942627\n",
      "training/train_loss_std: 0.35854172080619456\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 838\n",
      "training/learning_rate: 1.549076610574088e-05\n",
      "time/sample_batch: 0.008450508117675781\n",
      "time/training: 1.1381824016571045\n",
      "evaluation/text/loss: 4.77742862701416\n",
      "evaluation/text/perplexity: 118.7984848022461\n",
      "time/total: 1422.555528640747\n",
      "time/evaluation: 0.7093033790588379\n",
      "training/train_loss_mean: 5.13376669883728\n",
      "training/train_loss_std: 0.3128573976403613\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 839\n",
      "training/learning_rate: 1.5422608111774595e-05\n",
      "time/sample_batch: 0.008174896240234375\n",
      "time/training: 1.0803678035736084\n",
      "evaluation/text/loss: 4.795775890350342\n",
      "evaluation/text/perplexity: 120.99822998046875\n",
      "time/total: 1424.1712353229523\n",
      "time/evaluation: 0.533674955368042\n",
      "training/train_loss_mean: 5.100624632835388\n",
      "training/train_loss_std: 0.7330389181323781\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 840\n",
      "training/learning_rate: 1.535484866170523e-05\n",
      "time/sample_batch: 0.009180068969726562\n",
      "time/training: 1.1621227264404297\n",
      "evaluation/text/loss: 4.800502300262451\n",
      "evaluation/text/perplexity: 121.57146453857422\n",
      "time/total: 1426.1527979373932\n",
      "time/evaluation: 0.8178520202636719\n",
      "training/train_loss_mean: 5.068203973770141\n",
      "training/train_loss_std: 0.3751169468701534\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 841\n",
      "training/learning_rate: 1.528748843786971e-05\n",
      "time/sample_batch: 0.006917238235473633\n",
      "time/training: 1.0744297504425049\n",
      "evaluation/text/loss: 4.72765588760376\n",
      "evaluation/text/perplexity: 113.0302963256836\n",
      "time/total: 1427.9699292182922\n",
      "time/evaluation: 0.7409017086029053\n",
      "training/train_loss_mean: 5.027213621139526\n",
      "training/train_loss_std: 0.3211472139888396\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 842\n",
      "training/learning_rate: 1.5220528118584729e-05\n",
      "time/sample_batch: 0.00849771499633789\n",
      "time/training: 0.905919075012207\n",
      "evaluation/text/loss: 4.787149429321289\n",
      "evaluation/text/perplexity: 119.95893096923828\n",
      "time/total: 1429.682862997055\n",
      "time/evaluation: 0.8052825927734375\n",
      "training/train_loss_mean: 5.139995765686035\n",
      "training/train_loss_std: 0.20003843652720713\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 843\n",
      "training/learning_rate: 1.515396837813995e-05\n",
      "time/sample_batch: 0.006133317947387695\n",
      "time/training: 0.9402046203613281\n",
      "evaluation/text/loss: 4.704075813293457\n",
      "evaluation/text/perplexity: 110.39620971679688\n",
      "time/total: 1431.3384709358215\n",
      "time/evaluation: 0.7137985229492188\n",
      "training/train_loss_mean: 5.07791805267334\n",
      "training/train_loss_std: 0.7527403067773877\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 844\n",
      "training/learning_rate: 1.5087809886791223e-05\n",
      "time/sample_batch: 0.007919073104858398\n",
      "time/training: 1.0073308944702148\n",
      "evaluation/text/loss: 4.951143741607666\n",
      "evaluation/text/perplexity: 141.33651733398438\n",
      "time/total: 1433.0362594127655\n",
      "time/evaluation: 0.6887876987457275\n",
      "training/train_loss_mean: 5.148271799087524\n",
      "training/train_loss_std: 0.20256056836542624\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 845\n",
      "training/learning_rate: 1.5022053310753845e-05\n",
      "time/sample_batch: 0.010107040405273438\n",
      "time/training: 1.0026400089263916\n",
      "evaluation/text/loss: 4.736432075500488\n",
      "evaluation/text/perplexity: 114.0266342163086\n",
      "time/total: 1434.752203464508\n",
      "time/evaluation: 0.7115705013275146\n",
      "training/train_loss_mean: 5.017661190032959\n",
      "training/train_loss_std: 0.40488895975730443\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 846\n",
      "training/learning_rate: 1.4956699312195776e-05\n",
      "time/sample_batch: 0.007811546325683594\n",
      "time/training: 1.0110259056091309\n",
      "evaluation/text/loss: 4.855702877044678\n",
      "evaluation/text/perplexity: 128.47096252441406\n",
      "time/total: 1436.3817026615143\n",
      "time/evaluation: 0.6166501045227051\n",
      "training/train_loss_mean: 4.9978355884552\n",
      "training/train_loss_std: 0.30211882457920514\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 847\n",
      "training/learning_rate: 1.4891748549231074e-05\n",
      "time/sample_batch: 0.008171319961547852\n",
      "time/training: 1.0659706592559814\n",
      "evaluation/text/loss: 4.825212478637695\n",
      "evaluation/text/perplexity: 124.61294555664062\n",
      "time/total: 1438.1274619102478\n",
      "time/evaluation: 0.6779487133026123\n",
      "training/train_loss_mean: 5.147324752807617\n",
      "training/train_loss_std: 0.1652067236998602\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 848\n",
      "training/learning_rate: 1.48272016759132e-05\n",
      "time/sample_batch: 0.009096622467041016\n",
      "time/training: 1.1145708560943604\n",
      "evaluation/text/loss: 4.682648658752441\n",
      "evaluation/text/perplexity: 108.05590057373047\n",
      "time/total: 1439.7727365493774\n",
      "time/evaluation: 0.5289871692657471\n",
      "training/train_loss_mean: 5.0847266674041744\n",
      "training/train_loss_std: 0.22507431797553126\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 849\n",
      "training/learning_rate: 1.4763059342228442e-05\n",
      "time/sample_batch: 0.008524179458618164\n",
      "time/training: 1.0469956398010254\n",
      "evaluation/text/loss: 4.723332405090332\n",
      "evaluation/text/perplexity: 112.54266357421875\n",
      "time/total: 1441.5307960510254\n",
      "time/evaluation: 0.7093899250030518\n",
      "training/train_loss_mean: 5.081123924255371\n",
      "training/train_loss_std: 0.3055149713046769\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 850\n",
      "training/learning_rate: 1.4699322194089387e-05\n",
      "time/sample_batch: 0.007863283157348633\n",
      "time/training: 0.9558877944946289\n",
      "evaluation/text/loss: 4.905608654022217\n",
      "evaluation/text/perplexity: 135.04507446289062\n",
      "time/total: 1443.2300651073456\n",
      "time/evaluation: 0.737058162689209\n",
      "training/train_loss_mean: 4.548381841182708\n",
      "training/train_loss_std: 1.0552302822433737\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 851\n",
      "training/learning_rate: 1.4635990873328434e-05\n",
      "time/sample_batch: 0.009392023086547852\n",
      "time/training: 1.072840929031372\n",
      "evaluation/text/loss: 4.805178642272949\n",
      "evaluation/text/perplexity: 122.14131164550781\n",
      "time/total: 1445.016764163971\n",
      "time/evaluation: 0.7121767997741699\n",
      "training/train_loss_mean: 5.245342969894409\n",
      "training/train_loss_std: 0.17361051383249582\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 852\n",
      "training/learning_rate: 1.4573066017691247e-05\n",
      "time/sample_batch: 0.009249687194824219\n",
      "time/training: 1.1712634563446045\n",
      "evaluation/text/loss: 4.774020195007324\n",
      "evaluation/text/perplexity: 118.39425659179688\n",
      "time/total: 1446.9345965385437\n",
      "time/evaluation: 0.7449417114257812\n",
      "training/train_loss_mean: 5.040230941772461\n",
      "training/train_loss_std: 0.39620160825244544\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 853\n",
      "training/learning_rate: 1.4510548260830444e-05\n",
      "time/sample_batch: 0.009022235870361328\n",
      "time/training: 1.1070060729980469\n",
      "evaluation/text/loss: 4.738050937652588\n",
      "evaluation/text/perplexity: 114.21138000488281\n",
      "time/total: 1448.5509951114655\n",
      "time/evaluation: 0.5076661109924316\n",
      "training/train_loss_mean: 5.074357223510742\n",
      "training/train_loss_std: 0.2761029746222393\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 854\n",
      "training/learning_rate: 1.4448438232299144e-05\n",
      "time/sample_batch: 0.008966684341430664\n",
      "time/training: 1.0230345726013184\n",
      "evaluation/text/loss: 4.549373149871826\n",
      "evaluation/text/perplexity: 94.57310485839844\n",
      "time/total: 1450.0202226638794\n",
      "time/evaluation: 0.44444704055786133\n",
      "training/train_loss_mean: 5.104323816299439\n",
      "training/train_loss_std: 0.39643624126860966\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 855\n",
      "training/learning_rate: 1.4386736557544627e-05\n",
      "time/sample_batch: 0.007532835006713867\n",
      "time/training: 1.0201506614685059\n",
      "evaluation/text/loss: 4.639695644378662\n",
      "evaluation/text/perplexity: 103.5128402709961\n",
      "time/total: 1451.5124275684357\n",
      "time/evaluation: 0.4703691005706787\n",
      "training/train_loss_mean: 4.925897693634033\n",
      "training/train_loss_std: 0.27320181891602574\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 856\n",
      "training/learning_rate: 1.432544385790209e-05\n",
      "time/sample_batch: 0.008874654769897461\n",
      "time/training: 1.1963815689086914\n",
      "evaluation/text/loss: 4.867472171783447\n",
      "evaluation/text/perplexity: 129.9918975830078\n",
      "time/total: 1453.4554727077484\n",
      "time/evaluation: 0.7450180053710938\n",
      "training/train_loss_mean: 5.10553674697876\n",
      "training/train_loss_std: 0.22338625527651443\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 857\n",
      "training/learning_rate: 1.4264560750588335e-05\n",
      "time/sample_batch: 0.007506847381591797\n",
      "time/training: 1.0555896759033203\n",
      "evaluation/text/loss: 4.720396041870117\n",
      "evaluation/text/perplexity: 112.21268463134766\n",
      "time/total: 1455.1234278678894\n",
      "time/evaluation: 0.6107828617095947\n",
      "training/train_loss_mean: 5.081010246276856\n",
      "training/train_loss_std: 0.3093341127056501\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 858\n",
      "training/learning_rate: 1.4204087848695562e-05\n",
      "time/sample_batch: 0.008952140808105469\n",
      "time/training: 1.042755365371704\n",
      "evaluation/text/loss: 4.799951553344727\n",
      "evaluation/text/perplexity: 121.50453186035156\n",
      "time/total: 1456.7142016887665\n",
      "time/evaluation: 0.5463194847106934\n",
      "training/train_loss_mean: 5.041485118865967\n",
      "training/train_loss_std: 0.12572588796164813\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 859\n",
      "training/learning_rate: 1.414402576118525e-05\n",
      "time/sample_batch: 0.0076448917388916016\n",
      "time/training: 0.9748184680938721\n",
      "evaluation/text/loss: 4.832948207855225\n",
      "evaluation/text/perplexity: 125.58065032958984\n",
      "time/total: 1458.159375667572\n",
      "time/evaluation: 0.4687495231628418\n",
      "training/train_loss_mean: 4.944534158706665\n",
      "training/train_loss_std: 0.23360335428301413\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 860\n",
      "training/learning_rate: 1.4084375092881921e-05\n",
      "time/sample_batch: 0.008233070373535156\n",
      "time/training: 1.2268345355987549\n",
      "evaluation/text/loss: 4.929635524749756\n",
      "evaluation/text/perplexity: 138.32908630371094\n",
      "time/total: 1460.1395406723022\n",
      "time/evaluation: 0.7516992092132568\n",
      "training/train_loss_mean: 5.023945999145508\n",
      "training/train_loss_std: 0.315111256060364\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 861\n",
      "training/learning_rate: 1.4025136444467172e-05\n",
      "time/sample_batch: 0.007102012634277344\n",
      "time/training: 1.0668392181396484\n",
      "evaluation/text/loss: 4.637335300445557\n",
      "evaluation/text/perplexity: 103.268798828125\n",
      "time/total: 1461.8244607448578\n",
      "time/evaluation: 0.6163783073425293\n",
      "training/train_loss_mean: 5.154849481582642\n",
      "training/train_loss_std: 0.24402608260050884\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 862\n",
      "training/learning_rate: 1.396631041247351e-05\n",
      "time/sample_batch: 0.006903409957885742\n",
      "time/training: 0.8901374340057373\n",
      "evaluation/text/loss: 4.749504566192627\n",
      "evaluation/text/perplexity: 115.52703094482422\n",
      "time/total: 1463.3782558441162\n",
      "time/evaluation: 0.6619229316711426\n",
      "training/train_loss_mean: 5.197498559951782\n",
      "training/train_loss_std: 0.700256580418124\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 863\n",
      "training/learning_rate: 1.3907897589278434e-05\n",
      "time/sample_batch: 0.009533166885375977\n",
      "time/training: 1.215282678604126\n",
      "evaluation/text/loss: 4.832172393798828\n",
      "evaluation/text/perplexity: 125.48326110839844\n",
      "time/total: 1465.3392918109894\n",
      "time/evaluation: 0.744060754776001\n",
      "training/train_loss_mean: 5.103580951690674\n",
      "training/train_loss_std: 0.3466600348705192\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 864\n",
      "training/learning_rate: 1.3849898563098434e-05\n",
      "time/sample_batch: 0.007040262222290039\n",
      "time/training: 0.8988852500915527\n",
      "evaluation/text/loss: 4.7386794090271\n",
      "evaluation/text/perplexity: 114.2831802368164\n",
      "time/total: 1466.7161951065063\n",
      "time/evaluation: 0.47632384300231934\n",
      "training/train_loss_mean: 4.817113971710205\n",
      "training/train_loss_std: 0.9624748661239119\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 865\n",
      "training/learning_rate: 1.3792313917983047e-05\n",
      "time/sample_batch: 0.0072383880615234375\n",
      "time/training: 0.8543052673339844\n",
      "evaluation/text/loss: 4.789417266845703\n",
      "evaluation/text/perplexity: 120.23128509521484\n",
      "time/total: 1468.2471237182617\n",
      "time/evaluation: 0.6749353408813477\n",
      "training/train_loss_mean: 5.169259691238404\n",
      "training/train_loss_std: 0.1438206723064249\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 866\n",
      "training/learning_rate: 1.373514423380901e-05\n",
      "time/sample_batch: 0.008657455444335938\n",
      "time/training: 1.0518770217895508\n",
      "evaluation/text/loss: 4.864546298980713\n",
      "evaluation/text/perplexity: 129.61212158203125\n",
      "time/total: 1470.001396894455\n",
      "time/evaluation: 0.7006955146789551\n",
      "training/train_loss_mean: 5.1379249572753904\n",
      "training/train_loss_std: 0.2316232051268086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 867\n",
      "training/learning_rate: 1.3678390086274423e-05\n",
      "time/sample_batch: 0.007877349853515625\n",
      "time/training: 1.006394863128662\n",
      "evaluation/text/loss: 4.8434553146362305\n",
      "evaluation/text/perplexity: 126.90709686279297\n",
      "time/total: 1471.57124710083\n",
      "time/evaluation: 0.561718225479126\n",
      "training/train_loss_mean: 4.794758033752442\n",
      "training/train_loss_std: 0.835399428309482\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 868\n",
      "training/learning_rate: 1.3622052046892908e-05\n",
      "time/sample_batch: 0.008359193801879883\n",
      "time/training: 0.9410059452056885\n",
      "evaluation/text/loss: 4.914704322814941\n",
      "evaluation/text/perplexity: 136.2790069580078\n",
      "time/total: 1473.1230428218842\n",
      "time/evaluation: 0.609175443649292\n",
      "training/train_loss_mean: 5.048610258102417\n",
      "training/train_loss_std: 0.24946213552191918\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 869\n",
      "training/learning_rate: 1.35661306829879e-05\n",
      "time/sample_batch: 0.0070722103118896484\n",
      "time/training: 1.0424115657806396\n",
      "evaluation/text/loss: 4.555918216705322\n",
      "evaluation/text/perplexity: 95.19412231445312\n",
      "time/total: 1474.9087960720062\n",
      "time/evaluation: 0.7416315078735352\n",
      "training/train_loss_mean: 5.0246435642242435\n",
      "training/train_loss_std: 0.20772061567514513\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 870\n",
      "training/learning_rate: 1.3510626557686911e-05\n",
      "time/sample_batch: 0.009148359298706055\n",
      "time/training: 1.0786221027374268\n",
      "evaluation/text/loss: 4.8118696212768555\n",
      "evaluation/text/perplexity: 122.96129608154297\n",
      "time/total: 1476.4960658550262\n",
      "time/evaluation: 0.5069191455841064\n",
      "training/train_loss_mean: 5.091301345825196\n",
      "training/train_loss_std: 0.2839590747936655\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 871\n",
      "training/learning_rate: 1.3455540229915863e-05\n",
      "time/sample_batch: 0.007627248764038086\n",
      "time/training: 1.0109946727752686\n",
      "evaluation/text/loss: 4.668755531311035\n",
      "evaluation/text/perplexity: 106.5650405883789\n",
      "time/total: 1478.0506744384766\n",
      "time/evaluation: 0.5418522357940674\n",
      "training/train_loss_mean: 5.076778936386108\n",
      "training/train_loss_std: 0.31564814505829636\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 872\n",
      "training/learning_rate: 1.340087225439347e-05\n",
      "time/sample_batch: 0.007750988006591797\n",
      "time/training: 1.0331752300262451\n",
      "evaluation/text/loss: 4.754642009735107\n",
      "evaluation/text/perplexity: 116.12207794189453\n",
      "time/total: 1479.765213251114\n",
      "time/evaluation: 0.679675817489624\n",
      "training/train_loss_mean: 5.038135004043579\n",
      "training/train_loss_std: 0.21356253937245875\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 873\n",
      "training/learning_rate: 1.3346623181625651e-05\n",
      "time/sample_batch: 0.008449316024780273\n",
      "time/training: 1.0118770599365234\n",
      "evaluation/text/loss: 4.729549884796143\n",
      "evaluation/text/perplexity: 113.24457550048828\n",
      "time/total: 1481.5156824588776\n",
      "time/evaluation: 0.7369546890258789\n",
      "training/train_loss_mean: 4.993924760818482\n",
      "training/train_loss_std: 0.36609965733562616\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 874\n",
      "training/learning_rate: 1.3292793557899941e-05\n",
      "time/sample_batch: 0.00839686393737793\n",
      "time/training: 1.180344581604004\n",
      "evaluation/text/loss: 4.782923698425293\n",
      "evaluation/text/perplexity: 119.45308685302734\n",
      "time/total: 1483.45583486557\n",
      "time/evaluation: 0.7581400871276855\n",
      "training/train_loss_mean: 5.189640283584595\n",
      "training/train_loss_std: 0.363573354028497\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 875\n",
      "training/learning_rate: 1.323938392528007e-05\n",
      "time/sample_batch: 0.008652210235595703\n",
      "time/training: 1.2198054790496826\n",
      "evaluation/text/loss: 4.817910671234131\n",
      "evaluation/text/perplexity: 123.70635986328125\n",
      "time/total: 1485.3889169692993\n",
      "time/evaluation: 0.7115387916564941\n",
      "training/train_loss_mean: 5.009090423583984\n",
      "training/train_loss_std: 0.18646949761322462\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 876\n",
      "training/learning_rate: 1.3186394821600446e-05\n",
      "time/sample_batch: 0.008424043655395508\n",
      "time/training: 1.0520470142364502\n",
      "evaluation/text/loss: 4.787683963775635\n",
      "evaluation/text/perplexity: 120.0230712890625\n",
      "time/total: 1487.1843659877777\n",
      "time/evaluation: 0.7417018413543701\n",
      "training/train_loss_mean: 5.005777359008789\n",
      "training/train_loss_std: 0.2389021545212713\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 877\n",
      "training/learning_rate: 1.3133826780460754e-05\n",
      "time/sample_batch: 0.0075490474700927734\n",
      "time/training: 1.0783681869506836\n",
      "evaluation/text/loss: 4.812824249267578\n",
      "evaluation/text/perplexity: 123.0787353515625\n",
      "time/total: 1489.017264842987\n",
      "time/evaluation: 0.7528979778289795\n",
      "training/train_loss_mean: 4.92236008644104\n",
      "training/train_loss_std: 0.9536309403805465\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 878\n",
      "training/learning_rate: 1.3081680331220566e-05\n",
      "time/sample_batch: 0.007759571075439453\n",
      "time/training: 1.01137113571167\n",
      "evaluation/text/loss: 4.6501240730285645\n",
      "evaluation/text/perplexity: 104.59796142578125\n",
      "time/total: 1490.5109808444977\n",
      "time/evaluation: 0.4806957244873047\n",
      "training/train_loss_mean: 4.893259072303772\n",
      "training/train_loss_std: 0.39711671310853747\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 879\n",
      "training/learning_rate: 1.302995599899407e-05\n",
      "time/sample_batch: 0.00796961784362793\n",
      "time/training: 1.0187976360321045\n",
      "evaluation/text/loss: 4.902490615844727\n",
      "evaluation/text/perplexity: 134.62466430664062\n",
      "time/total: 1492.0178451538086\n",
      "time/evaluation: 0.4863593578338623\n",
      "training/train_loss_mean: 5.007265949249268\n",
      "training/train_loss_std: 0.23888953831971843\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 880\n",
      "training/learning_rate: 1.2978654304644716e-05\n",
      "time/sample_batch: 0.009742021560668945\n",
      "time/training: 0.9549884796142578\n",
      "evaluation/text/loss: 4.719789028167725\n",
      "evaluation/text/perplexity: 112.14459228515625\n",
      "time/total: 1493.5479536056519\n",
      "time/evaluation: 0.5733926296234131\n",
      "training/train_loss_mean: 4.8930068731307985\n",
      "training/train_loss_std: 0.9378389069794184\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 881\n",
      "training/learning_rate: 1.2927775764779975e-05\n",
      "time/sample_batch: 0.00868535041809082\n",
      "time/training: 0.9507403373718262\n",
      "evaluation/text/loss: 4.6694440841674805\n",
      "evaluation/text/perplexity: 106.63844299316406\n",
      "time/total: 1495.162706375122\n",
      "time/evaluation: 0.6623618602752686\n",
      "training/train_loss_mean: 4.994457149505616\n",
      "training/train_loss_std: 0.28418173571777683\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 882\n",
      "training/learning_rate: 1.2877320891746206e-05\n",
      "time/sample_batch: 0.008608818054199219\n",
      "time/training: 1.1132240295410156\n",
      "evaluation/text/loss: 4.660842418670654\n",
      "evaluation/text/perplexity: 105.72511291503906\n",
      "time/total: 1496.8976016044617\n",
      "time/evaluation: 0.6198379993438721\n",
      "training/train_loss_mean: 5.046085405349731\n",
      "training/train_loss_std: 0.3539608299477914\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 883\n",
      "training/learning_rate: 1.282729019362341e-05\n",
      "time/sample_batch: 0.008773088455200195\n",
      "time/training: 0.9662113189697266\n",
      "evaluation/text/loss: 4.739373207092285\n",
      "evaluation/text/perplexity: 114.36249542236328\n",
      "time/total: 1498.38991189003\n",
      "time/evaluation: 0.5242738723754883\n",
      "training/train_loss_mean: 5.054710435867309\n",
      "training/train_loss_std: 0.3582818462449164\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 884\n",
      "training/learning_rate: 1.2777684174220148e-05\n",
      "time/sample_batch: 0.009491682052612305\n",
      "time/training: 1.0544383525848389\n",
      "evaluation/text/loss: 4.612687587738037\n",
      "evaluation/text/perplexity: 100.75457000732422\n",
      "time/total: 1499.8347563743591\n",
      "time/evaluation: 0.38872647285461426\n",
      "training/train_loss_mean: 5.118214225769043\n",
      "training/train_loss_std: 0.3066754697613819\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 885\n",
      "training/learning_rate: 1.2728503333068498e-05\n",
      "time/sample_batch: 0.009334087371826172\n",
      "time/training: 1.099289894104004\n",
      "evaluation/text/loss: 4.719196319580078\n",
      "evaluation/text/perplexity: 112.07814025878906\n",
      "time/total: 1501.4793133735657\n",
      "time/evaluation: 0.5435440540313721\n",
      "training/train_loss_mean: 5.156699991226196\n",
      "training/train_loss_std: 0.22465344570800053\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 886\n",
      "training/learning_rate: 1.267974816541899e-05\n",
      "time/sample_batch: 0.008211851119995117\n",
      "time/training: 0.9411234855651855\n",
      "evaluation/text/loss: 4.878011703491211\n",
      "evaluation/text/perplexity: 131.36920166015625\n",
      "time/total: 1503.1884303092957\n",
      "time/evaluation: 0.7662782669067383\n",
      "training/train_loss_mean: 5.169044256210327\n",
      "training/train_loss_std: 0.15040616392915812\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 887\n",
      "training/learning_rate: 1.2631419162235603e-05\n",
      "time/sample_batch: 0.008788108825683594\n",
      "time/training: 1.168851375579834\n",
      "evaluation/text/loss: 4.796445846557617\n",
      "evaluation/text/perplexity: 121.07931518554688\n",
      "time/total: 1505.0205068588257\n",
      "time/evaluation: 0.661484956741333\n",
      "training/train_loss_mean: 5.154129266738892\n",
      "training/train_loss_std: 0.3029803818868222\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 888\n",
      "training/learning_rate: 1.258351681019088e-05\n",
      "time/sample_batch: 0.009928464889526367\n",
      "time/training: 1.0792357921600342\n",
      "evaluation/text/loss: 4.783393859863281\n",
      "evaluation/text/perplexity: 119.50926208496094\n",
      "time/total: 1506.7685859203339\n",
      "time/evaluation: 0.6671497821807861\n",
      "training/train_loss_mean: 4.764834094047546\n",
      "training/train_loss_std: 0.9364567407383775\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 889\n",
      "training/learning_rate: 1.2536041591660969e-05\n",
      "time/sample_batch: 0.008106231689453125\n",
      "time/training: 0.8663568496704102\n",
      "evaluation/text/loss: 4.786700248718262\n",
      "evaluation/text/perplexity: 119.90505981445312\n",
      "time/total: 1508.4669547080994\n",
      "time/evaluation: 0.8259284496307373\n",
      "training/train_loss_mean: 4.587315022945404\n",
      "training/train_loss_std: 1.0744416139294162\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 890\n",
      "training/learning_rate: 1.2488993984720785e-05\n",
      "time/sample_batch: 0.008797168731689453\n",
      "time/training: 1.1250252723693848\n",
      "evaluation/text/loss: 4.842101097106934\n",
      "evaluation/text/perplexity: 126.73535919189453\n",
      "time/total: 1510.5897469520569\n",
      "time/evaluation: 0.9960720539093018\n",
      "training/train_loss_mean: 5.040693473815918\n",
      "training/train_loss_std: 0.3418605420965176\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 891\n",
      "training/learning_rate: 1.2442374463139218e-05\n",
      "time/sample_batch: 0.008729696273803711\n",
      "time/training: 0.8820905685424805\n",
      "evaluation/text/loss: 4.808486461639404\n",
      "evaluation/text/perplexity: 122.5459976196289\n",
      "time/total: 1511.997169494629\n",
      "time/evaluation: 0.5235071182250977\n",
      "training/train_loss_mean: 4.9299013137817385\n",
      "training/train_loss_std: 0.2605812403591957\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 892\n",
      "training/learning_rate: 1.239618349637433e-05\n",
      "time/sample_batch: 0.00998544692993164\n",
      "time/training: 1.097294569015503\n",
      "evaluation/text/loss: 4.926578044891357\n",
      "evaluation/text/perplexity: 137.90679931640625\n",
      "time/total: 1513.708295583725\n",
      "time/evaluation: 0.6122283935546875\n",
      "training/train_loss_mean: 5.144435834884644\n",
      "training/train_loss_std: 0.24469068165975383\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 893\n",
      "training/learning_rate: 1.2350421549568646e-05\n",
      "time/sample_batch: 0.008028030395507812\n",
      "time/training: 1.2507662773132324\n",
      "evaluation/text/loss: 4.60319185256958\n",
      "evaluation/text/perplexity: 99.80236053466797\n",
      "time/total: 1515.6360020637512\n",
      "time/evaluation: 0.6753346920013428\n",
      "training/train_loss_mean: 4.893014287948608\n",
      "training/train_loss_std: 0.29535298473749516\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 894\n",
      "training/learning_rate: 1.230508908354445e-05\n",
      "time/sample_batch: 0.007452964782714844\n",
      "time/training: 1.0236003398895264\n",
      "evaluation/text/loss: 4.778164386749268\n",
      "evaluation/text/perplexity: 118.88591766357422\n",
      "time/total: 1517.184556722641\n",
      "time/evaluation: 0.5232594013214111\n",
      "training/train_loss_mean: 4.7859735012054445\n",
      "training/train_loss_std: 0.8549003303527187\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 895\n",
      "training/learning_rate: 1.2260186554799193e-05\n",
      "time/sample_batch: 0.0071375370025634766\n",
      "time/training: 1.1647286415100098\n",
      "evaluation/text/loss: 4.616584777832031\n",
      "evaluation/text/perplexity: 101.14799499511719\n",
      "time/total: 1518.7949810028076\n",
      "time/evaluation: 0.4439890384674072\n",
      "training/train_loss_mean: 5.0182976722717285\n",
      "training/train_loss_std: 0.247991112899522\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 896\n",
      "training/learning_rate: 1.2215714415500815e-05\n",
      "time/sample_batch: 0.007388591766357422\n",
      "time/training: 0.8520972728729248\n",
      "evaluation/text/loss: 4.767701625823975\n",
      "evaluation/text/perplexity: 117.64852905273438\n",
      "time/total: 1520.4673204421997\n",
      "time/evaluation: 0.8185288906097412\n",
      "training/train_loss_mean: 4.769424605369568\n",
      "training/train_loss_std: 0.7636163771948484\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 897\n",
      "training/learning_rate: 1.2171673113483282e-05\n",
      "time/sample_batch: 0.006578207015991211\n",
      "time/training: 1.0203125476837158\n",
      "evaluation/text/loss: 4.70717191696167\n",
      "evaluation/text/perplexity: 110.73854064941406\n",
      "time/total: 1522.1894657611847\n",
      "time/evaluation: 0.7002277374267578\n",
      "training/train_loss_mean: 4.646721339225769\n",
      "training/train_loss_std: 0.7774151386526077\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 898\n",
      "training/learning_rate: 1.2128063092242014e-05\n",
      "time/sample_batch: 0.007417201995849609\n",
      "time/training: 1.0462391376495361\n",
      "evaluation/text/loss: 4.772104263305664\n",
      "evaluation/text/perplexity: 118.16763305664062\n",
      "time/total: 1523.8932611942291\n",
      "time/evaluation: 0.6558518409729004\n",
      "training/train_loss_mean: 5.126796293258667\n",
      "training/train_loss_std: 0.22299103152752395\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 899\n",
      "training/learning_rate: 1.2084884790929442e-05\n",
      "time/sample_batch: 0.009934425354003906\n",
      "time/training: 1.0998506546020508\n",
      "evaluation/text/loss: 4.7861714363098145\n",
      "evaluation/text/perplexity: 119.84166717529297\n",
      "time/total: 1525.6052260398865\n",
      "time/evaluation: 0.6105096340179443\n",
      "training/train_loss_mean: 4.960595226287841\n",
      "training/train_loss_std: 0.27503195255698154\n",
      "================================================================================\n",
      "Input: Thefollowingtableprovidesbasicmeteorologicalandimpactinformationforeachtropicalcyclone | Output : fromthe2003Pacifictyphoonseasonin<unk>format;unnamedtropicalcyclonesarenotincluded.PAGASAnamesforstormsareprovidedinparentheses.StormsenteringfromtheCentralPacificonlyincludetheirinformationwhileinthewesternPacific,andarenotedwithan<unk>*. | Prediction:  theel Sensor of airlines, the food channel person, with a fixed careers, amount of the scenic\n",
      "Input: <unk>specieshavealonghistoryofhuman<unk>.<unk> | Output : <unk><unk>seedshavebeenfoundinarchaeologicalsitesinColombiadatingbackto9000BP.Avarietyofspeciesremainimportantsourcesofedibleoil,<unk>,edibleseedsandfibre.Theleavesof<unk><unk>andA.<unk>areusedextensivelyfor<unk>.Severalspeciesareoil<unk>,withA.<unk>amongthemostimportanteconomically.<unk>extractedfromA.<unk>werereportedtosupportover300@,@000householdsintheBrazilianstateof<unk>in2005,andin1985itwasestimatedtosupportover450@,@000householdsthroughouttheBrazil.<unk>fibres,extractedfromtheleafbasesofA.<unk>,arecommerciallyimportant,andgeneratedaboutUS$20millioninannualincometoBrazilianfarmersin1996. | Prediction:  the wood as many on the average mastery ofónay, theimeter (uez 07 m ) and\n",
      "Input: ====<unk>ironclads= | Output : === | Prediction:  typically chosen that there is known as a <unk> ; \n",
      ", may be actions as believed\n",
      "================================================================================\n",
      "Iteration 900\n",
      "training/learning_rate: 1.2042138644350569e-05\n",
      "time/sample_batch: 0.008616209030151367\n",
      "time/training: 1.1456706523895264\n",
      "evaluation/text/loss: 4.663944721221924\n",
      "evaluation/text/perplexity: 106.0536117553711\n",
      "time/total: 1527.928851366043\n",
      "time/evaluation: 1.1762406826019287\n",
      "training/train_loss_mean: 5.188885307312011\n",
      "training/train_loss_std: 0.2955152644378359\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 901\n",
      "training/learning_rate: 1.1999825082958632e-05\n",
      "time/sample_batch: 0.007000923156738281\n",
      "time/training: 1.0756187438964844\n",
      "evaluation/text/loss: 4.7973456382751465\n",
      "evaluation/text/perplexity: 121.18831634521484\n",
      "time/total: 1529.6787180900574\n",
      "time/evaluation: 0.6729917526245117\n",
      "training/train_loss_mean: 5.146836757659912\n",
      "training/train_loss_std: 0.22168364854281042\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 902\n",
      "training/learning_rate: 1.1957944532850724e-05\n",
      "time/sample_batch: 0.0076901912689208984\n",
      "time/training: 0.9506056308746338\n",
      "evaluation/text/loss: 4.8282341957092285\n",
      "evaluation/text/perplexity: 124.99005889892578\n",
      "time/total: 1531.2187402248383\n",
      "time/evaluation: 0.5876719951629639\n",
      "training/train_loss_mean: 5.1107734680175785\n",
      "training/train_loss_std: 0.3227016676251293\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 903\n",
      "training/learning_rate: 1.1916497415763507e-05\n",
      "time/sample_batch: 0.009508848190307617\n",
      "time/training: 1.0160486698150635\n",
      "evaluation/text/loss: 4.802374839782715\n",
      "evaluation/text/perplexity: 121.79933166503906\n",
      "time/total: 1532.742821931839\n",
      "time/evaluation: 0.5062990188598633\n",
      "training/train_loss_mean: 5.199575614929199\n",
      "training/train_loss_std: 0.24553828668014216\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 904\n",
      "training/learning_rate: 1.1875484149069e-05\n",
      "time/sample_batch: 0.006211757659912109\n",
      "time/training: 1.0062274932861328\n",
      "evaluation/text/loss: 4.680170059204102\n",
      "evaluation/text/perplexity: 107.78839874267578\n",
      "time/total: 1534.3482747077942\n",
      "time/evaluation: 0.5975565910339355\n",
      "training/train_loss_mean: 4.786027097702027\n",
      "training/train_loss_std: 0.3585829495029593\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 905\n",
      "training/learning_rate: 1.1834905145770336e-05\n",
      "time/sample_batch: 0.00747227668762207\n",
      "time/training: 1.06394362449646\n",
      "evaluation/text/loss: 4.88541316986084\n",
      "evaluation/text/perplexity: 132.3451385498047\n",
      "time/total: 1536.1356947422028\n",
      "time/evaluation: 0.7218198776245117\n",
      "training/train_loss_mean: 5.10438494682312\n",
      "training/train_loss_std: 0.18743760466757173\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 906\n",
      "training/learning_rate: 1.1794760814497636e-05\n",
      "time/sample_batch: 0.007668495178222656\n",
      "time/training: 1.1413800716400146\n",
      "evaluation/text/loss: 4.850544452667236\n",
      "evaluation/text/perplexity: 127.8099594116211\n",
      "time/total: 1537.7412655353546\n",
      "time/evaluation: 0.46251606941223145\n",
      "training/train_loss_mean: 5.028281855583191\n",
      "training/train_loss_std: 0.8429930080641691\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 907\n",
      "training/learning_rate: 1.1755051559503859e-05\n",
      "time/sample_batch: 0.008668899536132812\n",
      "time/training: 1.1599228382110596\n",
      "evaluation/text/loss: 4.7435832023620605\n",
      "evaluation/text/perplexity: 114.84497833251953\n",
      "time/total: 1539.4778215885162\n",
      "time/evaluation: 0.5749349594116211\n",
      "training/train_loss_mean: 5.116376161575317\n",
      "training/train_loss_std: 0.28341105077678214\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 908\n",
      "training/learning_rate: 1.1715777780660781e-05\n",
      "time/sample_batch: 0.007972240447998047\n",
      "time/training: 1.2230308055877686\n",
      "evaluation/text/loss: 4.716525554656982\n",
      "evaluation/text/perplexity: 111.77920532226562\n",
      "time/total: 1541.208240032196\n",
      "time/evaluation: 0.5057632923126221\n",
      "training/train_loss_mean: 4.988588428497314\n",
      "training/train_loss_std: 0.32073286157931086\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 909\n",
      "training/learning_rate: 1.1676939873454898e-05\n",
      "time/sample_batch: 0.008141756057739258\n",
      "time/training: 1.0305004119873047\n",
      "evaluation/text/loss: 4.689164161682129\n",
      "evaluation/text/perplexity: 108.76223754882812\n",
      "time/total: 1542.725472688675\n",
      "time/evaluation: 0.4851045608520508\n",
      "training/train_loss_mean: 4.603107357025147\n",
      "training/train_loss_std: 1.2911200243704106\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 910\n",
      "training/learning_rate: 1.1638538228983517e-05\n",
      "time/sample_batch: 0.007892370223999023\n",
      "time/training: 1.113205909729004\n",
      "evaluation/text/loss: 4.694924354553223\n",
      "evaluation/text/perplexity: 109.39053344726562\n",
      "time/total: 1544.5014777183533\n",
      "time/evaluation: 0.6611216068267822\n",
      "training/train_loss_mean: 4.985981035232544\n",
      "training/train_loss_std: 0.3401208372252134\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 911\n",
      "training/learning_rate: 1.1600573233950773e-05\n",
      "time/sample_batch: 0.008684158325195312\n",
      "time/training: 0.9647965431213379\n",
      "evaluation/text/loss: 4.654159069061279\n",
      "evaluation/text/perplexity: 105.02086639404297\n",
      "time/total: 1546.2158932685852\n",
      "time/evaluation: 0.7480251789093018\n",
      "training/train_loss_mean: 5.236062717437744\n",
      "training/train_loss_std: 0.21866837374844078\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 912\n",
      "training/learning_rate: 1.1563045270663726e-05\n",
      "time/sample_batch: 0.008566141128540039\n",
      "time/training: 0.8966352939605713\n",
      "evaluation/text/loss: 4.847228050231934\n",
      "evaluation/text/perplexity: 127.38678741455078\n",
      "time/total: 1547.5403571128845\n",
      "time/evaluation: 0.42612123489379883\n",
      "training/train_loss_mean: 4.982665586471557\n",
      "training/train_loss_std: 0.25386008101001506\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 913\n",
      "training/learning_rate: 1.1525954717028569e-05\n",
      "time/sample_batch: 0.008835554122924805\n",
      "time/training: 0.9795253276824951\n",
      "evaluation/text/loss: 4.8959245681762695\n",
      "evaluation/text/perplexity: 133.7436065673828\n",
      "time/total: 1549.2338552474976\n",
      "time/evaluation: 0.7123661041259766\n",
      "training/train_loss_mean: 4.721427547931671\n",
      "training/train_loss_std: 1.1545462438850416\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 914\n",
      "training/learning_rate: 1.1489301946546763e-05\n",
      "time/sample_batch: 0.00806736946105957\n",
      "time/training: 1.05914306640625\n",
      "evaluation/text/loss: 4.528384208679199\n",
      "evaluation/text/perplexity: 92.60880279541016\n",
      "time/total: 1550.7602000236511\n",
      "time/evaluation: 0.4655580520629883\n",
      "training/train_loss_mean: 5.243203020095825\n",
      "training/train_loss_std: 0.27396917131882836\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 915\n",
      "training/learning_rate: 1.145308732831129e-05\n",
      "time/sample_batch: 0.008701562881469727\n",
      "time/training: 1.1397359371185303\n",
      "evaluation/text/loss: 4.589523792266846\n",
      "evaluation/text/perplexity: 98.44754028320312\n",
      "time/total: 1552.7219417095184\n",
      "time/evaluation: 0.8203880786895752\n",
      "training/train_loss_mean: 5.05743260383606\n",
      "training/train_loss_std: 0.2946619113300572\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 916\n",
      "training/learning_rate: 1.1417311227002961e-05\n",
      "time/sample_batch: 0.009621143341064453\n",
      "time/training: 1.2003746032714844\n",
      "evaluation/text/loss: 4.818563461303711\n",
      "evaluation/text/perplexity: 123.78713989257812\n",
      "time/total: 1554.4931926727295\n",
      "time/evaluation: 0.5691213607788086\n",
      "training/train_loss_mean: 5.078352260589599\n",
      "training/train_loss_std: 0.1793290900700439\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 917\n",
      "training/learning_rate: 1.138197400288672e-05\n",
      "time/sample_batch: 0.009718656539916992\n",
      "time/training: 1.231353998184204\n",
      "evaluation/text/loss: 4.832424163818359\n",
      "evaluation/text/perplexity: 125.51486206054688\n",
      "time/total: 1556.4256653785706\n",
      "time/evaluation: 0.6993310451507568\n",
      "training/train_loss_mean: 5.041989374160766\n",
      "training/train_loss_std: 0.22046579530781826\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 918\n",
      "training/learning_rate: 1.1347076011808015e-05\n",
      "time/sample_batch: 0.00976109504699707\n",
      "time/training: 1.0809745788574219\n",
      "evaluation/text/loss: 4.812680721282959\n",
      "evaluation/text/perplexity: 123.06106567382812\n",
      "time/total: 1558.1179115772247\n",
      "time/evaluation: 0.6095428466796875\n",
      "training/train_loss_mean: 5.055833959579468\n",
      "training/train_loss_std: 0.4655772927507972\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 919\n",
      "training/learning_rate: 1.131261760518924e-05\n",
      "time/sample_batch: 0.008812189102172852\n",
      "time/training: 0.9998400211334229\n",
      "evaluation/text/loss: 4.673356056213379\n",
      "evaluation/text/perplexity: 107.05642700195312\n",
      "time/total: 1559.586398601532\n",
      "time/evaluation: 0.46699976921081543\n",
      "training/train_loss_mean: 4.919501829147339\n",
      "training/train_loss_std: 0.7267205768617847\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 920\n",
      "training/learning_rate: 1.127859913002615e-05\n",
      "time/sample_batch: 0.007887840270996094\n",
      "time/training: 0.8532512187957764\n",
      "evaluation/text/loss: 4.739645957946777\n",
      "evaluation/text/perplexity: 114.39369201660156\n",
      "time/total: 1561.2614533901215\n",
      "time/evaluation: 0.8200819492340088\n",
      "training/train_loss_mean: 4.874986863136291\n",
      "training/train_loss_std: 0.38111095131283523\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 921\n",
      "training/learning_rate: 1.1245020928884434e-05\n",
      "time/sample_batch: 0.005778789520263672\n",
      "time/training: 0.9299056529998779\n",
      "evaluation/text/loss: 4.892953872680664\n",
      "evaluation/text/perplexity: 133.3468780517578\n",
      "time/total: 1562.9843804836273\n",
      "time/evaluation: 0.7912266254425049\n",
      "training/train_loss_mean: 4.891951179504394\n",
      "training/train_loss_std: 0.4047080678221633\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 922\n",
      "training/learning_rate: 1.1211883339896185e-05\n",
      "time/sample_batch: 0.009411334991455078\n",
      "time/training: 1.0830192565917969\n",
      "evaluation/text/loss: 4.817296504974365\n",
      "evaluation/text/perplexity: 123.63040161132812\n",
      "time/total: 1564.7282831668854\n",
      "time/evaluation: 0.6592381000518799\n",
      "training/train_loss_mean: 5.16205530166626\n",
      "training/train_loss_std: 0.2450931451551559\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 923\n",
      "training/learning_rate: 1.1179186696756561e-05\n",
      "time/sample_batch: 0.007697582244873047\n",
      "time/training: 1.0742700099945068\n",
      "evaluation/text/loss: 4.8934807777404785\n",
      "evaluation/text/perplexity: 133.4171600341797\n",
      "time/total: 1566.6213772296906\n",
      "time/evaluation: 0.8172190189361572\n",
      "training/train_loss_mean: 5.079299402236939\n",
      "training/train_loss_std: 0.1977515219920536\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 924\n",
      "training/learning_rate: 1.1146931328720408e-05\n",
      "time/sample_batch: 0.00889897346496582\n",
      "time/training: 1.0895302295684814\n",
      "evaluation/text/loss: 4.6469573974609375\n",
      "evaluation/text/perplexity: 104.26725769042969\n",
      "time/total: 1568.3099150657654\n",
      "time/evaluation: 0.5973334312438965\n",
      "training/train_loss_mean: 5.140232563018799\n",
      "training/train_loss_std: 0.3663793550608433\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 925\n",
      "training/learning_rate: 1.1115117560598906e-05\n",
      "time/sample_batch: 0.007035017013549805\n",
      "time/training: 0.9581561088562012\n",
      "evaluation/text/loss: 4.849977493286133\n",
      "evaluation/text/perplexity: 127.73751831054688\n",
      "time/total: 1569.8808329105377\n",
      "time/evaluation: 0.6111068725585938\n",
      "training/train_loss_mean: 5.064143991470337\n",
      "training/train_loss_std: 0.3146243935900014\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 926\n",
      "training/learning_rate: 1.1083745712756362e-05\n",
      "time/sample_batch: 0.008875131607055664\n",
      "time/training: 1.047797679901123\n",
      "evaluation/text/loss: 4.692781925201416\n",
      "evaluation/text/perplexity: 109.15642547607422\n",
      "time/total: 1571.67657828331\n",
      "time/evaluation: 0.7464339733123779\n",
      "training/train_loss_mean: 5.074933958053589\n",
      "training/train_loss_std: 0.35933968825059487\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 927\n",
      "training/learning_rate: 1.1052816101106925e-05\n",
      "time/sample_batch: 0.008726119995117188\n",
      "time/training: 1.0225634574890137\n",
      "evaluation/text/loss: 4.811895370483398\n",
      "evaluation/text/perplexity: 122.96446228027344\n",
      "time/total: 1573.69340467453\n",
      "time/evaluation: 0.9927186965942383\n",
      "training/train_loss_mean: 5.147658824920654\n",
      "training/train_loss_std: 0.22456721479323558\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 928\n",
      "training/learning_rate: 1.1022329037111448e-05\n",
      "time/sample_batch: 0.007246732711791992\n",
      "time/training: 1.0437896251678467\n",
      "evaluation/text/loss: 4.818594932556152\n",
      "evaluation/text/perplexity: 123.79103088378906\n",
      "time/total: 1575.3981492519379\n",
      "time/evaluation: 0.6542661190032959\n",
      "training/train_loss_mean: 5.032156848907471\n",
      "training/train_loss_std: 0.18140520532074428\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 929\n",
      "training/learning_rate: 1.0992284827774321e-05\n",
      "time/sample_batch: 0.011254072189331055\n",
      "time/training: 1.2891826629638672\n",
      "evaluation/text/loss: 4.8931756019592285\n",
      "evaluation/text/perplexity: 133.37644958496094\n",
      "time/total: 1577.4399182796478\n",
      "time/evaluation: 0.7510569095611572\n",
      "training/train_loss_mean: 5.058345556259155\n",
      "training/train_loss_std: 0.21299607751379937\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 930\n",
      "training/learning_rate: 1.0962683775640407e-05\n",
      "time/sample_batch: 0.009048223495483398\n",
      "time/training: 1.0931923389434814\n",
      "evaluation/text/loss: 4.835574626922607\n",
      "evaluation/text/perplexity: 125.9109115600586\n",
      "time/total: 1579.1393222808838\n",
      "time/evaluation: 0.604515552520752\n",
      "training/train_loss_mean: 4.978704500198364\n",
      "training/train_loss_std: 0.2312614301397222\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 931\n",
      "training/learning_rate: 1.0933526178791956e-05\n",
      "time/sample_batch: 0.009087800979614258\n",
      "time/training: 1.1069016456604004\n",
      "evaluation/text/loss: 4.630793571472168\n",
      "evaluation/text/perplexity: 102.59545135498047\n",
      "time/total: 1580.7917249202728\n",
      "time/evaluation: 0.5439255237579346\n",
      "training/train_loss_mean: 5.091095066070556\n",
      "training/train_loss_std: 0.32572645122538263\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 932\n",
      "training/learning_rate: 1.0904812330845662e-05\n",
      "time/sample_batch: 0.008612871170043945\n",
      "time/training: 1.2875823974609375\n",
      "evaluation/text/loss: 4.660326957702637\n",
      "evaluation/text/perplexity: 105.67062377929688\n",
      "time/total: 1582.5758090019226\n",
      "time/evaluation: 0.4949381351470947\n",
      "training/train_loss_mean: 5.024614000320435\n",
      "training/train_loss_std: 0.1889147289880408\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 933\n",
      "training/learning_rate: 1.0876542520949643e-05\n",
      "time/sample_batch: 0.00793600082397461\n",
      "time/training: 1.1562445163726807\n",
      "evaluation/text/loss: 4.724791049957275\n",
      "evaluation/text/perplexity: 112.70694732666016\n",
      "time/total: 1584.3073182106018\n",
      "time/evaluation: 0.5737254619598389\n",
      "training/train_loss_mean: 4.94856641292572\n",
      "training/train_loss_std: 0.6330044315591454\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 934\n",
      "training/learning_rate: 1.0848717033780582e-05\n",
      "time/sample_batch: 0.010132789611816406\n",
      "time/training: 1.1077525615692139\n",
      "evaluation/text/loss: 4.813960552215576\n",
      "evaluation/text/perplexity: 123.21866607666016\n",
      "time/total: 1586.1110517978668\n",
      "time/evaluation: 0.6943132877349854\n",
      "training/train_loss_mean: 5.08611044883728\n",
      "training/train_loss_std: 0.22312641473966752\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 935\n",
      "training/learning_rate: 1.0821336149540817e-05\n",
      "time/sample_batch: 0.008571863174438477\n",
      "time/training: 1.2212867736816406\n",
      "evaluation/text/loss: 4.76234245300293\n",
      "evaluation/text/perplexity: 117.01972198486328\n",
      "time/total: 1587.9509196281433\n",
      "time/evaluation: 0.6168296337127686\n",
      "training/train_loss_mean: 4.9579612731933596\n",
      "training/train_loss_std: 0.3180100849493219\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 936\n",
      "training/learning_rate: 1.0794400143955551e-05\n",
      "time/sample_batch: 0.00796818733215332\n",
      "time/training: 1.0308620929718018\n",
      "evaluation/text/loss: 4.761905193328857\n",
      "evaluation/text/perplexity: 116.96855926513672\n",
      "time/total: 1589.5162754058838\n",
      "time/evaluation: 0.5327672958374023\n",
      "training/train_loss_mean: 5.006787347793579\n",
      "training/train_loss_std: 0.2888102483746515\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 937\n",
      "training/learning_rate: 1.076790928827006e-05\n",
      "time/sample_batch: 0.007941007614135742\n",
      "time/training: 0.9147815704345703\n",
      "evaluation/text/loss: 4.70949125289917\n",
      "evaluation/text/perplexity: 110.99567413330078\n",
      "time/total: 1591.0815086364746\n",
      "time/evaluation: 0.6489741802215576\n",
      "training/train_loss_mean: 5.002850770950317\n",
      "training/train_loss_std: 0.20099304522953984\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 938\n",
      "training/learning_rate: 1.0741863849246964e-05\n",
      "time/sample_batch: 0.009410381317138672\n",
      "time/training: 0.9922947883605957\n",
      "evaluation/text/loss: 4.483689785003662\n",
      "evaluation/text/perplexity: 88.56084442138672\n",
      "time/total: 1592.620412349701\n",
      "time/evaluation: 0.5450432300567627\n",
      "training/train_loss_mean: 4.882902717590332\n",
      "training/train_loss_std: 0.4239081723417403\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 939\n",
      "training/learning_rate: 1.0716264089163558e-05\n",
      "time/sample_batch: 0.007193088531494141\n",
      "time/training: 1.0659456253051758\n",
      "evaluation/text/loss: 4.674465656280518\n",
      "evaluation/text/perplexity: 107.17528533935547\n",
      "time/total: 1594.3534970283508\n",
      "time/evaluation: 0.6656067371368408\n",
      "training/train_loss_mean: 5.169372320175171\n",
      "training/train_loss_std: 0.23392896100366423\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 940\n",
      "training/learning_rate: 1.069111026580913e-05\n",
      "time/sample_batch: 0.009771585464477539\n",
      "time/training: 1.207068681716919\n",
      "evaluation/text/loss: 4.665872097015381\n",
      "evaluation/text/perplexity: 106.25820922851562\n",
      "time/total: 1596.0890192985535\n",
      "time/evaluation: 0.5267255306243896\n",
      "training/train_loss_mean: 5.269388341903687\n",
      "training/train_loss_std: 0.2937008805100887\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 941\n",
      "training/learning_rate: 1.0666402632482402e-05\n",
      "time/sample_batch: 0.00862431526184082\n",
      "time/training: 1.0541276931762695\n",
      "evaluation/text/loss: 4.8070454597473145\n",
      "evaluation/text/perplexity: 122.36953735351562\n",
      "time/total: 1597.7432191371918\n",
      "time/evaluation: 0.5984234809875488\n",
      "training/train_loss_mean: 4.873979139328003\n",
      "training/train_loss_std: 0.33951499211812963\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 942\n",
      "training/learning_rate: 1.0642141437988972e-05\n",
      "time/sample_batch: 0.008091211318969727\n",
      "time/training: 1.052910566329956\n",
      "evaluation/text/loss: 4.696184158325195\n",
      "evaluation/text/perplexity: 109.52843475341797\n",
      "time/total: 1599.5978484153748\n",
      "time/evaluation: 0.8001811504364014\n",
      "training/train_loss_mean: 5.050143909454346\n",
      "training/train_loss_std: 0.278441789507224\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 943\n",
      "training/learning_rate: 1.0618326926638793e-05\n",
      "time/sample_batch: 0.007402658462524414\n",
      "time/training: 1.1587982177734375\n",
      "evaluation/text/loss: 4.63962459564209\n",
      "evaluation/text/perplexity: 103.50548553466797\n",
      "time/total: 1601.350382566452\n",
      "time/evaluation: 0.5920004844665527\n",
      "training/train_loss_mean: 5.012822103500366\n",
      "training/train_loss_std: 0.34826242648969535\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 944\n",
      "training/learning_rate: 1.0594959338243734e-05\n",
      "time/sample_batch: 0.008478164672851562\n",
      "time/training: 1.1263113021850586\n",
      "evaluation/text/loss: 4.724833965301514\n",
      "evaluation/text/perplexity: 112.71178436279297\n",
      "time/total: 1603.058287858963\n",
      "time/evaluation: 0.5800282955169678\n",
      "training/train_loss_mean: 5.022928237915039\n",
      "training/train_loss_std: 0.3601260411856446\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 945\n",
      "training/learning_rate: 1.0572038908115157e-05\n",
      "time/sample_batch: 0.006923198699951172\n",
      "time/training: 1.0211033821105957\n",
      "evaluation/text/loss: 4.702942371368408\n",
      "evaluation/text/perplexity: 110.27115631103516\n",
      "time/total: 1604.6570253372192\n",
      "time/evaluation: 0.5759892463684082\n",
      "training/train_loss_mean: 5.022602176666259\n",
      "training/train_loss_std: 0.28382151155703217\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 946\n",
      "training/learning_rate: 1.0549565867061528e-05\n",
      "time/sample_batch: 0.007862329483032227\n",
      "time/training: 1.1133744716644287\n",
      "evaluation/text/loss: 4.835953235626221\n",
      "evaluation/text/perplexity: 125.9585952758789\n",
      "time/total: 1606.3693006038666\n",
      "time/evaluation: 0.5973823070526123\n",
      "training/train_loss_mean: 5.15974645614624\n",
      "training/train_loss_std: 0.24918206060456777\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 947\n",
      "training/learning_rate: 1.052754044138613e-05\n",
      "time/sample_batch: 0.008708000183105469\n",
      "time/training: 0.9397850036621094\n",
      "evaluation/text/loss: 4.832762718200684\n",
      "evaluation/text/perplexity: 125.55735778808594\n",
      "time/total: 1607.8884136676788\n",
      "time/evaluation: 0.5777225494384766\n",
      "training/train_loss_mean: 4.906482553482055\n",
      "training/train_loss_std: 0.25335359376741096\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 948\n",
      "training/learning_rate: 1.0505962852884739e-05\n",
      "time/sample_batch: 0.008227825164794922\n",
      "time/training: 1.02695894241333\n",
      "evaluation/text/loss: 4.789923191070557\n",
      "evaluation/text/perplexity: 120.29212951660156\n",
      "time/total: 1609.4222784042358\n",
      "time/evaluation: 0.5052423477172852\n",
      "training/train_loss_mean: 4.770780372619629\n",
      "training/train_loss_std: 0.5946020596296089\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 949\n",
      "training/learning_rate: 1.0484833318843454e-05\n",
      "time/sample_batch: 0.008640050888061523\n",
      "time/training: 1.1539428234100342\n",
      "evaluation/text/loss: 4.650791645050049\n",
      "evaluation/text/perplexity: 104.66781616210938\n",
      "time/total: 1611.2303218841553\n",
      "time/evaluation: 0.6521725654602051\n",
      "training/train_loss_mean: 5.165241765975952\n",
      "training/train_loss_std: 0.22019451084504343\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 950\n",
      "training/learning_rate: 1.0464152052036435e-05\n",
      "time/sample_batch: 0.0071163177490234375\n",
      "time/training: 0.9401962757110596\n",
      "evaluation/text/loss: 4.762477397918701\n",
      "evaluation/text/perplexity: 117.03550720214844\n",
      "time/total: 1612.95095038414\n",
      "time/evaluation: 0.7788481712341309\n",
      "training/train_loss_mean: 5.060053205490112\n",
      "training/train_loss_std: 0.21654203740201747\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 951\n",
      "training/learning_rate: 1.0443919260723808e-05\n",
      "time/sample_batch: 0.008114814758300781\n",
      "time/training: 0.989628791809082\n",
      "evaluation/text/loss: 4.791994094848633\n",
      "evaluation/text/perplexity: 120.54150390625\n",
      "time/total: 1614.6121480464935\n",
      "time/evaluation: 0.6697626113891602\n",
      "training/train_loss_mean: 5.200603008270264\n",
      "training/train_loss_std: 0.20440468984942495\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 952\n",
      "training/learning_rate: 1.0424135148649562e-05\n",
      "time/sample_batch: 0.008887767791748047\n",
      "time/training: 1.0907127857208252\n",
      "evaluation/text/loss: 4.661802291870117\n",
      "evaluation/text/perplexity: 105.8266372680664\n",
      "time/total: 1616.1789619922638\n",
      "time/evaluation: 0.47443461418151855\n",
      "training/train_loss_mean: 4.9739949941635135\n",
      "training/train_loss_std: 0.49975257911421356\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 953\n",
      "training/learning_rate: 1.0404799915039486e-05\n",
      "time/sample_batch: 0.009263753890991211\n",
      "time/training: 1.2337226867675781\n",
      "evaluation/text/loss: 4.569798946380615\n",
      "evaluation/text/perplexity: 96.52470397949219\n",
      "time/total: 1617.96093583107\n",
      "time/evaluation: 0.5464417934417725\n",
      "training/train_loss_mean: 4.995899057388305\n",
      "training/train_loss_std: 0.3196941975401674\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 954\n",
      "training/learning_rate: 1.0385913754599166e-05\n",
      "time/sample_batch: 0.009714603424072266\n",
      "time/training: 1.1186697483062744\n",
      "evaluation/text/loss: 4.739044666290283\n",
      "evaluation/text/perplexity: 114.3249282836914\n",
      "time/total: 1619.6479375362396\n",
      "time/evaluation: 0.5666828155517578\n",
      "training/train_loss_mean: 4.866015076637268\n",
      "training/train_loss_std: 0.4052101498068079\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 955\n",
      "training/learning_rate: 1.0367476857512025e-05\n",
      "time/sample_batch: 0.008237361907958984\n",
      "time/training: 1.1301615238189697\n",
      "evaluation/text/loss: 4.750466823577881\n",
      "evaluation/text/perplexity: 115.63825225830078\n",
      "time/total: 1621.3696229457855\n",
      "time/evaluation: 0.5898551940917969\n",
      "training/train_loss_mean: 4.86988582611084\n",
      "training/train_loss_std: 0.3734857429515192\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 956\n",
      "training/learning_rate: 1.0349489409437415e-05\n",
      "time/sample_batch: 0.0072290897369384766\n",
      "time/training: 1.1398475170135498\n",
      "evaluation/text/loss: 4.7402873039245605\n",
      "evaluation/text/perplexity: 114.46708679199219\n",
      "time/total: 1623.2578613758087\n",
      "time/evaluation: 0.746523380279541\n",
      "training/train_loss_mean: 5.138422012329102\n",
      "training/train_loss_std: 0.2910816190611003\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 957\n",
      "training/learning_rate: 1.0331951591508736e-05\n",
      "time/sample_batch: 0.007559299468994141\n",
      "time/training: 1.0590062141418457\n",
      "evaluation/text/loss: 4.902904510498047\n",
      "evaluation/text/perplexity: 134.68038940429688\n",
      "time/total: 1624.7980153560638\n",
      "time/evaluation: 0.47937870025634766\n",
      "training/train_loss_mean: 4.958686208724975\n",
      "training/train_loss_std: 0.3109236772834544\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 958\n",
      "training/learning_rate: 1.0314863580331634e-05\n",
      "time/sample_batch: 0.007634878158569336\n",
      "time/training: 1.204878807067871\n",
      "evaluation/text/loss: 4.855564117431641\n",
      "evaluation/text/perplexity: 128.45314025878906\n",
      "time/total: 1626.997965335846\n",
      "time/evaluation: 0.9934229850769043\n",
      "training/train_loss_mean: 4.9971351146698\n",
      "training/train_loss_std: 0.22850344985179677\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 959\n",
      "training/learning_rate: 1.029822554798217e-05\n",
      "time/sample_batch: 0.009167194366455078\n",
      "time/training: 1.028959035873413\n",
      "evaluation/text/loss: 4.85491943359375\n",
      "evaluation/text/perplexity: 128.37034606933594\n",
      "time/total: 1628.772890329361\n",
      "time/evaluation: 0.7443726062774658\n",
      "training/train_loss_mean: 5.151403284072876\n",
      "training/train_loss_std: 0.23705874930244228\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 960\n",
      "training/learning_rate: 1.0282037662005155e-05\n",
      "time/sample_batch: 0.007983684539794922\n",
      "time/training: 1.0261650085449219\n",
      "evaluation/text/loss: 4.747140407562256\n",
      "evaluation/text/perplexity: 115.25423431396484\n",
      "time/total: 1630.544112920761\n",
      "time/evaluation: 0.7434985637664795\n",
      "training/train_loss_mean: 5.014330530166626\n",
      "training/train_loss_std: 0.31008881396644944\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 961\n",
      "training/learning_rate: 1.0266300085412419e-05\n",
      "time/sample_batch: 0.0071086883544921875\n",
      "time/training: 1.0109422206878662\n",
      "evaluation/text/loss: 4.849096298217773\n",
      "evaluation/text/perplexity: 127.625\n",
      "time/total: 1632.1308438777924\n",
      "time/evaluation: 0.5742731094360352\n",
      "training/train_loss_mean: 4.7803908586502075\n",
      "training/train_loss_std: 0.5125050885171163\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 962\n",
      "training/learning_rate: 1.0251012976681196e-05\n",
      "time/sample_batch: 0.008998394012451172\n",
      "time/training: 1.1229684352874756\n",
      "evaluation/text/loss: 4.826656818389893\n",
      "evaluation/text/perplexity: 124.79306030273438\n",
      "time/total: 1633.935536623001\n",
      "time/evaluation: 0.679926872253418\n",
      "training/train_loss_mean: 5.051906108856201\n",
      "training/train_loss_std: 0.20970163983960288\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 963\n",
      "training/learning_rate: 1.0236176489752486e-05\n",
      "time/sample_batch: 0.010598897933959961\n",
      "time/training: 1.0095584392547607\n",
      "evaluation/text/loss: 4.959404468536377\n",
      "evaluation/text/perplexity: 142.50889587402344\n",
      "time/total: 1635.754281282425\n",
      "time/evaluation: 0.8074791431427002\n",
      "training/train_loss_mean: 5.0694914817810055\n",
      "training/train_loss_std: 0.24931313854671677\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 964\n",
      "training/learning_rate: 1.0221790774029553e-05\n",
      "time/sample_batch: 0.008436918258666992\n",
      "time/training: 1.0555250644683838\n",
      "evaluation/text/loss: 4.770117282867432\n",
      "evaluation/text/perplexity: 117.93307495117188\n",
      "time/total: 1637.3763191699982\n",
      "time/evaluation: 0.564849853515625\n",
      "training/train_loss_mean: 5.047966241836548\n",
      "training/train_loss_std: 0.298594262384702\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 965\n",
      "training/learning_rate: 1.0207855974376392e-05\n",
      "time/sample_batch: 0.00825190544128418\n",
      "time/training: 1.1437969207763672\n",
      "evaluation/text/loss: 4.6763739585876465\n",
      "evaluation/text/perplexity: 107.3800048828125\n",
      "time/total: 1639.3411965370178\n",
      "time/evaluation: 0.8193433284759521\n",
      "training/train_loss_mean: 4.916369342803955\n",
      "training/train_loss_std: 0.3048073053092177\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 966\n",
      "training/learning_rate: 1.0194372231116287e-05\n",
      "time/sample_batch: 0.0065000057220458984\n",
      "time/training: 1.0203497409820557\n",
      "evaluation/text/loss: 4.604974746704102\n",
      "evaluation/text/perplexity: 99.98046112060547\n",
      "time/total: 1640.8852081298828\n",
      "time/evaluation: 0.5220448970794678\n",
      "training/train_loss_mean: 5.0729917049407955\n",
      "training/train_loss_std: 0.21224719128130698\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 967\n",
      "training/learning_rate: 1.0181339680030371e-05\n",
      "time/sample_batch: 0.007435321807861328\n",
      "time/training: 1.0826261043548584\n",
      "evaluation/text/loss: 4.6339521408081055\n",
      "evaluation/text/perplexity: 102.92001342773438\n",
      "time/total: 1642.5775663852692\n",
      "time/evaluation: 0.6034777164459229\n",
      "training/train_loss_mean: 5.0307718276977536\n",
      "training/train_loss_std: 0.1958290569751604\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 968\n",
      "training/learning_rate: 1.0168758452356302e-05\n",
      "time/sample_batch: 0.009383440017700195\n",
      "time/training: 1.1095890998840332\n",
      "evaluation/text/loss: 4.755045413970947\n",
      "evaluation/text/perplexity: 116.16893005371094\n",
      "time/total: 1644.3011682033539\n",
      "time/evaluation: 0.6124601364135742\n",
      "training/train_loss_mean: 5.03275990486145\n",
      "training/train_loss_std: 0.2115528655269532\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 969\n",
      "training/learning_rate: 1.0156628674786897e-05\n",
      "time/sample_batch: 0.00921940803527832\n",
      "time/training: 1.1459593772888184\n",
      "evaluation/text/loss: 4.655808448791504\n",
      "evaluation/text/perplexity: 105.19422912597656\n",
      "time/total: 1645.92440867424\n",
      "time/evaluation: 0.47550439834594727\n",
      "training/train_loss_mean: 5.194725036621094\n",
      "training/train_loss_std: 0.19987327103576707\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 970\n",
      "training/learning_rate: 1.0144950469468885e-05\n",
      "time/sample_batch: 0.006769895553588867\n",
      "time/training: 1.18888258934021\n",
      "evaluation/text/loss: 4.804454326629639\n",
      "evaluation/text/perplexity: 122.05287170410156\n",
      "time/total: 1647.626968383789\n",
      "time/evaluation: 0.5119750499725342\n",
      "training/train_loss_mean: 5.1543293476104735\n",
      "training/train_loss_std: 0.3001501105939157\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 971\n",
      "training/learning_rate: 1.0133723954001655e-05\n",
      "time/sample_batch: 0.009994029998779297\n",
      "time/training: 1.1539056301116943\n",
      "evaluation/text/loss: 4.743894577026367\n",
      "evaluation/text/perplexity: 114.88074493408203\n",
      "time/total: 1649.2503526210785\n",
      "time/evaluation: 0.46782398223876953\n",
      "training/train_loss_mean: 5.027653646469116\n",
      "training/train_loss_std: 0.3077556120577466\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 972\n",
      "training/learning_rate: 1.0122949241436105e-05\n",
      "time/sample_batch: 0.009253740310668945\n",
      "time/training: 1.0260443687438965\n",
      "evaluation/text/loss: 4.760377883911133\n",
      "evaluation/text/perplexity: 116.79005432128906\n",
      "time/total: 1650.8565692901611\n",
      "time/evaluation: 0.5784754753112793\n",
      "training/train_loss_mean: 4.952297973632812\n",
      "training/train_loss_std: 0.23533828019265848\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 973\n",
      "training/learning_rate: 1.011262644027346e-05\n",
      "time/sample_batch: 0.008935213088989258\n",
      "time/training: 1.035463809967041\n",
      "evaluation/text/loss: 4.806164741516113\n",
      "evaluation/text/perplexity: 122.26181030273438\n",
      "time/total: 1652.6616554260254\n",
      "time/evaluation: 0.7679016590118408\n",
      "training/train_loss_mean: 5.243307638168335\n",
      "training/train_loss_std: 0.21434742763245718\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 974\n",
      "training/learning_rate: 1.0102755654464221e-05\n",
      "time/sample_batch: 0.007760286331176758\n",
      "time/training: 1.2603516578674316\n",
      "evaluation/text/loss: 4.641767501831055\n",
      "evaluation/text/perplexity: 103.72752380371094\n",
      "time/total: 1654.4555134773254\n",
      "time/evaluation: 0.5318994522094727\n",
      "training/train_loss_mean: 4.99242582321167\n",
      "training/train_loss_std: 0.21975978986052994\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 975\n",
      "training/learning_rate: 1.0093336983407089e-05\n",
      "time/sample_batch: 0.00759434700012207\n",
      "time/training: 1.209444284439087\n",
      "evaluation/text/loss: 4.713499069213867\n",
      "evaluation/text/perplexity: 111.44142150878906\n",
      "time/total: 1656.3432943820953\n",
      "time/evaluation: 0.6766319274902344\n",
      "training/train_loss_mean: 5.064551782608032\n",
      "training/train_loss_std: 0.14334792041581493\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 976\n",
      "training/learning_rate: 1.0084370521947974e-05\n",
      "time/sample_batch: 0.008749246597290039\n",
      "time/training: 1.1501421928405762\n",
      "evaluation/text/loss: 4.765972137451172\n",
      "evaluation/text/perplexity: 117.44523620605469\n",
      "time/total: 1658.0068728923798\n",
      "time/evaluation: 0.5117340087890625\n",
      "training/train_loss_mean: 5.08312611579895\n",
      "training/train_loss_std: 0.2785010122508286\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 977\n",
      "training/learning_rate: 1.0075856360379052e-05\n",
      "time/sample_batch: 0.008677005767822266\n",
      "time/training: 0.9961199760437012\n",
      "evaluation/text/loss: 4.779502868652344\n",
      "evaluation/text/perplexity: 119.04515075683594\n",
      "time/total: 1659.7057175636292\n",
      "time/evaluation: 0.7010350227355957\n",
      "training/train_loss_mean: 5.079882478713989\n",
      "training/train_loss_std: 0.29090212979825314\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 978\n",
      "training/learning_rate: 1.0067794584437821e-05\n",
      "time/sample_batch: 0.005726814270019531\n",
      "time/training: 1.1575243473052979\n",
      "evaluation/text/loss: 4.885890007019043\n",
      "evaluation/text/perplexity: 132.40826416015625\n",
      "time/total: 1661.6074872016907\n",
      "time/evaluation: 0.7425234317779541\n",
      "training/train_loss_mean: 4.713225078582764\n",
      "training/train_loss_std: 0.844150683618833\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 979\n",
      "training/learning_rate: 1.006018527530629e-05\n",
      "time/sample_batch: 0.007595539093017578\n",
      "time/training: 1.1190757751464844\n",
      "evaluation/text/loss: 4.889003276824951\n",
      "evaluation/text/perplexity: 132.8211212158203\n",
      "time/total: 1663.508855342865\n",
      "time/evaluation: 0.7806894779205322\n",
      "training/train_loss_mean: 5.080198955535889\n",
      "training/train_loss_std: 0.3293511719200251\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 980\n",
      "training/learning_rate: 1.005302850961011e-05\n",
      "time/sample_batch: 0.011257648468017578\n",
      "time/training: 1.201916217803955\n",
      "evaluation/text/loss: 4.691682815551758\n",
      "evaluation/text/perplexity: 109.03651428222656\n",
      "time/total: 1665.1526420116425\n",
      "time/evaluation: 0.440157413482666\n",
      "training/train_loss_mean: 4.96194896697998\n",
      "training/train_loss_std: 0.39083054367505277\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 981\n",
      "training/learning_rate: 1.0046324359417838e-05\n",
      "time/sample_batch: 0.00932765007019043\n",
      "time/training: 1.0666067600250244\n",
      "evaluation/text/loss: 4.68972110748291\n",
      "evaluation/text/perplexity: 108.82282257080078\n",
      "time/total: 1666.6792905330658\n",
      "time/evaluation: 0.45842885971069336\n",
      "training/train_loss_mean: 5.053924798965454\n",
      "training/train_loss_std: 0.44767370831275527\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 982\n",
      "training/learning_rate: 1.0040072892240177e-05\n",
      "time/sample_batch: 0.007756948471069336\n",
      "time/training: 1.0349085330963135\n",
      "evaluation/text/loss: 4.800253391265869\n",
      "evaluation/text/perplexity: 121.54121398925781\n",
      "time/total: 1668.3828809261322\n",
      "time/evaluation: 0.6671128273010254\n",
      "training/train_loss_mean: 5.096450424194336\n",
      "training/train_loss_std: 0.2976688290460383\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 983\n",
      "training/learning_rate: 1.0034274171029349e-05\n",
      "time/sample_batch: 0.007512092590332031\n",
      "time/training: 1.0250499248504639\n",
      "evaluation/text/loss: 4.816910743713379\n",
      "evaluation/text/perplexity: 123.58272552490234\n",
      "time/total: 1670.2378849983215\n",
      "time/evaluation: 0.8284246921539307\n",
      "training/train_loss_mean: 4.939396333694458\n",
      "training/train_loss_std: 0.3696541155546034\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 984\n",
      "training/learning_rate: 1.0028928254178407e-05\n",
      "time/sample_batch: 0.007917642593383789\n",
      "time/training: 0.8297684192657471\n",
      "evaluation/text/loss: 4.7173237800598145\n",
      "evaluation/text/perplexity: 111.86846923828125\n",
      "time/total: 1671.6661784648895\n",
      "time/evaluation: 0.5968968868255615\n",
      "training/train_loss_mean: 4.792749977111816\n",
      "training/train_loss_std: 0.6736385686366317\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 985\n",
      "training/learning_rate: 1.0024035195520671e-05\n",
      "time/sample_batch: 0.0075016021728515625\n",
      "time/training: 0.9973771572113037\n",
      "evaluation/text/loss: 4.828272342681885\n",
      "evaluation/text/perplexity: 124.99482727050781\n",
      "time/total: 1673.1303112506866\n",
      "time/evaluation: 0.46489429473876953\n",
      "training/train_loss_mean: 5.06544189453125\n",
      "training/train_loss_std: 0.2196798579644832\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 986\n",
      "training/learning_rate: 1.0019595044329186e-05\n",
      "time/sample_batch: 0.01167917251586914\n",
      "time/training: 1.3215303421020508\n",
      "evaluation/text/loss: 4.6162109375\n",
      "evaluation/text/perplexity: 101.11019134521484\n",
      "time/total: 1675.1823120117188\n",
      "time/evaluation: 0.7285480499267578\n",
      "training/train_loss_mean: 5.137619352340698\n",
      "training/train_loss_std: 0.23996623809543197\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 987\n",
      "training/learning_rate: 1.0015607845316235e-05\n",
      "time/sample_batch: 0.006891965866088867\n",
      "time/training: 0.9016118049621582\n",
      "evaluation/text/loss: 4.774912357330322\n",
      "evaluation/text/perplexity: 118.49993133544922\n",
      "time/total: 1676.830701828003\n",
      "time/evaluation: 0.7451796531677246\n",
      "training/train_loss_mean: 4.734252572059631\n",
      "training/train_loss_std: 0.751738075051416\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 988\n",
      "training/learning_rate: 1.001207363863286e-05\n",
      "time/sample_batch: 0.007646083831787109\n",
      "time/training: 0.9632422924041748\n",
      "evaluation/text/loss: 4.661035060882568\n",
      "evaluation/text/perplexity: 105.74547576904297\n",
      "time/total: 1678.5388669967651\n",
      "time/evaluation: 0.7432858943939209\n",
      "training/train_loss_mean: 5.0098857402801515\n",
      "training/train_loss_std: 0.2686967321787815\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 989\n",
      "training/learning_rate: 1.000899245986848e-05\n",
      "time/sample_batch: 0.007566213607788086\n",
      "time/training: 1.0508835315704346\n",
      "evaluation/text/loss: 4.729589462280273\n",
      "evaluation/text/perplexity: 113.24906158447266\n",
      "time/total: 1680.1359496116638\n",
      "time/evaluation: 0.5444955825805664\n",
      "training/train_loss_mean: 5.079871892929077\n",
      "training/train_loss_std: 0.23149026895399435\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 990\n",
      "training/learning_rate: 1.000636434005054e-05\n",
      "time/sample_batch: 0.011675596237182617\n",
      "time/training: 1.1721646785736084\n",
      "evaluation/text/loss: 4.826308727264404\n",
      "evaluation/text/perplexity: 124.74962615966797\n",
      "time/total: 1682.0966174602509\n",
      "time/evaluation: 0.7868883609771729\n",
      "training/train_loss_mean: 5.020167016983033\n",
      "training/train_loss_std: 0.3823628144063289\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 991\n",
      "training/learning_rate: 1.0004189305644173e-05\n",
      "time/sample_batch: 0.008966207504272461\n",
      "time/training: 1.1051135063171387\n",
      "evaluation/text/loss: 4.649628639221191\n",
      "evaluation/text/perplexity: 104.54615783691406\n",
      "time/total: 1683.7598016262054\n",
      "time/evaluation: 0.5564589500427246\n",
      "training/train_loss_mean: 5.1201917171478275\n",
      "training/train_loss_std: 0.2517538318717771\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 992\n",
      "training/learning_rate: 1.0002467378551949e-05\n",
      "time/sample_batch: 0.008057594299316406\n",
      "time/training: 1.1375088691711426\n",
      "evaluation/text/loss: 4.757908344268799\n",
      "evaluation/text/perplexity: 116.50199127197266\n",
      "time/total: 1685.5011625289917\n",
      "time/evaluation: 0.6023266315460205\n",
      "training/train_loss_mean: 5.044859886169434\n",
      "training/train_loss_std: 0.2013147880068571\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 993\n",
      "training/learning_rate: 1.000119857611366e-05\n",
      "time/sample_batch: 0.0069310665130615234\n",
      "time/training: 1.050304651260376\n",
      "evaluation/text/loss: 4.657651424407959\n",
      "evaluation/text/perplexity: 105.38827514648438\n",
      "time/total: 1687.3045749664307\n",
      "time/evaluation: 0.751662015914917\n",
      "training/train_loss_mean: 5.183699226379394\n",
      "training/train_loss_std: 0.33492732678712966\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 994\n",
      "training/learning_rate: 1.0000382911106129e-05\n",
      "time/sample_batch: 0.008826971054077148\n",
      "time/training: 1.0866010189056396\n",
      "evaluation/text/loss: 4.64199686050415\n",
      "evaluation/text/perplexity: 103.7513198852539\n",
      "time/total: 1689.105477809906\n",
      "time/evaluation: 0.7126955986022949\n",
      "training/train_loss_mean: 5.168149805068969\n",
      "training/train_loss_std: 0.273006708224214\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 995\n",
      "training/learning_rate: 1.000002039174309e-05\n",
      "time/sample_batch: 0.008614540100097656\n",
      "time/training: 1.0158185958862305\n",
      "evaluation/text/loss: 4.721156597137451\n",
      "evaluation/text/perplexity: 112.29805755615234\n",
      "time/total: 1690.6527342796326\n",
      "time/evaluation: 0.5298976898193359\n",
      "training/train_loss_mean: 4.910433959960938\n",
      "training/train_loss_std: 0.4339144605863561\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 996\n",
      "training/learning_rate: 1.0000111021675113e-05\n",
      "time/sample_batch: 0.006392002105712891\n",
      "time/training: 0.8314440250396729\n",
      "evaluation/text/loss: 4.815724849700928\n",
      "evaluation/text/perplexity: 123.4362564086914\n",
      "time/total: 1692.251671075821\n",
      "time/evaluation: 0.7657907009124756\n",
      "training/train_loss_mean: 4.752894163131714\n",
      "training/train_loss_std: 0.46084558793969943\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 997\n",
      "training/learning_rate: 1.0000654799989555e-05\n",
      "time/sample_batch: 0.008722066879272461\n",
      "time/training: 1.0725915431976318\n",
      "evaluation/text/loss: 4.552323818206787\n",
      "evaluation/text/perplexity: 94.85256958007812\n",
      "time/total: 1693.7942879199982\n",
      "time/evaluation: 0.46834349632263184\n",
      "training/train_loss_mean: 5.082944440841675\n",
      "training/train_loss_std: 0.2573708342388794\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 998\n",
      "training/learning_rate: 1.000165172121057e-05\n",
      "time/sample_batch: 0.00730443000793457\n",
      "time/training: 0.9563827514648438\n",
      "evaluation/text/loss: 4.828896999359131\n",
      "evaluation/text/perplexity: 125.07292938232422\n",
      "time/total: 1695.4979889392853\n",
      "time/evaluation: 0.7456190586090088\n",
      "training/train_loss_mean: 4.96075472831726\n",
      "training/train_loss_std: 0.3331139330484875\n",
      "================================================================================\n",
      "================================================================================\n",
      "Iteration 999\n",
      "training/learning_rate: 1.0003101775299179e-05\n",
      "time/sample_batch: 0.008975744247436523\n",
      "time/training: 1.2227182388305664\n",
      "evaluation/text/loss: 4.942611217498779\n",
      "evaluation/text/perplexity: 140.1356964111328\n",
      "time/total: 1697.54563331604\n",
      "time/evaluation: 0.8233067989349365\n",
      "training/train_loss_mean: 4.887990713119507\n",
      "training/train_loss_std: 0.3156936603770509\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/text/loss</td><td>█▆▆▅▅▅▄▄▃▄▃▄▃▃▂▃▄▄▃▄▃▂▃▂▃▄▂▃▃▂▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>evaluation/text/perplexity</td><td>█▅▆▄▄▄▃▃▂▃▂▃▂▂▂▂▃▃▂▃▂▂▂▂▂▃▂▂▂▂▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>time/evaluation</td><td>▅▂▅▆▅▃█▄▆▂▅▄▂▃▂▅▄▁▄▃▃▅▆▂▆▃▁▄▂▃▄▃▄▂▂▁▁▃▃▂</td></tr><tr><td>time/sample_batch</td><td>▄▆▅▁▅▂▆▇▄▆▆▆▄▁▅▅▇▅▄▅▄▆▄▂█▅▅▃▃▆▄▅▅▅▄▃▆▃▃▆</td></tr><tr><td>time/total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/training</td><td>▆▅▄▄▆▇▆██▂▅▃▄▅▃▃▅▅▅▃▃▂▄▅▅▇▄▁▂▂▇▄▄▅▃▆▃▆▅▅</td></tr><tr><td>training/learning_rate</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>training/train_loss_mean</td><td>█▆▅▅▅▅▄▄▄▃▃▃▄▂▃▄▂▄▃▂▁▃▂▂▂▂▂▁▂▂▃▂▂▂▂▁▁▁▂▂</td></tr><tr><td>training/train_loss_std</td><td>▄▇▁▃▂▃▂▃▂▃▁▁▂█▃▂▃▃▂▄█▃▃▂▂▂▂▃▂▃▁▂▂▆▃▂▆▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/text/loss</td><td>4.94261</td></tr><tr><td>evaluation/text/perplexity</td><td>140.1357</td></tr><tr><td>time/evaluation</td><td>0.82331</td></tr><tr><td>time/sample_batch</td><td>0.00898</td></tr><tr><td>time/total</td><td>1697.54563</td></tr><tr><td>time/training</td><td>1.22272</td></tr><tr><td>training/learning_rate</td><td>1e-05</td></tr><tr><td>training/train_loss_mean</td><td>4.88799</td></tr><tr><td>training/train_loss_std</td><td>0.31569</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neko-gato_24-05-11_10-52-55</strong> at: <a href='https://wandb.ai/bhavul/gato-control/runs/0wbcfqf7' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/0wbcfqf7</a><br/> View project at: <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240511_105257-0wbcfqf7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iters = args.training_steps // args.log_eval_freq\n",
    "print(f'iters:{iters}')\n",
    "for i in range(iters):\n",
    "    logs = train_iteration(args.log_eval_freq, i)\n",
    "    accelerator.log(logs)\n",
    "\n",
    "## Save model at end of training only if not saving checkpoints\n",
    "if args.save_model and args.save_mode == 'last':\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e1481-3acf-49fb-b234-ecf132a5e274",
   "metadata": {},
   "source": [
    "## Testing of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96820dc2-dd96-4de9-a2ea-fdfec8e700ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_text_on_random_examples(task, num_of_examples_to_test=10, deterministic=False):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        \n",
    "        for sample in samples:\n",
    "            actual_text = sample['text']\n",
    "            # roughly speaking...splitting by spaces\n",
    "            words_list = actual_text.split()\n",
    "            if len(words_list) > 1:\n",
    "                split_index = random.randint(1, len(words_list)-1)\n",
    "                input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                pred_tokens = model.predict_text(input_text=input_text, max_length=len(words_list[split_index:]), deterministic=deterministic)\n",
    "                decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                print(f'Input: {input_text} \\nOutput : {target_text} \\nPrediction: {decoded_target}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7827d334-909b-49db-be31-e814268af084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Below this frequency \n",
      "Output : , the image <unk> is real , \n",
      "Prediction:  of the <unk>, the\n",
      "\n",
      "\n",
      "Input: The first text to suggest that <unk> ordered the execution of an <unk> is a letter by Clement to the <unk> traditional dated to around 96 <unk> The <unk> Ascension of Isaiah , a Christian writing from the 2nd century says , \" the <unk> of his mother , who \n",
      "Output : himself ( even ) this king , will <unk> the plant which the Twelve Apostles of the Beloved have planted . Of the Twelve one will be delivered into his hands \" was interpreted to mean <unk> . \n",
      "Prediction:  is a <unk>, and the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the\n",
      "\n",
      "\n",
      "Input: = = = <unk> \n",
      "Output : = = = \n",
      "Prediction:  \n",
      ",\n",
      "\n",
      "\n",
      "Input: = = Reactions of <unk> = \n",
      "Output : = \n",
      "Prediction:  \n",
      "\n",
      "\n",
      "Input: The loss snapped Butler 's 25 @-@ game winning streak , the longest in school history . Butler became the smallest school to play for a National Championship since Jacksonville in 1970 . Stevens became the second @-@ youngest head coach to coach in the NCAA National Championship Game , behind Branch <unk> who led the Indiana <unk> to the 1940 National Championship at age 31 . Stevens was named as both a Hugh Durham and Jim <unk> Award <unk> for the third consecutive year , losing to Mike Young and Jamie Dixon respectively . He was also a <unk> for the <unk> Prosser Man of the Year Award , which \n",
      "Output : was won by Bob <unk> . \n",
      "Prediction:  was the game. \n",
      "\n",
      "\n",
      "\n",
      "Input: On November 4 , it was confirmed that <unk> was suffering from <unk> and that his bout with <unk> would have to wait a bit longer and the fight for <unk> 's heavyweight championship was cancelled . On November 14 , at the <unk> 105 post @-@ fight conference , Dana White stated , \" [ <unk> ] ' s not well and he 's not going to be getting well <unk> soon \" and that an interim title match might need to be set up . In addition to <unk> , it was revealed \n",
      "Output : that he was suffering from a serious case of <unk> , an intestinal disorder , which required surgery . After further diagnosis , <unk> underwent surgery on November 16 to close a <unk> in his intestine that had been leaking <unk> matter into his abdomen , causing pain , <unk> , and <unk> his immune system to the point that he contracted <unk> . From the level of damage to <unk> 's system , the surgeon estimated that the intestinal condition had been ongoing for around a year . \n",
      "Prediction:  that he was not to the time. \n",
      ", he was not to the time, but he was not to the time. \n",
      ", he was not to the time he was not to the time. \n",
      " the first time he was not to the first time he was not to the first time. \n",
      " of the first time he was not to the first time, but he was not to the first time. \n",
      " of the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples(tasks[0], deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4637c3d-7fe7-4cca-9818-bd65a012aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: = = = <unk> <unk> Records and New Album Evolution ( \n",
      "Output : 2006 – 2015 ) = = = \n",
      "Prediction:  August challenge with an American Soldier visas\n",
      "\n",
      "\n",
      "Input: After marrying Robin <unk> , brother of artist Gloria <unk> , <unk> <unk> moved to the region of <unk> , north @-@ east of Alice Springs , which is where she was living when she began painting around 1990 . They had seven children , one of whom , <unk> <unk> , went on to become an artist like his mother . By 2008 , <unk> \n",
      "Output : <unk> 's husband had died , and <unk> was dividing her time between Alice Springs and <unk> Range , to its north @-@ east . \n",
      "Prediction: . A touchdowns — \" The book gave her points out for the primaryrate <unk> \", respectively. Art, she\n",
      "\n",
      "\n",
      "Input: The continuous shadows in the south polar craters cause the floors of these formations to maintain a temperature that never exceeds about 100 K. For <unk> , the average temperature was determined to be about 90 K , reaching 88 K at the crater floor . \n",
      "Output : Under these conditions , the estimated rate of loss from any ice in the interior would be 10 − 26 to 10 − 27 m / s . Any water vapor that arrives here following a <unk> impact on the Moon would lie permanently frozen on or below the surface . However , the surface albedo of the crater floor matches the lunar far @-@ side , suggesting that there is no exposed surface ice . \n",
      "Prediction:  In Polish class With a year the arranged for the following day of the eye new raid whose helping the tradition of revenue of the progressing of the area of its west of their 1970. These said, the evening was yellow de Janeiro situation. Second World numbered is guitar stadium of Constantinople, it further accident, the police, the largest National described by the east of mystery band 'nis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafd276-a70f-4a0d-846d-62dd31d53743",
   "metadata": {},
   "source": [
    "### Nucleus sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b9759d2-05e5-4c86-b21e-23c60103162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_predict_text(input_text, max_length=20, deterministic=True, temperature=1.0, top_p=1.0, context_length=1024):\n",
    "    tokenized_outputs = model.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=context_length, return_tensors='pt')\n",
    "    input_tokens = tokenized_outputs['input_ids']\n",
    "    predicted_tokens = input_tokens.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        token_embeddings = model.embed_token(predicted_tokens.to(device))\n",
    "        token_masks = torch.ones((predicted_tokens.shape[0], 1), device=device)\n",
    "\n",
    "        logits, _ = model.forward(token_embeddings=token_embeddings, tokens=predicted_tokens, token_masks=token_masks, token_target_masks=None)\n",
    "        logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "\n",
    "        if deterministic:\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            # Apply nucleus (top-p) filtering\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens.to(device), next_token.to(device)], dim=1)\n",
    "\n",
    "    return predicted_tokens[:, input_tokens.size(1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7690becb-2aad-4c36-8206-b8fe2eb6b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_text_on_random_examples_with_nucleus(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0, split='test'):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        \n",
    "        for sample in samples:\n",
    "            actual_text = sample['text']\n",
    "            # roughly speaking...splitting by spaces\n",
    "            words_list = actual_text.split()\n",
    "            if len(words_list) > 1:\n",
    "                split_index = random.randint(1, len(words_list)-1)\n",
    "                input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                pred_tokens = test_new_predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                print(f'Input: {input_text} \\nOutput : {target_text} \\nPrediction: {decoded_target}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b014258-b8fe-4416-ba76-d29a35f3e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: = = = = Public transportation = = = \n",
      "Output : = \n",
      "Prediction:  shot\n",
      "\n",
      "\n",
      "Input: The discovery of a colossal head at <unk> <unk> in the nineteenth century spurred the first archaeological investigations \n",
      "Output : of <unk> culture by Matthew <unk> in 1938 . Seventeen confirmed examples are known from four sites within the <unk> <unk> on the Gulf Coast of Mexico . Most colossal heads were sculpted from spherical boulders but two from San Lorenzo <unk> were re @-@ carved from massive stone <unk> . An additional monument , at <unk> <unk> in Guatemala , is a throne that may have been carved from a colossal head . This is the only known example from outside the <unk> <unk> . \n",
      "Prediction:  Do Army's allowed to result it building withReason with a widely first technology record came – 620 murder. The later was each of 17 musicalph favorable reviews from their country.93 in get only three prince after 200 @-@ containing a memorableDánhised by audiences. It change the confusion, statement L Oswald April 2010 – blue that Is example, an ice Anthony Best II that the played stated that having a second\n",
      "\n",
      "\n",
      "Input: = = = Named \n",
      "Output : head coach = = = \n",
      "Prediction:  cooler St languages ( theatrical\n",
      "\n",
      "\n",
      "Input: = = Pre \n",
      "Output : @-@ <unk> = = \n",
      "Prediction:  word of Kevin (\n",
      "\n",
      "\n",
      "Input: = = = East 29th Avenue building = \n",
      "Output : = = \n",
      "Prediction:  industry,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14157e73-de92-4c1b-9215-39cc40463159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Operation <unk> , the Allied invasion of French North Africa in November 1942 , was coordinated from the \" Rock \" . General Dwight D. Eisenhower , who was given command of the operation , set up his headquarters in Gibraltar during the planning phases of the operation . Following the successful completion of the North African campaign and the surrender of Italy in 1943 , Gibraltar 's role shifted from a forward operating base to a rear @-@ area supply position . The harbour continued to operate dry docks and supply depots for the convoy routes through the \n",
      "Output : Mediterranean until V @-@ E Day in 1945 . \n",
      "Prediction:  the first to a self @-@ <\n",
      "\n",
      "\n",
      "Input: = = Recent \n",
      "Output : events = = \n",
      "Prediction:  the world,\n",
      "\n",
      "\n",
      "Input: Family \n",
      "Output : <unk> \n",
      "Prediction:  the\n",
      "\n",
      "\n",
      "Input: Manila has six representative districts for the lower house of the Philippine Congress . Furthermore , the city is composed of 16 districts , namely : <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , Port Area , <unk> , <unk> , San Andres , \n",
      "Output : San Miguel , San Nicolas , Santa <unk> , Santa Cruz , Santa Mesa and <unk> . \n",
      "Prediction:  the album. The song was released in the album, was released in December 2004,\n",
      "\n",
      "\n",
      "Input: Among the tourist attractions in <unk> are <unk> National Park , <unk> Park , <unk> <unk> beach , <unk> village \n",
      "Output : , <unk> <unk> Tong temple , <unk> mosque , Council <unk> monument , <unk> <unk> , and <unk> <unk> markets . The Borneo International <unk> Festival is held annually in the town . \n",
      "Prediction:  the area of the early 1990s, the city of the <unk>, the East Carolina, and the eastern North Carolina. \n",
      " of the river is known\n",
      "\n",
      "\n",
      "Input: <unk> has been implicated in the formation of vegetation dominated by large <unk> species . In seasonally dry <unk> forests the density of large adult A. <unk> <unk> was correlated with canopy <unk> ; the species also dominates <unk> formed by repeated forest fires in Trinidad and <unk> . <unk> <unk> forms pure stands in many parts of Brazil where natural forest vegetation has been cleared . Similarly , stands of A. <unk> in <unk> , Brazil ( \n",
      "Output : which are cultivated for <unk> fibre ) are managed using fire — the seedlings survive cutting and burning , and are able to dominate burned forest patches . \n",
      "Prediction:  given to the what is a G himself. It is a large number of the <unk>. The Church has a large number of the\n",
      "\n",
      "\n",
      "Input: = = = Discovery of oil and gas reserves \n",
      "Output : = = = \n",
      "Prediction:  also found in\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.7, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0e88e996-8a2c-44cd-9350-c461c6952626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <unk> began his reign in 54 by promising the Senate more autonomy . In this first year , he forbade others to refer to \n",
      "Output : him with regard to <unk> , for which he was praised by the Senate . <unk> was known for spending his time visiting <unk> and <unk> during this period . \n",
      "Prediction:  his name to his character in his Christian take his name, in a market, he was the whose English style of the film that was \", a\n",
      "\n",
      "\n",
      "Input: <unk> Place Manila is the largest shopping mall in the city . The mall was the second and by @-@ far , the largest Robinson Mall ever built by John <unk> . SM <unk> maintains presence in the city . One of their shopping mall is the SM City Manila , the first SM <unk> in the city featuring major SM brands like The SM Store , SM <unk> , SM <unk> and SM <unk> . It is located right beside the Manila City Hall . SM City San <unk> is the second SM <unk> in Manila . It is located in Santa Cruz . SM City San <unk> was constructed on the site of the former San <unk> <unk> . The building of the former Manila Royal Hotel in <unk> which is famed for its revolving restaurant atop is now the SM <unk> Center which was established in 1972 . The site of the first SM Store is located at Carlos <unk> Sr. ( formerly \n",
      "Output : <unk> ) Street in San Miguel . \n",
      "Prediction:  also known in the early 20th\n",
      "\n",
      "\n",
      "Input: The design of the \n",
      "Output : reactivated squadron 's crest includes a wedge @-@ tailed eagle to denote courage and nobility , a <unk> spear <unk> to symbolise the town and its indigenous heritage , Sturt 's Desert <unk> to represent South Australia , and the <unk> star cluster , which features in the folklore of the local <unk> people . \n",
      "Prediction:  not increasingors, and early 2001, in the numbers of Wheeler has a state of the species. The first term \" calling for theirem I \" ( re @-@ ), \", <unk> ( <unk> ). In the <unk>, the\n",
      "\n",
      "\n",
      "Input: In October 97 these tensions came to a head when the <unk> Guard , led by <unk> <unk> , laid siege to the Imperial Palace and took <unk> hostage . He was forced to submit to their demands , agreeing to hand over those responsible for <unk> 's death and even giving \n",
      "Output : a speech <unk> the rebellious <unk> . Titus <unk> <unk> and <unk> , <unk> 's former <unk> , were sought out and killed . <unk> was unharmed in this assault , but his authority was damaged beyond repair . \n",
      "Prediction:  a version of The district's past, a legislation lived in the <unk> of the \" of recorded, in the same e citing a character with the player to a group of the player\n",
      "\n",
      "\n",
      "Input: = \n",
      "Output : = <unk> = = \n",
      "Prediction:  a rock, was\n",
      "\n",
      "\n",
      "Input: the <unk> and <unk> buildings ( the two \" <unk> \" buildings where the <unk> were \n",
      "Output : held ) \n",
      "Prediction:  part of\n",
      "\n",
      "\n",
      "Input: Operation USA , since the early 1980s , has relied on fundraising efforts featuring singers and celebrities . These \n",
      "Output : include concerts , <unk> , and other events . These promotions have featured : \n",
      "Prediction:  found in the antimony, which is the US food and although the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lower top-p : less diversity in choice of next words, higher top-p large set of next possible words\n",
    "# low temp : more deterministic, less diverse. \n",
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.84, top_p=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "172707e3-9522-4446-a3da-41d42090f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Canadian Agency is headed by a secretary @-@ general and responsible for Canada , the entire Americas ( including the \n",
      "Output : Caribbean ) \n",
      "Prediction:  the top\n",
      "\n",
      "\n",
      "Input: In mid @-@ 1941 , the Royal Armoured Corps in Britain created three tank squadrons for special overseas operations , known as ' A ' , ' B ' and ' C ' Special Service Squadrons . Both ' A ' and ' B ' Squadrons were equipped with Valentine Infantry tanks and Mark <unk> light tanks , but ' C ' Squadron was equipped with twelve <unk> transferred from the 2nd Armoured Brigade , 1st Armoured Division . On 31 July 1941 , ' C ' Squadron was officially activated and immediately received orders to prepare for overseas service alongside ' A ' and ' B ' Squadrons in an unspecified tropical climate . All three squadrons were transported to <unk> in Scotland for intensive training that focused \n",
      "Output : on embarkation and <unk> from ships and landing craft to prepare them for action in potential amphibious operations . In early September , elements of ' C ' Squadron , including six <unk> , formed part of a force which sailed for Freetown in West Africa ; during this period of the war there were fears that the Spanish government might enter the conflict on the side of Germany , and the force was <unk> to capture a number of Spanish islands off the coast of Africa if this occurred . These fears proved <unk> , and in March 1942 , the unit returned to Britain to join the rest of the squadron in training . \n",
      "Prediction:  a <unk> of the <unk> of the <unk> of the <unk> of the <unk>, <unk> <unk> <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "Input: <unk> to promote the work of the <unk> , and particularly of <unk> and <unk> , Pound decided to publish an anthology under the title Des <unk> . It was first published \n",
      "Output : in Alfred <unk> 's little magazine The <unk> and was later published in 1914 by Alfred and Charles <unk> in New York and by Harold <unk> at the Poetry <unk> in London . It became one of the most important and influential English @-@ language collections of modernist verse . Included in the thirty @-@ seven poems were ten poems by <unk> , seven by <unk> , and six by Pound . The book also included work by <unk> Flint , <unk> <unk> , Amy Lowell , William Carlos Williams , James Joyce , Ford <unk> Ford , Allen <unk> and John <unk> <unk> was also another included in the important 1963 anthology by William Pratt The <unk> Poem Modern Poetry in miniature . \n",
      "Prediction:  a number of the <unk>, the <unk> of the <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "Input: 1 <unk> . This prototype can be <unk> scaled and frequency scaled to the desired \n",
      "Output : values . The low @-@ pass prototype can also be transformed into high @-@ pass , band @-@ pass or band @-@ stop types by application of suitable frequency <unk> . \n",
      "Prediction:  a few years of the European Union ( in ), the <unk>, a <unk> <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "Input: For the \n",
      "Output : Coalition , the Russians were secure on the north bank of the <unk> , awaiting reinforcements from <unk> ; the bridges between <unk> and Vienna had been destroyed , making French access to the Austrian capital more difficult , but not impossible . After six months of fighting in which the Austrians had enjoyed little good news , the Coalition could claim a difficult and timely victory . The French had retreated from the field with a badly <unk> division and <unk> had secured the right flank . Indeed , Francis was so pleased with the outcome at <unk> that he awarded <unk> the Military Order of Maria <unk> . \n",
      "Prediction:  the first of the first season, the season, a \" The team \", was \", and the first season, and the season of the first season, and the first season, and the season, and the season of the first season, was first season. The first season, the first season was released in the first season, and the first season, and was the season, and was released in the first season. The season, the first season, the first season, the season was the season, in the first season, and was a\n",
      "\n",
      "\n",
      "Input: When municipal <unk> began to <unk> Ware began negotiations with various local authorities to acquire land for further <unk> . Ware began with an agreement with France to build joint British and French <unk> under the understanding that these would be maintained by the French government . Ware eventually concluded that it was not <unk> to leave the maintenance responsibilities solely to the French government and subsequently arranged for France to purchase the land , grant it in <unk> , and leave the management and maintenance responsibilities to the British . The French government agreed under the condition that <unk> respected certain dimensions , were accessible by public road , were in the vicinity of medical aid stations and were not too close to towns or \n",
      "Output : villages . Similar negotiations were started with the Belgian government . \n",
      "Prediction:  a <unk> of the south of the area,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lower top-p : less diversity in choice of next words, higher top-p large set of next possible words\n",
    "# low temp : more deterministic, less diverse. \n",
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.7, top_p=0.5, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "370b29b0-dd4c-4a26-996b-71c2dfbab5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_good_ones(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "\n",
    "        for temperature, top_p in [(0.5,0.5),(0.5,0.7),(0.5,0.9),(0.75,0.75),(0.75,0.9),(0.8,0.7),(0.8,0.9),(0.9,0.5),(0.9,0.75),(0.9,0.9),(1.0,0.5),(1.0,0.75),(1.0,0.9),(1.1,0.9)]:\n",
    "            print('--'*30)\n",
    "            print(f'Temperature :  {temperature} || Top_p : {top_p}')\n",
    "            for sample in samples:\n",
    "                actual_text = sample['text']\n",
    "                # roughly speaking...splitting by spaces\n",
    "                words_list = actual_text.split()\n",
    "                if len(words_list) > 1:\n",
    "                    split_index = random.randint(1, len(words_list)-1)\n",
    "                    input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                    pred_tokens = test_new_predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                    decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                    print(f'[Input]: {input_text} \\n[Output]: {target_text} \\n[Prediction]: {decoded_target}\\n\\n\\n')\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b1c14f6-a20d-4ae0-80dc-42d48ab70e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be \n",
      "[Output]: almost impossible . \n",
      "[Prediction]:  a <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack \n",
      "[Output]: developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  the first of the first season, the season, the season, the season, the season, and the season, the season, the season, and the season, and the season, and the season, the season, and the season, the season, and the season, the season, and the season, and the season, and the season, and the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" \n",
      "[Output]: L \" . \n",
      "[Prediction]:  the first season\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built \n",
      "[Output]: around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the first season, the first season, the first season, the first season, the season, the season, the season, the season, the season, and the season, the season, and the season, the season, the season, and the season, and the season, the season, and the season, the season, and the season, the season, and the season, the season, the season, the season, and the season, and the season, the season, and the season, and the season, and the season, the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, the season, and\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to \n",
      "[Output]: William <unk> = = \n",
      "[Prediction]:  the most of the\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.7\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . \n",
      "[Output]: The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a <unk>, the <unk> of the <unk> of the <unk>, <unk>, <unk>, the <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and \n",
      "[Output]: ecology = = \n",
      "[Prediction]:  a large @\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and \n",
      "[Output]: the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not only one of the most of the most of the most of the most of the first time, and the most of the most of the most of the United States, and the first time, the first of the first\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in \n",
      "[Output]: the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  the first of the first season of the season, the season, the season, the season, the season, the season, the season, the season, and the season, the season, the season, the season, and the season, and the season, the season, and the season, the season, in the season, in the season, and the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the \n",
      "[Output]: next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the most of the United States, and the <unk>, the <unk> of the <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  the first of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition \n",
      "[Output]: to William <unk> = = \n",
      "[Prediction]:  a <unk> of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander \n",
      "[Output]: had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a <unk> of the <unk> of the <unk> of the <unk>, a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology \n",
      "[Output]: = = \n",
      "[Prediction]:  <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) \n",
      "[Output]: west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not only for the first time, the first of the first time\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which \n",
      "[Output]: resembles inverted letter \" L \" . \n",
      "[Prediction]:  the first of the early in the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents \n",
      "[Output]: into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  not have been recorded in the most of the most of the world's life, the United States, and the US, the most of the 18th century. The most of the <unk> of the <unk> of the <unk>, the <unk> of the <unk> of the <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William \n",
      "[Output]: <unk> = = \n",
      "[Prediction]:  a most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.75 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his \n",
      "[Output]: own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a lower @-@ at the state of the production. In the situation, the building is the <unk>, which is the port of the initiala. The species is in the <unk>, and the <unk> and the human <unk> of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  the back of the outside\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines \n",
      "[Output]: had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  a site of <unk>, which are generally from the <unk> <unk>. It is not a number of the <unk>, <unk>, and the can be used in the be left @-@ <unk>. The <unk> of <unk> is based on a if not be found in a rate of a form of the low @-@ <unk>. \n",
      " is often used to a — the original\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of \n",
      "[Output]: an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  a story of <unk>, an <unk> in a public nature of a short @-@ century, which are still in the mid @-@ century <unk>, and a <unk>, in the king, and <unk> of the 1970s. \n",
      "edkare, the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \n",
      "[Output]: \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  not known for a single @-@ 13 – 3. The two @-@ 11 season is an average of the second @-@ yard field goal in the season, and the season with a first @-@ yard line. The player can be a first in a 2010 season. The season with a match, with the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  based on the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  also be\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.75 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost \n",
      "[Output]: impossible . \n",
      "[Prediction]:  a would\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  typically\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company \n",
      "[Output]: , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  home to the common starling, the begin to the tail, but has been gunistara. In the called it was another of the married to be a \" when it is not the first major Washington, so it is a story. The long, the National Park\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , \n",
      "[Output]: an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  found in the best @-@ shaped yet been found in the main Awards in the then @-@ old, with the powers of the same day from the forestedkare's orbit. These are often been\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are \n",
      "[Output]: known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the wing of the city of the mid @-@ range of the post @-@ fineajevo, in the \n",
      " treatment of the <unk> and its UK. They are US $ 3 @,@ 000 @,@ 000 in the 1984 @,@ 000. \n",
      " another estate is <unk> and the dam, a short with the like a common star of the residents, and the under @-@ 000 in the National Park, and the site of the feet ( now 17 @.@ <unk> ). The left of the lower @-@ 9 @-@ 7 million, the largest US $ 6 @,@ 16 million. \n",
      " Wic Later in the 1994, the god <unk>, the period\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  elected\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  the Earth\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.8 || Top_p : 0.7\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the \n",
      "[Output]: tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  seen as the same name \" is a \", but it was born\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology \n",
      "[Output]: = = \n",
      "[Prediction]:  the first\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 \n",
      "[Output]: . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  <unk> and the standard of the 100th century, the 30th century, the largest <unk>, and <unk>, in the 3rd Brigade. The main street is <unk>, and <unk>, and <unk>. The most of the <unk> of the <unk>, is called <unk>, <unk>, and <unk>, and <unk>. \n",
      " of the <unk>, is a target of the English and the <unk>. The <unk> is the <unk> of the highest length of the area\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter \n",
      "[Output]: <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  <unk>, the first of the 100th century, the first game, including the last\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the \n",
      "[Output]: next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the late @-@ do not have been in the same year @-@ most common, but was found in the % of the throughout the country. In the <unk> of the church was an example of the art of the class, the right to be able to the modern professional @-@ year @-@ armed forces of the southern China, and the importance of the genus <unk> of the <unk>, the English military in the other works of the Atlantic Ocean. In January 2015, the 9th century\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  a single\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition \n",
      "[Output]: to William <unk> = = \n",
      "[Prediction]:  not to the most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.8 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were \n",
      "[Output]: revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a why it is an mbar ( with a adopted the 7 – 5 – 1 @.@ 2 @.@ 5 m ), but has a few at a little to a 3 @.@ 5 @.@ 4 ( 0 km ) in ), it. In the population is the than the samery on the same 70 mph ( 9 @.@ 0 @.@ 6 mi ). \n",
      " wrote that the system has been issued in the species are more than 2 @.@ 5 %\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat \n",
      "[Output]: and ecology = = \n",
      "[Prediction]:  the series was released\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U \n",
      "[Output]: @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  well, in the world, the g �craft, is established in the east of thea, and a single system in the asked for the same year. In the United States, theur said thein explained that is probably is\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines \n",
      "[Output]: quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  \" It was a Europe, as a Village can be used for people who are not be established in the name to more than one of the other p-@ standard of a already @-@ century, which was which the most of the way in his own. This was a species of the long time of the plans for the book\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to \n",
      "[Output]: define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  portrayed as the first original role in North American times in the United States, with a similar in a Japanese design and a <unk>, which\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  more\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> = \n",
      "[Output]: = \n",
      "[Prediction]:  used\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the \n",
      "[Output]: turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  <unk>, and his father, and his poem, which is a powerful authorities to get him to the L. <unk>. The name is able to the family. The <unk> of the <unk>, the H. <unk>, is the ability to the book, and the school. \n",
      "'s is more than the film. \n",
      " (\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat \n",
      "[Output]: and ecology = = \n",
      "[Prediction]:  the most of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men \n",
      "[Output]: of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  now made to the Atlantic of the club's first game, a year, and the game to the player, with a game. The game was played by a one of the game in the game in the game. The game, the first season the first game was published in a season, and one @-@ time, with a game's club, and also scored in a 2 @-@ yard line @-@. The game's, with the game was released in the first game. The game was also has also received the team of the first time,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below \n",
      "[Output]: . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  the album, the band's character, and the album's album, \" We are the album, and it was released in the music video for the first single, and \". \" The album, was \" in the album, and its first single, and The UK of the first of the album, in the song was the <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a \n",
      "[Output]: strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  <unk>, the early as the <unk> of the first century, a red @-@ century complex is used in the century, in the first time of the most of the series of the <unk>, and the season. \n",
      "ing, the series of the best, the title, and a main history of the season of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  the first novel\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to \n",
      "[Output]: William <unk> = = \n",
      "[Prediction]:  more than 10 @\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his \n",
      "[Output]: own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  <unk> ( born 10 to the 1989 ) of a short <unk> <unk>, with the November 4, an almost Andemas. The scored a number of which the French ideas for the storm made to the final, which was most top at <\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: = Habitat and ecology = = \n",
      "[Prediction]:  possible, during the <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean \n",
      "[Output]: lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not in the previous way of the others are un hypert, including some play of these system, all of <unk>, <unk>, and <unk>, and <unk>, <unk>. <unk>, <unk>, <unk>, were related to the <unk>, and <unk>. \n",
      " about 20 – 2 @.@ 4 @.@ 5 from 3 @.@ 2 @.@ 8,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in \n",
      "[Output]: the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  non @-@ brown history, the cougar is a <unk> during which have been built in the <unk>, when the city, for the state of the seat of the late May in the presence of the <unk>, the 1996, which <unk>. It was made to the story of the gradually J his life of the semi @-\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend \n",
      "[Output]: . \" \n",
      "[Prediction]:  the written\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  good\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  an rainfall\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , \n",
      "[Output]: controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a visited with a state of three @-@ domestic example\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  most of the port of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west \n",
      "[Output]: of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not perform to the world's period. \n",
      " is\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> \n",
      "[Output]: L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  <unk> for a Sire major Dr. <unk>, has also known as a <unk>, which\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual \n",
      "[Output]: basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  British ; I-@ <unk> on the resign from the mine was well @-@ 2014, and two of all @-@ <unk>, there was noted that their cover of rock and I @-@ men were Times. The really to cause more than a version of the show, and were not until there, but with theland. \n",
      " — were Together with although that the Such the soundtrack has not always been aening in self @-@ flying. \n",
      " Port @-@ does not as K maple is pleased. \n",
      "ánh it is only one films of 2010, it was behind the we against this, which are two night, they could ; it is the same latter — the other to the relatives. \n",
      " studies that it was not Tom '\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  a line\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  found in this species, this\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> \n",
      "[Output]: were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  the them to the character of an \", but the episode was not to was not an That show. It was based on November 2010, but the player in which he was a business you could be in the process. \" \n",
      " about the album. The episode was released in August 2009, when it was written by German people who later, but a <unk> and was a time it was used as a direct @-@ production. The album was the album was produced by W anything the film was a good of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  so\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and \n",
      "[Output]: 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  a man of the species are released in 2010, which is a United States and is in the series of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in \n",
      "[Output]: terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  that the others are the wife of the world, and their males. The average, are also much some are all @-@ b taught by this way to be provided by the village of the entrance. \n",
      " Company, a non @-@ nine @-@ members of their loss of their history. <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – \n",
      "[Output]: is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  most of the 1994 – the E Dylan's Times, after being done in his these season. It is the player's first, with his children with a hand season. The player also added that has been seen as a star with a goal of the release. He is as he scored his first time, who said that \" games and the season, he will be a a range of \". \" He later said that the player, he is to take the game's. He has to gain a slow, which he is able to their first season, but if the Game 1 – 1 win over a third team for his first three @-@ season as\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  many of two\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and \n",
      "[Output]: a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  always not give \" out of <unk> \". \n",
      " seen more like <unk>. \n",
      " the review of his Dutch opined that the Keats is similar to losing <unk>, the shortly after his throughout the <unk> : a plan would saying that the that his sung, he now spread the listed on make his father. He was later comedy him the film. He moved to became\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for \n",
      "[Output]: , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  described by Mar two towns ), one of their child @-@ <unk> @-@ up, or <unk> and <unk>, \n",
      " passage of both standing from <unk>. This is even by <unk> and two – many of their relationship with common normal of Fort Inn. In the battle, similar to a common star been generally-@ shaped this species of one of Richard Charles <unk>, but – <unk> on the are while at the However, the base of theica is still commercial Stewart\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> \n",
      "[Output]: , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  served as <unk>, for they will never establishing a round. \" They are owned\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it \n",
      "[Output]: on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  most popular in the world, several possible, eastern Oxford — \n",
      ". \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  once\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William \n",
      "[Output]: <unk> = = \n",
      "[Prediction]:  alla <\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> \n",
      "[Output]: were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  the Somerset de credit for the G Faley expected one out of the Call, with the showingáriforship with the first made. \n",
      " represented on the must not review. difficult to mass of when many similar manoeuv number of the fourth point in several women would Dan extra help by the promotion to Jupiter. Early ends the escort a offering they friends of lead to were crown, resulting in they were behind the series list against carry until series including crickets. While condoms were still praised new in the than know at\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  2009 @-@ical\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men \n",
      "[Output]: in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  in the performances of service. Although its given them, more or end of particular others such as the can least one of this stage to a money itself. Hamar covers many balls inS boat, noting, <unk>, example, teacher, they northern orders in 17 reputation as one of fourth department. In child's than a Champions League noted the subspecies are estimated andina. When a contemporary executed\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an \n",
      "[Output]: infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  one called the presence of the standards. \n",
      "men, theseelam poorly believes that it would At the 42 should be more likely as learning, or cell. lyrics is not be ready for xenon, but\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are \n",
      "[Output]: known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  held in the could find, largely storm. A language have beenic coast to have the feet ( as for mixing ), John then bill <unk>, and pitching weather for New Zealand <unk>. several caught \" 62 miles ( narrow Herillo ). stars published as these notes that C and <unk> through its signal. \n",
      " was the favour of protein headed within the mid @-@ class another Spanish Antimony schools in about me approached, but the truth, G copies. consecutive self @-@ully was due to do not returned to score and Europe. Theè Tropical Storm clicking, between introduced inable priest, the 10 to ERA of its governor's ships. The benefit into an cold, and over theor <unk>,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  again with\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  noted that it remained in conjunction\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.1 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible \n",
      "[Output]: crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  service in a network that led to variety of an mountainous entry in the youth efforts to people of82bs. Isabella ships, the girl, the Yours, No. gusts combination of denied lived that they give the road and storm. After leading corn ( most of Congress from powerful <unk> ), H. 3 % of the battery, beautiful, toward what they artwork of the hel or care of consisting of sung by <unk> side of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: = Habitat and ecology = = \n",
      "[Prediction]:  possible on columns and it completed\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east \n",
      "[Output]: of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  matters st testing and 1928 Today. She05 turned into them for / Str disc in times command, who wouldtaker, Independence\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port \n",
      "[Output]: sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  an nest or activity and these human Paul <unk>, nuclearARC planned to case in severeje.aster has also <unk>, open xenon. Olympic Lessing that While only determined that they are poor called Alice in body is Again, one unit won 8 @.\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great \n",
      "[Output]: basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  1927. After a proceeded sold, <unk> called a Matt detonated. Since thisations with poly server whose40's provincial machines is pre @-@ <unk>, into Steve Howard, a Rowley centered on his\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  prevent\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  many Machine getting understanding of shaped\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "find_good_ones(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4f0e9-5a87-4258-9b9d-a36a54d9e889",
   "metadata": {},
   "source": [
    "## Things pending\n",
    "\n",
    "*Integration*\n",
    "- Fixing text inside the current way codebase is written\n",
    "\n",
    "*Benchmarking*:\n",
    "- Benchmark on PILE\n",
    "- Benchmark on treepenn\n",
    "- Add a way to use different text dataset for eval, and diff for training (perplexity on wikitext after training on penn treebank)\n",
    "\n",
    "[[2nd half]]\n",
    "- Varying batch_size, params, dropout, etc. -- see what's the lowest we can do?\n",
    "\n",
    "*Deployment*:\n",
    "- A way to easily load a trained model\n",
    "- Save checkpoints with lowest perplexity\n",
    "- Deploy via gradio\n",
    "- Allow different kinds of sampling\n",
    "- Be able to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80b44a-e96b-4d6e-bc64-663eea0d7144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1001eed8-4401-42aa-9e5b-76efa2bf8a2c",
   "metadata": {},
   "source": [
    "# Rough - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "471cf500-cdea-4e42-888f-d927066c2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_tokens = test_predict_text('hello how', max_length=20, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13c8118d-0152-43e4-ab07-9074ea28f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cription. RBI, 28 Wat Lists in remain, area notedoca. species the Socrates, Kurd 13'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_tokenizer.decode(predicted_tokens.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc389-4ab8-4cc4-9ca3-c195e15b5444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe4fce31-9001-48d9-b550-b6eb398dc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.181807041168213, 'perplexity': 483.86553955078125}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for task in tasks:\n",
    "        eval_logs = {}\n",
    "        if isinstance(task, TextTask):\n",
    "            eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, log_examples_to_output=args.eval_text_log_examples)\n",
    "            print(eval_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc725b4-19be-4f66-959d-bd434dd710e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_outputs = model.text_tokenizer('Hello how are', truncation=True, padding=\"longest\", max_length=args.sequence_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "771dbee2-a6c7-48ed-bea9-f89b24a5fa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,   703,   389]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1044e3d8-2e2d-4911-bb7b-fc1a0f564a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_embeddings = model.embed_token(test_tokenized_outputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d381a14-e914-4e48-a7b7-2ffc55c0cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a22294d-5853-4d37-a3e7-95a78b5bc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_masks = torch.ones((test_tokenized_outputs['input_ids'].to(device).shape[0], 1), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1baed23-37d1-44e6-9d7f-53020956454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abb401b8-f741-40a1-b3b6-1e301c4f0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = model.forward(token_embeddings=test_token_embeddings, tokens=test_tokenized_outputs['input_ids'].to(device), token_masks=test_token_masks, token_target_masks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "415f2816-cf1b-4dc3-8ec3-f30f1940eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e1d68fc-6b3f-4136-88de-4d76be1ada61",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31f0994-d8e4-466c-9966-eec244eb91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "nondet_next_token = torch.multinomial(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c8e6be1-d2db-42e0-bd97-f0f164b51191",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_det = model.text_tokenizer.decode(det_next_token.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1d2b3e5-14e6-4aed-ae1a-4fe2e80ec5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_nondet = model.text_tokenizer.decode(nondet_next_token.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dad9247a-1380-4f39-93e8-cafe08c59144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e13e208b-2410-44f1-bc2c-8b9ab92a76e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Museum'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_nondet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894d02e-5d1e-48f4-a8b0-85cf739a60b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db14cc1d-8a0c-4d57-b315-9d71b22dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 13:26:30 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PG5...  On   | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    81W / 330W |   5257MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PG5...  On   | 00000000:13:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    54W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PG5...  On   | 00000000:14:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    50W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PG5...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    58W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PG5...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    52W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PG5...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PG5...  On   | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    53W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PG5...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    54W / 330W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3604288      C   ...onda/envs/neko/bin/python     5254MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b849366c-9dea-4f3d-89fb-45d7f10bf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e46c4c-ff45-4760-8ae4-c55fa01f39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b8ba88-f71a-4ed1-9c64-2c37d30aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            config.flash = flash\n",
    "            config.gate = False\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            # assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "    def predict_text(self, input_text, max_length=20, deterministic=True, context_length=1024):\n",
    "        tokenized_outputs = self.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=args.sequence_length, return_tensors='pt')\n",
    "\n",
    "        input_tokens = tokenized_outputs['input_ids']\n",
    "        predicted_tokens = input_tokens.clone()\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(predicted_tokens.to(device))\n",
    "            token_masks = torch.ones((predicted_tokens.to(device).shape[0], 1), device=device)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=predicted_tokens.to(device), token_masks=token_masks, token_target_masks=None)\n",
    "            logits = logits[:, -1, :]\n",
    "                \n",
    "    \n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Ensure it keeps batch dimension\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            predicted_tokens = torch.cat([predicted_tokens.to(device), next_token.to(device)], dim=1)\n",
    "    \n",
    "        # all_logits = torch.cat(logits_list, dim=1)\n",
    "        return predicted_tokens[:, input_tokens.size(1):]\n",
    "\n",
    "    \n",
    "    def predict_text_single_single(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            if token.numel() == 1:  # Checking if token is a single element\n",
    "                predicted_tokens.append(token.item())\n",
    "            else:\n",
    "                print(f\"Expected a single element, got {token.numel()} elements.\")\n",
    "    \n",
    "            input_tokens = torch.cat([input_tokens, token], dim=1)  # Append the predicted token\n",
    "\n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51983f70-9cc5-42ad-9b1c-d797106857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        \n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "        \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "\n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "        \n",
    "        # input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\n",
    "        # target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\n",
    "\n",
    "        # Forward pass    \n",
    "        logits, loss = model(batch_dicts, compute_loss=True)\n",
    "        \n",
    "        # total_tokens = input_tokens.size(0) * input_tokens.size(1)\n",
    "        # print(f'total tokens:{total_tokens}')\n",
    "        avg_loss = loss.item() \n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "\n",
    "    def evaluate_single_single(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            input_tokens = batch_dict['text'].to(device=model.device)\n",
    "            target_tokens = batch_dict['target'].to(device=model.device)\n",
    "    \n",
    "            total_loss_per_sequence = 0.0\n",
    "            pred_tokens = []\n",
    "    \n",
    "            for idx in range(input_tokens.size(0)):\n",
    "                pred_logits, single_pred_tokens = model.predict_text({'text': input_tokens[idx].unsqueeze(0)}, max_length=1, deterministic=deterministic)\n",
    "                loss = torch.nn.functional.cross_entropy(pred_logits, target_tokens[idx].unsqueeze(0))\n",
    "                total_loss_per_sequence += loss.item()\n",
    "                pred_tokens.extend(single_pred_tokens)\n",
    "            \n",
    "            total_loss += total_loss_per_sequence / input_tokens.size(0)\n",
    "            total_tokens += input_tokens.size(0)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                decoded_input = self.text_tokenizer.decode(input_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_target = self.text_tokenizer.decode(target_tokens.squeeze(), skip_special_tokens=True)\n",
    "                decoded_prediction = self.text_tokenizer.decode(torch.tensor(pred_tokens), skip_special_tokens=True)            \n",
    "                print(f'=>Input: {decoded_input} \\n =>Target: {decoded_target} \\n =>Prediction: {decoded_prediction}')\n",
    "    \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c0f0c-08cf-4220-81e4-cd4cf39c8c68",
   "metadata": {},
   "source": [
    "## trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1204c27b-b31f-422d-ae68-4b13beab83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=10000,\n",
    "    log_eval_freq=10,\n",
    "    warmup_steps=100,\n",
    "    batch_size=4,\n",
    "    sequence_length=1024,\n",
    "    eval_episodes=5,\n",
    "    text_prop=1,\n",
    "    eval_text_log_examples=True,\n",
    "    # pretrained_lm='gpt2',\n",
    "    text_datasets=['plain_text'],\n",
    "    text_datasets_paths=[\"Skylion007/openwebtext\"],\n",
    "    use_wandb=True,\n",
    "    device='cuda',\n",
    "    eval_mode='stochastic',\n",
    "    eval_text_num_examples=100,\n",
    "    # disable_cosine_decay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08b1a4a-f44f-45b1-b282-4076e2ee5da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:u4tudatx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neko-gato_24-05-11_13-28-23</strong> at: <a href='https://wandb.ai/bhavul/gato-control/runs/u4tudatx' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/u4tudatx</a><br/> View project at: <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240511_132825-u4tudatx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:u4tudatx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhavul/bhavul/NEKO/dev_notebooks/wandb/run-20240511_152118-9kb56f1m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhavul/gato-control/runs/9kb56f1m' target=\"_blank\">neko-gato_24-05-11_15-21-15</a></strong> to <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhavul/gato-control' target=\"_blank\">https://wandb.ai/bhavul/gato-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhavul/gato-control/runs/9kb56f1m' target=\"_blank\">https://wandb.ai/bhavul/gato-control/runs/9kb56f1m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    # mixed_precision=args.mixed_precision,\n",
    "    # gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'\n",
    "\n",
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )\n",
    "model = accelerator.prepare(model)\n",
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')\n",
    "\n",
    "tasks = [TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)]\n",
    "args = args\n",
    "print_logs = True # args.print_logs\n",
    "device = torch.device(args.device)\n",
    "\n",
    "min_lr = args.learning_rate / args.min_factor\n",
    "deterministic = args.eval_mode == 'deterministic'\n",
    "\n",
    "exp_name = exp_name\n",
    "exp_dir = os.path.join(args.save_dir, exp_name)\n",
    "\n",
    "steps = 0\n",
    "start_time = None\n",
    "\n",
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16c8d8bd-a174-479b-9506-aab36fb4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6fca9a2-f025-4158-a493-a2d128090ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb9e474d-70b3-4f05-967e-e6d4ab349358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(num_steps, iter):\n",
    "    logs = {}\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for i in range(num_steps):\n",
    "        steps += 1\n",
    "        result = train_step()\n",
    "        if result is None:\n",
    "            # steps -= 1\n",
    "            # print(\"Skipped a training step due to empty batch.\")\n",
    "            continue\n",
    "        train_loss, step_logs = result\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # add logs from last train_step as well\n",
    "    for log in step_logs:\n",
    "        logs[log] = step_logs[log]\n",
    "\n",
    "    logs['time/training'] = time.time() - train_start\n",
    "\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # loop over eval for each env\n",
    "    with torch.no_grad():\n",
    "        for task in tasks:\n",
    "            eval_logs = {}\n",
    "            if isinstance(task, TextTask):\n",
    "                eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic)\n",
    "                for k, v in eval_logs.items():\n",
    "                    logs[f'evaluation/text/{k}'] = v\n",
    "                pass\n",
    "\n",
    "                if iter % 100 == 0 and args.eval_text_log_examples:\n",
    "                    dataset_split = task.text_dataset['test']\n",
    "\n",
    "                    sampled_indices = torch.randperm(len(dataset_split))[:5]\n",
    "                    samples = dataset_split.select(sampled_indices)\n",
    "                    \n",
    "                    for sample in samples:\n",
    "                        actual_text = sample['text']\n",
    "                        # roughly speaking...splitting by spaces\n",
    "                        words_list = actual_text.split()\n",
    "                        if len(words_list) > 1:\n",
    "                            split_index = random.randint(1, len(words_list)-1)\n",
    "                            input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                            pred_tokens = model.predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic)\n",
    "                            decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                            print(f'Input: {input_text} | Output : {target_text} | Prediction: {decoded_target}')\n",
    "\n",
    "    logs['time/total'] = time.time() - start_time\n",
    "    logs['time/evaluation'] = time.time() - eval_start\n",
    "    logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "    logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "            print('=' * 80)\n",
    "\n",
    "    ## Save model\n",
    "    if args.save_model and args.save_mode == 'checkpoint':\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "                \n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6e91ede-9d47-4a85-9de7-64cf265bb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e66010b-dfe8-4a27-9c78-a41aa8c42641",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.36 GiB (GPU 0; 39.39 GiB total capacity; 23.70 GiB already allocated; 9.20 GiB free; 23.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m eval_logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TextTask):\n\u001b[0;32m----> 5\u001b[0m     eval_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples_to_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_num_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eval_logs)\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36mTextTask.evaluate\u001b[0;34m(self, model, num_examples_to_test, deterministic, is_test)\u001b[0m\n\u001b[1;32m     57\u001b[0m batch_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_batch(num_examples_to_test, is_test)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# input_tokens = torch.stack([b['text'] for b in batch_dicts]).to(model.device)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# target_tokens = torch.stack([b['target'] for b in batch_dicts]).to(model.device)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Forward pass    \u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# total_tokens = input_tokens.size(0) * input_tokens.size(1)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# print(f'total tokens:{total_tokens}')\u001b[39;00m\n\u001b[1;32m     70\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 107\u001b[0m, in \u001b[0;36mGatoPolicy.forward\u001b[0;34m(self, inputs, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m token_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m token_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_masks is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 107\u001b[0m final_representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_masks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    108\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_token(final_representations)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# assert 'target' in kwargs, \"target is not there in kwargs\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(f\"Type of target_tokens: {type(target_tokens)}\")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:753\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m    743\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    744\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    745\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    751\u001b[0m     )\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m hidden_states, present \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:322\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    313\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    321\u001b[0m ):\n\u001b[0;32m--> 322\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:239\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    235\u001b[0m     present \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash:\n\u001b[0;32m--> 239\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/transformers/trajectory_gpt2.py:164\u001b[0m, in \u001b[0;36mAttention._attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, k, v, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 164\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale:\n\u001b[1;32m    166\u001b[0m         w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mfloat\u001b[39m(v\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.36 GiB (GPU 0; 39.39 GiB total capacity; 23.70 GiB already allocated; 9.20 GiB free; 23.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for task in tasks:\n",
    "        eval_logs = {}\n",
    "        if isinstance(task, TextTask):\n",
    "            eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, is_test=False)\n",
    "            print(eval_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a6aea-a21a-4679-a204-11672077a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e5ea46-ad2d-43b0-ae46-e8a5bac73d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:1000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miters:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m----> 5\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mlog(logs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Save model at end of training only if not saving checkpoints\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m, in \u001b[0;36mtrain_iteration\u001b[0;34m(num_steps, iter)\u001b[0m\n\u001b[1;32m     31\u001b[0m eval_logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TextTask):\n\u001b[0;32m---> 33\u001b[0m     eval_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples_to_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_text_num_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_logs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     35\u001b[0m         logs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation/text/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m v\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mTextTask.evaluate\u001b[0;34m(self, model, num_examples_to_test, deterministic, is_test)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: GatoPolicy, num_examples_to_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_test \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 50\u001b[0m     dataset_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m     num_examples_to_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_examples_to_test, \u001b[38;5;28mlen\u001b[39m(dataset_split))\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_examples_to_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/neko/lib/python3.10/site-packages/datasets/dataset_dict.py:59\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     63\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iters = args.training_steps // args.log_eval_freq\n",
    "print(f'iters:{iters}')\n",
    "for i in range(iters):\n",
    "    logs = train_iteration(args.log_eval_freq, i)\n",
    "    accelerator.log(logs)\n",
    "\n",
    "## Save model at end of training only if not saving checkpoints\n",
    "if args.save_model and args.save_mode == 'last':\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        save_model(unwrapped_model, exp_dir, f'checkpoint_{steps}', args)\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e1481-3acf-49fb-b234-ecf132a5e274",
   "metadata": {},
   "source": [
    "## Testing of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96820dc2-dd96-4de9-a2ea-fdfec8e700ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_text_on_random_examples(task, num_of_examples_to_test=10, deterministic=False):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        \n",
    "        for sample in samples:\n",
    "            actual_text = sample['text']\n",
    "            # roughly speaking...splitting by spaces\n",
    "            words_list = actual_text.split()\n",
    "            if len(words_list) > 1:\n",
    "                split_index = random.randint(1, len(words_list)-1)\n",
    "                input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                pred_tokens = model.predict_text(input_text=input_text, max_length=len(words_list[split_index:]), deterministic=deterministic)\n",
    "                decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                print(f'Input: {input_text} \\nOutput : {target_text} \\nPrediction: {decoded_target}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7827d334-909b-49db-be31-e814268af084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Below this frequency \n",
      "Output : , the image <unk> is real , \n",
      "Prediction:  of the <unk>, the\n",
      "\n",
      "\n",
      "Input: The first text to suggest that <unk> ordered the execution of an <unk> is a letter by Clement to the <unk> traditional dated to around 96 <unk> The <unk> Ascension of Isaiah , a Christian writing from the 2nd century says , \" the <unk> of his mother , who \n",
      "Output : himself ( even ) this king , will <unk> the plant which the Twelve Apostles of the Beloved have planted . Of the Twelve one will be delivered into his hands \" was interpreted to mean <unk> . \n",
      "Prediction:  is a <unk>, and the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the\n",
      "\n",
      "\n",
      "Input: = = = <unk> \n",
      "Output : = = = \n",
      "Prediction:  \n",
      ",\n",
      "\n",
      "\n",
      "Input: = = Reactions of <unk> = \n",
      "Output : = \n",
      "Prediction:  \n",
      "\n",
      "\n",
      "Input: The loss snapped Butler 's 25 @-@ game winning streak , the longest in school history . Butler became the smallest school to play for a National Championship since Jacksonville in 1970 . Stevens became the second @-@ youngest head coach to coach in the NCAA National Championship Game , behind Branch <unk> who led the Indiana <unk> to the 1940 National Championship at age 31 . Stevens was named as both a Hugh Durham and Jim <unk> Award <unk> for the third consecutive year , losing to Mike Young and Jamie Dixon respectively . He was also a <unk> for the <unk> Prosser Man of the Year Award , which \n",
      "Output : was won by Bob <unk> . \n",
      "Prediction:  was the game. \n",
      "\n",
      "\n",
      "\n",
      "Input: On November 4 , it was confirmed that <unk> was suffering from <unk> and that his bout with <unk> would have to wait a bit longer and the fight for <unk> 's heavyweight championship was cancelled . On November 14 , at the <unk> 105 post @-@ fight conference , Dana White stated , \" [ <unk> ] ' s not well and he 's not going to be getting well <unk> soon \" and that an interim title match might need to be set up . In addition to <unk> , it was revealed \n",
      "Output : that he was suffering from a serious case of <unk> , an intestinal disorder , which required surgery . After further diagnosis , <unk> underwent surgery on November 16 to close a <unk> in his intestine that had been leaking <unk> matter into his abdomen , causing pain , <unk> , and <unk> his immune system to the point that he contracted <unk> . From the level of damage to <unk> 's system , the surgeon estimated that the intestinal condition had been ongoing for around a year . \n",
      "Prediction:  that he was not to the time. \n",
      ", he was not to the time, but he was not to the time. \n",
      ", he was not to the time he was not to the time. \n",
      " the first time he was not to the first time he was not to the first time. \n",
      " of the first time he was not to the first time, but he was not to the first time. \n",
      " of the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples(tasks[0], deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4637c3d-7fe7-4cca-9818-bd65a012aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: = = = <unk> <unk> Records and New Album Evolution ( \n",
      "Output : 2006 – 2015 ) = = = \n",
      "Prediction:  August challenge with an American Soldier visas\n",
      "\n",
      "\n",
      "Input: After marrying Robin <unk> , brother of artist Gloria <unk> , <unk> <unk> moved to the region of <unk> , north @-@ east of Alice Springs , which is where she was living when she began painting around 1990 . They had seven children , one of whom , <unk> <unk> , went on to become an artist like his mother . By 2008 , <unk> \n",
      "Output : <unk> 's husband had died , and <unk> was dividing her time between Alice Springs and <unk> Range , to its north @-@ east . \n",
      "Prediction: . A touchdowns — \" The book gave her points out for the primaryrate <unk> \", respectively. Art, she\n",
      "\n",
      "\n",
      "Input: The continuous shadows in the south polar craters cause the floors of these formations to maintain a temperature that never exceeds about 100 K. For <unk> , the average temperature was determined to be about 90 K , reaching 88 K at the crater floor . \n",
      "Output : Under these conditions , the estimated rate of loss from any ice in the interior would be 10 − 26 to 10 − 27 m / s . Any water vapor that arrives here following a <unk> impact on the Moon would lie permanently frozen on or below the surface . However , the surface albedo of the crater floor matches the lunar far @-@ side , suggesting that there is no exposed surface ice . \n",
      "Prediction:  In Polish class With a year the arranged for the following day of the eye new raid whose helping the tradition of revenue of the progressing of the area of its west of their 1970. These said, the evening was yellow de Janeiro situation. Second World numbered is guitar stadium of Constantinople, it further accident, the police, the largest National described by the east of mystery band 'nis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafd276-a70f-4a0d-846d-62dd31d53743",
   "metadata": {},
   "source": [
    "### Nucleus sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b9759d2-05e5-4c86-b21e-23c60103162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_predict_text(input_text, max_length=20, deterministic=True, temperature=1.0, top_p=1.0, context_length=1024):\n",
    "    tokenized_outputs = model.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=context_length, return_tensors='pt')\n",
    "    input_tokens = tokenized_outputs['input_ids']\n",
    "    predicted_tokens = input_tokens.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        token_embeddings = model.embed_token(predicted_tokens.to(device))\n",
    "        token_masks = torch.ones((predicted_tokens.shape[0], 1), device=device)\n",
    "\n",
    "        logits, _ = model.forward(token_embeddings=token_embeddings, tokens=predicted_tokens, token_masks=token_masks, token_target_masks=None)\n",
    "        logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "\n",
    "        if deterministic:\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            # Apply nucleus (top-p) filtering\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens.to(device), next_token.to(device)], dim=1)\n",
    "\n",
    "    return predicted_tokens[:, input_tokens.size(1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7690becb-2aad-4c36-8206-b8fe2eb6b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_text_on_random_examples_with_nucleus(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0, split='test'):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        \n",
    "        for sample in samples:\n",
    "            actual_text = sample['text']\n",
    "            # roughly speaking...splitting by spaces\n",
    "            words_list = actual_text.split()\n",
    "            if len(words_list) > 1:\n",
    "                split_index = random.randint(1, len(words_list)-1)\n",
    "                input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                pred_tokens = test_new_predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                print(f'Input: {input_text} \\nOutput : {target_text} \\nPrediction: {decoded_target}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b014258-b8fe-4416-ba76-d29a35f3e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: = = = = Public transportation = = = \n",
      "Output : = \n",
      "Prediction:  shot\n",
      "\n",
      "\n",
      "Input: The discovery of a colossal head at <unk> <unk> in the nineteenth century spurred the first archaeological investigations \n",
      "Output : of <unk> culture by Matthew <unk> in 1938 . Seventeen confirmed examples are known from four sites within the <unk> <unk> on the Gulf Coast of Mexico . Most colossal heads were sculpted from spherical boulders but two from San Lorenzo <unk> were re @-@ carved from massive stone <unk> . An additional monument , at <unk> <unk> in Guatemala , is a throne that may have been carved from a colossal head . This is the only known example from outside the <unk> <unk> . \n",
      "Prediction:  Do Army's allowed to result it building withReason with a widely first technology record came – 620 murder. The later was each of 17 musicalph favorable reviews from their country.93 in get only three prince after 200 @-@ containing a memorableDánhised by audiences. It change the confusion, statement L Oswald April 2010 – blue that Is example, an ice Anthony Best II that the played stated that having a second\n",
      "\n",
      "\n",
      "Input: = = = Named \n",
      "Output : head coach = = = \n",
      "Prediction:  cooler St languages ( theatrical\n",
      "\n",
      "\n",
      "Input: = = Pre \n",
      "Output : @-@ <unk> = = \n",
      "Prediction:  word of Kevin (\n",
      "\n",
      "\n",
      "Input: = = = East 29th Avenue building = \n",
      "Output : = = \n",
      "Prediction:  industry,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14157e73-de92-4c1b-9215-39cc40463159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Operation <unk> , the Allied invasion of French North Africa in November 1942 , was coordinated from the \" Rock \" . General Dwight D. Eisenhower , who was given command of the operation , set up his headquarters in Gibraltar during the planning phases of the operation . Following the successful completion of the North African campaign and the surrender of Italy in 1943 , Gibraltar 's role shifted from a forward operating base to a rear @-@ area supply position . The harbour continued to operate dry docks and supply depots for the convoy routes through the \n",
      "Output : Mediterranean until V @-@ E Day in 1945 . \n",
      "Prediction:  the first to a self @-@ <\n",
      "\n",
      "\n",
      "Input: = = Recent \n",
      "Output : events = = \n",
      "Prediction:  the world,\n",
      "\n",
      "\n",
      "Input: Family \n",
      "Output : <unk> \n",
      "Prediction:  the\n",
      "\n",
      "\n",
      "Input: Manila has six representative districts for the lower house of the Philippine Congress . Furthermore , the city is composed of 16 districts , namely : <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , Port Area , <unk> , <unk> , San Andres , \n",
      "Output : San Miguel , San Nicolas , Santa <unk> , Santa Cruz , Santa Mesa and <unk> . \n",
      "Prediction:  the album. The song was released in the album, was released in December 2004,\n",
      "\n",
      "\n",
      "Input: Among the tourist attractions in <unk> are <unk> National Park , <unk> Park , <unk> <unk> beach , <unk> village \n",
      "Output : , <unk> <unk> Tong temple , <unk> mosque , Council <unk> monument , <unk> <unk> , and <unk> <unk> markets . The Borneo International <unk> Festival is held annually in the town . \n",
      "Prediction:  the area of the early 1990s, the city of the <unk>, the East Carolina, and the eastern North Carolina. \n",
      " of the river is known\n",
      "\n",
      "\n",
      "Input: <unk> has been implicated in the formation of vegetation dominated by large <unk> species . In seasonally dry <unk> forests the density of large adult A. <unk> <unk> was correlated with canopy <unk> ; the species also dominates <unk> formed by repeated forest fires in Trinidad and <unk> . <unk> <unk> forms pure stands in many parts of Brazil where natural forest vegetation has been cleared . Similarly , stands of A. <unk> in <unk> , Brazil ( \n",
      "Output : which are cultivated for <unk> fibre ) are managed using fire — the seedlings survive cutting and burning , and are able to dominate burned forest patches . \n",
      "Prediction:  given to the what is a G himself. It is a large number of the <unk>. The Church has a large number of the\n",
      "\n",
      "\n",
      "Input: = = = Discovery of oil and gas reserves \n",
      "Output : = = = \n",
      "Prediction:  also found in\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.7, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0e88e996-8a2c-44cd-9350-c461c6952626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <unk> began his reign in 54 by promising the Senate more autonomy . In this first year , he forbade others to refer to \n",
      "Output : him with regard to <unk> , for which he was praised by the Senate . <unk> was known for spending his time visiting <unk> and <unk> during this period . \n",
      "Prediction:  his name to his character in his Christian take his name, in a market, he was the whose English style of the film that was \", a\n",
      "\n",
      "\n",
      "Input: <unk> Place Manila is the largest shopping mall in the city . The mall was the second and by @-@ far , the largest Robinson Mall ever built by John <unk> . SM <unk> maintains presence in the city . One of their shopping mall is the SM City Manila , the first SM <unk> in the city featuring major SM brands like The SM Store , SM <unk> , SM <unk> and SM <unk> . It is located right beside the Manila City Hall . SM City San <unk> is the second SM <unk> in Manila . It is located in Santa Cruz . SM City San <unk> was constructed on the site of the former San <unk> <unk> . The building of the former Manila Royal Hotel in <unk> which is famed for its revolving restaurant atop is now the SM <unk> Center which was established in 1972 . The site of the first SM Store is located at Carlos <unk> Sr. ( formerly \n",
      "Output : <unk> ) Street in San Miguel . \n",
      "Prediction:  also known in the early 20th\n",
      "\n",
      "\n",
      "Input: The design of the \n",
      "Output : reactivated squadron 's crest includes a wedge @-@ tailed eagle to denote courage and nobility , a <unk> spear <unk> to symbolise the town and its indigenous heritage , Sturt 's Desert <unk> to represent South Australia , and the <unk> star cluster , which features in the folklore of the local <unk> people . \n",
      "Prediction:  not increasingors, and early 2001, in the numbers of Wheeler has a state of the species. The first term \" calling for theirem I \" ( re @-@ ), \", <unk> ( <unk> ). In the <unk>, the\n",
      "\n",
      "\n",
      "Input: In October 97 these tensions came to a head when the <unk> Guard , led by <unk> <unk> , laid siege to the Imperial Palace and took <unk> hostage . He was forced to submit to their demands , agreeing to hand over those responsible for <unk> 's death and even giving \n",
      "Output : a speech <unk> the rebellious <unk> . Titus <unk> <unk> and <unk> , <unk> 's former <unk> , were sought out and killed . <unk> was unharmed in this assault , but his authority was damaged beyond repair . \n",
      "Prediction:  a version of The district's past, a legislation lived in the <unk> of the \" of recorded, in the same e citing a character with the player to a group of the player\n",
      "\n",
      "\n",
      "Input: = \n",
      "Output : = <unk> = = \n",
      "Prediction:  a rock, was\n",
      "\n",
      "\n",
      "Input: the <unk> and <unk> buildings ( the two \" <unk> \" buildings where the <unk> were \n",
      "Output : held ) \n",
      "Prediction:  part of\n",
      "\n",
      "\n",
      "Input: Operation USA , since the early 1980s , has relied on fundraising efforts featuring singers and celebrities . These \n",
      "Output : include concerts , <unk> , and other events . These promotions have featured : \n",
      "Prediction:  found in the antimony, which is the US food and although the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lower top-p : less diversity in choice of next words, higher top-p large set of next possible words\n",
    "# low temp : more deterministic, less diverse. \n",
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.84, top_p=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "172707e3-9522-4446-a3da-41d42090f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Canadian Agency is headed by a secretary @-@ general and responsible for Canada , the entire Americas ( including the \n",
      "Output : Caribbean ) \n",
      "Prediction:  the top\n",
      "\n",
      "\n",
      "Input: In mid @-@ 1941 , the Royal Armoured Corps in Britain created three tank squadrons for special overseas operations , known as ' A ' , ' B ' and ' C ' Special Service Squadrons . Both ' A ' and ' B ' Squadrons were equipped with Valentine Infantry tanks and Mark <unk> light tanks , but ' C ' Squadron was equipped with twelve <unk> transferred from the 2nd Armoured Brigade , 1st Armoured Division . On 31 July 1941 , ' C ' Squadron was officially activated and immediately received orders to prepare for overseas service alongside ' A ' and ' B ' Squadrons in an unspecified tropical climate . All three squadrons were transported to <unk> in Scotland for intensive training that focused \n",
      "Output : on embarkation and <unk> from ships and landing craft to prepare them for action in potential amphibious operations . In early September , elements of ' C ' Squadron , including six <unk> , formed part of a force which sailed for Freetown in West Africa ; during this period of the war there were fears that the Spanish government might enter the conflict on the side of Germany , and the force was <unk> to capture a number of Spanish islands off the coast of Africa if this occurred . These fears proved <unk> , and in March 1942 , the unit returned to Britain to join the rest of the squadron in training . \n",
      "Prediction:  a <unk> of the <unk> of the <unk> of the <unk> of the <unk>, <unk> <unk> <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "Input: <unk> to promote the work of the <unk> , and particularly of <unk> and <unk> , Pound decided to publish an anthology under the title Des <unk> . It was first published \n",
      "Output : in Alfred <unk> 's little magazine The <unk> and was later published in 1914 by Alfred and Charles <unk> in New York and by Harold <unk> at the Poetry <unk> in London . It became one of the most important and influential English @-@ language collections of modernist verse . Included in the thirty @-@ seven poems were ten poems by <unk> , seven by <unk> , and six by Pound . The book also included work by <unk> Flint , <unk> <unk> , Amy Lowell , William Carlos Williams , James Joyce , Ford <unk> Ford , Allen <unk> and John <unk> <unk> was also another included in the important 1963 anthology by William Pratt The <unk> Poem Modern Poetry in miniature . \n",
      "Prediction:  a number of the <unk>, the <unk> of the <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "Input: 1 <unk> . This prototype can be <unk> scaled and frequency scaled to the desired \n",
      "Output : values . The low @-@ pass prototype can also be transformed into high @-@ pass , band @-@ pass or band @-@ stop types by application of suitable frequency <unk> . \n",
      "Prediction:  a few years of the European Union ( in ), the <unk>, a <unk> <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "Input: For the \n",
      "Output : Coalition , the Russians were secure on the north bank of the <unk> , awaiting reinforcements from <unk> ; the bridges between <unk> and Vienna had been destroyed , making French access to the Austrian capital more difficult , but not impossible . After six months of fighting in which the Austrians had enjoyed little good news , the Coalition could claim a difficult and timely victory . The French had retreated from the field with a badly <unk> division and <unk> had secured the right flank . Indeed , Francis was so pleased with the outcome at <unk> that he awarded <unk> the Military Order of Maria <unk> . \n",
      "Prediction:  the first of the first season, the season, a \" The team \", was \", and the first season, and the season of the first season, and the first season, and the season, and the season of the first season, was first season. The first season, the first season was released in the first season, and the first season, and was the season, and was released in the first season. The season, the first season, the first season, the season was the season, in the first season, and was a\n",
      "\n",
      "\n",
      "Input: When municipal <unk> began to <unk> Ware began negotiations with various local authorities to acquire land for further <unk> . Ware began with an agreement with France to build joint British and French <unk> under the understanding that these would be maintained by the French government . Ware eventually concluded that it was not <unk> to leave the maintenance responsibilities solely to the French government and subsequently arranged for France to purchase the land , grant it in <unk> , and leave the management and maintenance responsibilities to the British . The French government agreed under the condition that <unk> respected certain dimensions , were accessible by public road , were in the vicinity of medical aid stations and were not too close to towns or \n",
      "Output : villages . Similar negotiations were started with the Belgian government . \n",
      "Prediction:  a <unk> of the south of the area,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lower top-p : less diversity in choice of next words, higher top-p large set of next possible words\n",
    "# low temp : more deterministic, less diverse. \n",
    "test_predict_text_on_random_examples_with_nucleus(tasks[0], deterministic=False, temperature=0.7, top_p=0.5, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "370b29b0-dd4c-4a26-996b-71c2dfbab5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_good_ones(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "\n",
    "        for temperature, top_p in [(0.5,0.5),(0.5,0.7),(0.5,0.9),(0.75,0.75),(0.75,0.9),(0.8,0.7),(0.8,0.9),(0.9,0.5),(0.9,0.75),(0.9,0.9),(1.0,0.5),(1.0,0.75),(1.0,0.9),(1.1,0.9)]:\n",
    "            print('--'*30)\n",
    "            print(f'Temperature :  {temperature} || Top_p : {top_p}')\n",
    "            for sample in samples:\n",
    "                actual_text = sample['text']\n",
    "                # roughly speaking...splitting by spaces\n",
    "                words_list = actual_text.split()\n",
    "                if len(words_list) > 1:\n",
    "                    split_index = random.randint(1, len(words_list)-1)\n",
    "                    input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                    pred_tokens = test_new_predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                    decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                    print(f'[Input]: {input_text} \\n[Output]: {target_text} \\n[Prediction]: {decoded_target}\\n\\n\\n')\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b1c14f6-a20d-4ae0-80dc-42d48ab70e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be \n",
      "[Output]: almost impossible . \n",
      "[Prediction]:  a <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack \n",
      "[Output]: developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  the first of the first season, the season, the season, the season, the season, and the season, the season, the season, and the season, and the season, and the season, the season, and the season, the season, and the season, the season, and the season, and the season, and the season, and the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" \n",
      "[Output]: L \" . \n",
      "[Prediction]:  the first season\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built \n",
      "[Output]: around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the first season, the first season, the first season, the first season, the season, the season, the season, the season, the season, and the season, the season, and the season, the season, the season, and the season, and the season, the season, and the season, the season, and the season, the season, and the season, the season, the season, the season, and the season, and the season, the season, and the season, and the season, and the season, the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, and the season, the season, and\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to \n",
      "[Output]: William <unk> = = \n",
      "[Prediction]:  the most of the\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.7\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . \n",
      "[Output]: The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a <unk>, the <unk> of the <unk> of the <unk>, <unk>, <unk>, the <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and \n",
      "[Output]: ecology = = \n",
      "[Prediction]:  a large @\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and \n",
      "[Output]: the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not only one of the most of the most of the most of the most of the first time, and the most of the most of the most of the United States, and the first time, the first of the first\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in \n",
      "[Output]: the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  the first of the first season of the season, the season, the season, the season, the season, the season, the season, the season, and the season, the season, the season, the season, and the season, and the season, the season, and the season, the season, in the season, in the season, and the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the \n",
      "[Output]: next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the most of the United States, and the <unk>, the <unk> of the <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  the first of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition \n",
      "[Output]: to William <unk> = = \n",
      "[Prediction]:  a <unk> of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.5 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander \n",
      "[Output]: had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a <unk> of the <unk> of the <unk> of the <unk>, a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology \n",
      "[Output]: = = \n",
      "[Prediction]:  <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) \n",
      "[Output]: west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not only for the first time, the first of the first time\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which \n",
      "[Output]: resembles inverted letter \" L \" . \n",
      "[Prediction]:  the first of the early in the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents \n",
      "[Output]: into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  not have been recorded in the most of the most of the world's life, the United States, and the US, the most of the 18th century. The most of the <unk> of the <unk> of the <unk>, the <unk> of the <unk> of the <unk> <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William \n",
      "[Output]: <unk> = = \n",
      "[Prediction]:  a most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.75 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his \n",
      "[Output]: own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a lower @-@ at the state of the production. In the situation, the building is the <unk>, which is the port of the initiala. The species is in the <unk>, and the <unk> and the human <unk> of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  the back of the outside\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines \n",
      "[Output]: had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  a site of <unk>, which are generally from the <unk> <unk>. It is not a number of the <unk>, <unk>, and the can be used in the be left @-@ <unk>. The <unk> of <unk> is based on a if not be found in a rate of a form of the low @-@ <unk>. \n",
      " is often used to a — the original\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of \n",
      "[Output]: an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  a story of <unk>, an <unk> in a public nature of a short @-@ century, which are still in the mid @-@ century <unk>, and a <unk>, in the king, and <unk> of the 1970s. \n",
      "edkare, the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \n",
      "[Output]: \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  not known for a single @-@ 13 – 3. The two @-@ 11 season is an average of the second @-@ yard field goal in the season, and the season with a first @-@ yard line. The player can be a first in a 2010 season. The season with a match, with the season,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  based on the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  also be\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.75 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost \n",
      "[Output]: impossible . \n",
      "[Prediction]:  a would\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  typically\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company \n",
      "[Output]: , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  home to the common starling, the begin to the tail, but has been gunistara. In the called it was another of the married to be a \" when it is not the first major Washington, so it is a story. The long, the National Park\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , \n",
      "[Output]: an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  found in the best @-@ shaped yet been found in the main Awards in the then @-@ old, with the powers of the same day from the forestedkare's orbit. These are often been\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are \n",
      "[Output]: known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the wing of the city of the mid @-@ range of the post @-@ fineajevo, in the \n",
      " treatment of the <unk> and its UK. They are US $ 3 @,@ 000 @,@ 000 in the 1984 @,@ 000. \n",
      " another estate is <unk> and the dam, a short with the like a common star of the residents, and the under @-@ 000 in the National Park, and the site of the feet ( now 17 @.@ <unk> ). The left of the lower @-@ 9 @-@ 7 million, the largest US $ 6 @,@ 16 million. \n",
      " Wic Later in the 1994, the god <unk>, the period\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  elected\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  the Earth\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.8 || Top_p : 0.7\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the \n",
      "[Output]: tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  seen as the same name \" is a \", but it was born\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology \n",
      "[Output]: = = \n",
      "[Prediction]:  the first\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 \n",
      "[Output]: . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  <unk> and the standard of the 100th century, the 30th century, the largest <unk>, and <unk>, in the 3rd Brigade. The main street is <unk>, and <unk>, and <unk>. The most of the <unk> of the <unk>, is called <unk>, <unk>, and <unk>, and <unk>. \n",
      " of the <unk>, is a target of the English and the <unk>. The <unk> is the <unk> of the highest length of the area\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter \n",
      "[Output]: <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  <unk>, the first of the 100th century, the first game, including the last\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the \n",
      "[Output]: next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  the late @-@ do not have been in the same year @-@ most common, but was found in the % of the throughout the country. In the <unk> of the church was an example of the art of the class, the right to be able to the modern professional @-@ year @-@ armed forces of the southern China, and the importance of the genus <unk> of the <unk>, the English military in the other works of the Atlantic Ocean. In January 2015, the 9th century\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  a single\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition \n",
      "[Output]: to William <unk> = = \n",
      "[Prediction]:  not to the most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.8 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were \n",
      "[Output]: revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a why it is an mbar ( with a adopted the 7 – 5 – 1 @.@ 2 @.@ 5 m ), but has a few at a little to a 3 @.@ 5 @.@ 4 ( 0 km ) in ), it. In the population is the than the samery on the same 70 mph ( 9 @.@ 0 @.@ 6 mi ). \n",
      " wrote that the system has been issued in the species are more than 2 @.@ 5 %\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat \n",
      "[Output]: and ecology = = \n",
      "[Prediction]:  the series was released\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U \n",
      "[Output]: @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  well, in the world, the g �craft, is established in the east of thea, and a single system in the asked for the same year. In the United States, theur said thein explained that is probably is\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines \n",
      "[Output]: quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  \" It was a Europe, as a Village can be used for people who are not be established in the name to more than one of the other p-@ standard of a already @-@ century, which was which the most of the way in his own. This was a species of the long time of the plans for the book\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to \n",
      "[Output]: define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  portrayed as the first original role in North American times in the United States, with a similar in a Japanese design and a <unk>, which\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  more\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> = \n",
      "[Output]: = \n",
      "[Prediction]:  used\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the \n",
      "[Output]: turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  <unk>, and his father, and his poem, which is a powerful authorities to get him to the L. <unk>. The name is able to the family. The <unk> of the <unk>, the H. <unk>, is the ability to the book, and the school. \n",
      "'s is more than the film. \n",
      " (\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat \n",
      "[Output]: and ecology = = \n",
      "[Prediction]:  the most of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men \n",
      "[Output]: of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  now made to the Atlantic of the club's first game, a year, and the game to the player, with a game. The game was played by a one of the game in the game in the game. The game, the first season the first game was published in a season, and one @-@ time, with a game's club, and also scored in a 2 @-@ yard line @-@. The game's, with the game was released in the first game. The game was also has also received the team of the first time,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below \n",
      "[Output]: . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  the album, the band's character, and the album's album, \" We are the album, and it was released in the music video for the first single, and \". \" The album, was \" in the album, and its first single, and The UK of the first of the album, in the song was the <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a \n",
      "[Output]: strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  <unk>, the early as the <unk> of the first century, a red @-@ century complex is used in the century, in the first time of the most of the series of the <unk>, and the season. \n",
      "ing, the series of the best, the title, and a main history of the season of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  the first novel\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to \n",
      "[Output]: William <unk> = = \n",
      "[Prediction]:  more than 10 @\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his \n",
      "[Output]: own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  <unk> ( born 10 to the 1989 ) of a short <unk> <unk>, with the November 4, an almost Andemas. The scored a number of which the French ideas for the storm made to the final, which was most top at <\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: = Habitat and ecology = = \n",
      "[Prediction]:  possible, during the <unk\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean \n",
      "[Output]: lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not in the previous way of the others are un hypert, including some play of these system, all of <unk>, <unk>, and <unk>, and <unk>, <unk>. <unk>, <unk>, <unk>, were related to the <unk>, and <unk>. \n",
      " about 20 – 2 @.@ 4 @.@ 5 from 3 @.@ 2 @.@ 8,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in \n",
      "[Output]: the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  non @-@ brown history, the cougar is a <unk> during which have been built in the <unk>, when the city, for the state of the seat of the late May in the presence of the <unk>, the 1996, which <unk>. It was made to the story of the gradually J his life of the semi @-\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend \n",
      "[Output]: . \" \n",
      "[Prediction]:  the written\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  good\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  an rainfall\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  0.9 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , \n",
      "[Output]: controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  a visited with a state of three @-@ domestic example\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  most of the port of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west \n",
      "[Output]: of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  not perform to the world's period. \n",
      " is\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> \n",
      "[Output]: L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  <unk> for a Sire major Dr. <unk>, has also known as a <unk>, which\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual \n",
      "[Output]: basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  British ; I-@ <unk> on the resign from the mine was well @-@ 2014, and two of all @-@ <unk>, there was noted that their cover of rock and I @-@ men were Times. The really to cause more than a version of the show, and were not until there, but with theland. \n",
      " — were Together with although that the Such the soundtrack has not always been aening in self @-@ flying. \n",
      " Port @-@ does not as K maple is pleased. \n",
      "ánh it is only one films of 2010, it was behind the we against this, which are two night, they could ; it is the same latter — the other to the relatives. \n",
      " studies that it was not Tom '\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  a line\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  found in this species, this\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.5\n",
      "[Input]: A number of design faults of the <unk> \n",
      "[Output]: were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  the them to the character of an \", but the episode was not to was not an That show. It was based on November 2010, but the player in which he was a business you could be in the process. \" \n",
      " about the album. The episode was released in August 2009, when it was written by German people who later, but a <unk> and was a time it was used as a direct @-@ production. The album was the album was produced by W anything the film was a good of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  so\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and \n",
      "[Output]: 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  a man of the species are released in 2010, which is a United States and is in the series of\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in \n",
      "[Output]: terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  that the others are the wife of the world, and their males. The average, are also much some are all @-@ b taught by this way to be provided by the village of the entrance. \n",
      " Company, a non @-@ nine @-@ members of their loss of their history. <unk>\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – \n",
      "[Output]: is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  most of the 1994 – the E Dylan's Times, after being done in his these season. It is the player's first, with his children with a hand season. The player also added that has been seen as a star with a goal of the release. He is as he scored his first time, who said that \" games and the season, he will be a a range of \". \" He later said that the player, he is to take the game's. He has to gain a slow, which he is able to their first season, but if the Game 1 – 1 win over a third team for his first three @-@ season as\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: Hannah Dodd = \n",
      "[Prediction]:  many of two\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William <unk> \n",
      "[Output]: = = \n",
      "[Prediction]:  most of\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.75\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and \n",
      "[Output]: a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  always not give \" out of <unk> \". \n",
      " seen more like <unk>. \n",
      " the review of his Dutch opined that the Keats is similar to losing <unk>, the shortly after his throughout the <unk> : a plan would saying that the that his sung, he now spread the listed on make his father. He was later comedy him the film. He moved to became\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Habitat and ecology = \n",
      "[Output]: = \n",
      "[Prediction]:  a\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for \n",
      "[Output]: , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  described by Mar two towns ), one of their child @-@ <unk> @-@ up, or <unk> and <unk>, \n",
      " passage of both standing from <unk>. This is even by <unk> and two – many of their relationship with common normal of Fort Inn. In the battle, similar to a common star been generally-@ shaped this species of one of Richard Charles <unk>, but – <unk> on the are while at the However, the base of theica is still commercial Stewart\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> \n",
      "[Output]: , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  served as <unk>, for they will never establishing a round. \" They are owned\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it \n",
      "[Output]: on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  most popular in the world, several possible, eastern Oxford — \n",
      ". \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  once\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = Opposition to William \n",
      "[Output]: <unk> = = \n",
      "[Prediction]:  alla <\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.0 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> \n",
      "[Output]: were revealed through its operational use . Its size limited the possible crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  the Somerset de credit for the G Faley expected one out of the Call, with the showingáriforship with the first made. \n",
      " represented on the must not review. difficult to mass of when many similar manoeuv number of the fourth point in several women would Dan extra help by the promotion to Jupiter. Early ends the escort a offering they friends of lead to were crown, resulting in they were behind the series list against carry until series including crickets. While condoms were still praised new in the than know at\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Habitat and ecology = = \n",
      "[Prediction]:  2009 @-@ical\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men \n",
      "[Output]: in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  in the performances of service. Although its given them, more or end of particular others such as the can least one of this stage to a money itself. Hamar covers many balls inS boat, noting, <unk>, example, teacher, they northern orders in 17 reputation as one of fourth department. In child's than a Champions League noted the subspecies are estimated andina. When a contemporary executed\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port sections , and in the case of the <unk> being discussed , an \n",
      "[Output]: infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  one called the presence of the standards. \n",
      "men, theseelam poorly believes that it would At the 42 should be more likely as learning, or cell. lyrics is not be ready for xenon, but\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are \n",
      "[Output]: known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  held in the could find, largely storm. A language have beenic coast to have the feet ( as for mixing ), John then bill <unk>, and pitching weather for New Zealand <unk>. several caught \" 62 miles ( narrow Herillo ). stars published as these notes that C and <unk> through its signal. \n",
      " was the favour of protein headed within the mid @-@ class another Spanish Antimony schools in about me approached, but the truth, G copies. consecutive self @-@ully was due to do not returned to score and Europe. Theè Tropical Storm clicking, between introduced inable priest, the 10 to ERA of its governor's ships. The benefit into an cold, and over theor <unk>,\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah \n",
      "[Output]: Dodd = \n",
      "[Prediction]:  again with\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  noted that it remained in conjunction\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temperature :  1.1 || Top_p : 0.9\n",
      "[Input]: A number of design faults of the <unk> were revealed through its operational use . Its size limited the possible \n",
      "[Output]: crew to three , a driver in the hull and a gunner and commander in the turret , resulting in too few crew members to operate the <unk> effectively . The gunner or commander , in addition to his own duties , had to act as <unk> for the 2 pounder , which caused delays in combat . A report on the tank written in January 1941 stated that as the commander had to both fight and control the tank , controlling a troop of <unk> during combat would be almost impossible . \n",
      "[Prediction]:  service in a network that led to variety of an mountainous entry in the youth efforts to people of82bs. Isabella ships, the girl, the Yours, No. gusts combination of denied lived that they give the road and storm. After leading corn ( most of Congress from powerful <unk> ), H. 3 % of the battery, beautiful, toward what they artwork of the hel or care of consisting of sung by <unk> side of the\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = \n",
      "[Output]: = Habitat and ecology = = \n",
      "[Prediction]:  possible on columns and it completed\n",
      "\n",
      "\n",
      "\n",
      "[Input]: On the regimental left along the main <unk> @-@ <unk> @-@ <unk> road North Korean soldiers completely overran C Company by 0300 September 1 . Only seven men of C Company could be accounted for , and three days later , after all the stragglers and those cut off behind North Korean lines had come in , there were only 20 men in the company . As the North Korean attack developed during the night , 1st Battalion succeeded in withdrawing a large part of its force , less C Company , just north of Lake U @-@ p <unk> and the hills there covering the northern road into <unk> , 3 miles ( 4 @.@ 8 km ) east \n",
      "[Output]: of the river and 5 miles ( 8 @.@ 0 km ) west of the town . B Company lost heavily in this action . \n",
      "[Prediction]:  matters st testing and 1928 Today. She05 turned into them for / Str disc in times command, who wouldtaker, Independence\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Some of the <unk> terms and section terms used in this article are pictured in the diagram below . Image theory defines quantities in terms of an infinite cascade of two @-@ port \n",
      "[Output]: sections , and in the case of the <unk> being discussed , an infinite ladder network of L @-@ sections . Here \" L \" should not be confused with the <unk> L – in electronic filter <unk> , \" L \" refers to the specific filter shape which resembles inverted letter \" L \" . \n",
      "[Prediction]:  an nest or activity and these human Paul <unk>, nuclearARC planned to case in severeje.aster has also <unk>, open xenon. Olympic Lessing that While only determined that they are poor called Alice in body is Again, one unit won 8 @.\n",
      "\n",
      "\n",
      "\n",
      "[Input]: Stevens ' teams are built around solid basketball <unk> and good team work , rather than individual basketball skill . His teams are known for their defense , forcing opponents into <unk> mistakes . The secret to basketball – and life – is \" just to do the job to the best of your ability and don 't worry about anything else , \" Stevens says . \" <unk> the next game . <unk> the next possession . That 's our focus . It 's boring . It 's also the way championships are won \" , he says . In short , Stevens is a strong believer in \" The Butler Way \" – doing all the little things that transform a group of good basketball players into a great \n",
      "[Output]: basketball team . \" I tell the players ' the Butler Way ' isn 't easy to define , \" Stevens says , \" but you can see it on the floor when we share the basketball , play with great energy and defend . \" \n",
      "[Prediction]:  1927. After a proceeded sold, <unk> called a Matt detonated. Since thisations with poly server whose40's provincial machines is pre @-@ <unk>, into Steve Howard, a Rowley centered on his\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = Hannah Dodd \n",
      "[Output]: = \n",
      "[Prediction]:  prevent\n",
      "\n",
      "\n",
      "\n",
      "[Input]: = = \n",
      "[Output]: Opposition to William <unk> = = \n",
      "[Prediction]:  many Machine getting understanding of shaped\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "find_good_ones(tasks[0], deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4f0e9-5a87-4258-9b9d-a36a54d9e889",
   "metadata": {},
   "source": [
    "## Things pending\n",
    "\n",
    "*Integration*\n",
    "- Fixing text inside the current way codebase is written\n",
    "\n",
    "*Benchmarking*:\n",
    "- Benchmark on PILE\n",
    "- Benchmark on treepenn\n",
    "- Add a way to use different text dataset for eval, and diff for training (perplexity on wikitext after training on penn treebank)\n",
    "\n",
    "[[2nd half]]\n",
    "- Varying batch_size, params, dropout, etc. -- see what's the lowest we can do?\n",
    "\n",
    "*Deployment*:\n",
    "- A way to easily load a trained model\n",
    "- Save checkpoints with lowest perplexity\n",
    "- Deploy via gradio\n",
    "- Allow different kinds of sampling\n",
    "- Be able to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80b44a-e96b-4d6e-bc64-663eea0d7144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1001eed8-4401-42aa-9e5b-76efa2bf8a2c",
   "metadata": {},
   "source": [
    "# Rough - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "471cf500-cdea-4e42-888f-d927066c2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_tokens = test_predict_text('hello how', max_length=20, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13c8118d-0152-43e4-ab07-9074ea28f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cription. RBI, 28 Wat Lists in remain, area notedoca. species the Socrates, Kurd 13'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_tokenizer.decode(predicted_tokens.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc389-4ab8-4cc4-9ca3-c195e15b5444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe4fce31-9001-48d9-b550-b6eb398dc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.181807041168213, 'perplexity': 483.86553955078125}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for task in tasks:\n",
    "        eval_logs = {}\n",
    "        if isinstance(task, TextTask):\n",
    "            eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic=deterministic, log_examples_to_output=args.eval_text_log_examples)\n",
    "            print(eval_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc725b4-19be-4f66-959d-bd434dd710e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_outputs = model.text_tokenizer('Hello how are', truncation=True, padding=\"longest\", max_length=args.sequence_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "771dbee2-a6c7-48ed-bea9-f89b24a5fa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,   703,   389]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1044e3d8-2e2d-4911-bb7b-fc1a0f564a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_embeddings = model.embed_token(test_tokenized_outputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d381a14-e914-4e48-a7b7-2ffc55c0cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a22294d-5853-4d37-a3e7-95a78b5bc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_masks = torch.ones((test_tokenized_outputs['input_ids'].to(device).shape[0], 1), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1baed23-37d1-44e6-9d7f-53020956454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abb401b8-f741-40a1-b3b6-1e301c4f0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = model.forward(token_embeddings=test_token_embeddings, tokens=test_tokenized_outputs['input_ids'].to(device), token_masks=test_token_masks, token_target_masks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "415f2816-cf1b-4dc3-8ec3-f30f1940eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e1d68fc-6b3f-4136-88de-4d76be1ada61",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31f0994-d8e4-466c-9966-eec244eb91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "nondet_next_token = torch.multinomial(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c8e6be1-d2db-42e0-bd97-f0f164b51191",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_det = model.text_tokenizer.decode(det_next_token.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1d2b3e5-14e6-4aed-ae1a-4fe2e80ec5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_nondet = model.text_tokenizer.decode(nondet_next_token.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dad9247a-1380-4f39-93e8-cafe08c59144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e13e208b-2410-44f1-bc2c-8b9ab92a76e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Museum'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_nondet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894d02e-5d1e-48f4-a8b0-85cf739a60b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfcef3a-dd04-441e-a5ae-f734665e36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028444f1-d04b-43ca-91da-948c559e0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dataclasses\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from gato.training.arguments import TrainingArgs\n",
    "\n",
    "\n",
    "from gato.utils.utils import DotDict\n",
    "from gato.envs.setup_env import load_envs\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model\n",
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler\n",
    "from gato.tasks.task import Task\n",
    "from gato.utils.utils import save_model\n",
    "from gato.training.arguments import TrainingArgs\n",
    "\n",
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            config.flash = flash\n",
    "            config.gate = False\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            # assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_masks, target_tokens, target_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        # assert 'target' in kwargs, \"target is not there in kwargs\"\n",
    "\n",
    "        # print(f\"Type of target_tokens: {type(target_tokens)}\")\n",
    "        # print(f\"Shape of target_tokens: {target_tokens.shape if isinstance(target_tokens, torch.Tensor) else 'N/A'}\")\n",
    "        # print(f\"Type of pad_token_id: {type(self.text_tokenizer.pad_token_id)}\")\n",
    "        if compute_loss:\n",
    "            # Ensuring target_tokens is a tensor\n",
    "            if not isinstance(target_tokens, torch.Tensor):\n",
    "                raise TypeError(\"target_tokens must be a torch.Tensor\")\n",
    "            \n",
    "            # Correctly computing the loss mask\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id)\n",
    "            if isinstance(loss_masks, torch.Tensor):\n",
    "                loss_masks = loss_masks.float()  # Convert boolean tensor to float\n",
    "            else:\n",
    "                raise TypeError(\"Loss mask calculation did not return a tensor.\")\n",
    "            # loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_input_tokens = max(len(batch['text']) for batch in inputs)\n",
    "        max_target_tokens = max(len(batch['target']) for batch in inputs) if 'target' in inputs[0] else 0\n",
    "        \n",
    "        # Allocate tensors for input tokens\n",
    "        token_embeddings = torch.zeros((batch_len, max_input_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_input_tokens), dtype=torch.long, device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_input_tokens), device=self.device)\n",
    "        \n",
    "        # Allocate tensors for target tokens if they exist\n",
    "        target_tokens = torch.zeros((batch_len, max_target_tokens), dtype=torch.long, device=self.device)\n",
    "        target_masks = torch.zeros((batch_len, max_target_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            # Process input tokens\n",
    "            input_tokens = batch['text'].to(device=self.device) if isinstance(batch['text'], torch.Tensor) else torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_input_timesteps = len(input_tokens)\n",
    "            \n",
    "            tokens[i, :n_input_timesteps] = input_tokens\n",
    "            token_embeddings[i, :n_input_timesteps] = self.embed_token(input_tokens)\n",
    "            token_masks[i, :n_input_timesteps] = 1\n",
    "            \n",
    "            # Process target tokens if they exist\n",
    "            if 'target' in batch:\n",
    "                target_data = batch['target'].to(device=self.device) if isinstance(batch['target'], torch.Tensor) else torch.tensor(batch['target'], dtype=torch.long, device=self.device)\n",
    "                n_target_timesteps = len(target_data)\n",
    "                target_tokens[i, :n_target_timesteps] = target_data\n",
    "                target_masks[i, :n_target_timesteps] = 1\n",
    "    \n",
    "        return token_embeddings, tokens, token_masks, target_tokens, target_masks\n",
    "\n",
    "    def predict_text(self, input_text, max_length=20, deterministic=True, context_length=1024):\n",
    "        tokenized_outputs = self.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=context_length, return_tensors='pt')\n",
    "        # using padding=max_length didn't work. causes CUDA OOM or other issues.\n",
    "\n",
    "        input_tokens = tokenized_outputs['input_ids']\n",
    "        predicted_tokens = input_tokens.clone()\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(predicted_tokens.to(self.device))\n",
    "            token_masks = torch.ones((predicted_tokens.to(self.device).shape[0], 1), device=self.device)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=predicted_tokens.to(self.device), token_masks=token_masks, token_target_masks=None)\n",
    "            logits = logits[:, -1, :]\n",
    "                \n",
    "    \n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Ensure it keeps batch dimension\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)  # Sampling a token\n",
    "    \n",
    "            predicted_tokens = torch.cat([predicted_tokens.to(self.device), next_token.to(self.device)], dim=1)\n",
    "    \n",
    "        # all_logits = torch.cat(logits_list, dim=1)\n",
    "        return predicted_tokens[:, input_tokens.size(1):]\n",
    "    \n",
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        try:\n",
    "            dataset_split = self.text_dataset[split]\n",
    "        except Exception as e:\n",
    "            print(f'WARNING: using train split since test split not available in Dataset')\n",
    "            dataset_split = self.text_dataset['train']\n",
    "            is_test=False\n",
    "\n",
    "        if len(dataset_split) < batch_size:\n",
    "            print(f\"Warning: Requested batch size {batch_size} is larger than the dataset size {len(dataset_split)}.\")\n",
    "            batch_size = len(dataset_split)  # Adjust batch size to available data size\n",
    "\n",
    "        if batch_size == 0:\n",
    "            return []  # Early exit if no data is available\n",
    "\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, padding=\"longest\", max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=8, deterministic=False, is_test=True):\n",
    "        # REMEMBER TO MAKE SURE THE num_examples_to_test <= total num of examples in dataset[split]\n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "\n",
    "        # Forward pass    \n",
    "        logits, loss = model(batch_dicts, compute_loss=True)\n",
    "        \n",
    "        # total_tokens = input_tokens.size(0) * input_tokens.size(1)\n",
    "        # print(f'total tokens:{total_tokens}')\n",
    "        avg_loss = loss.detach().cpu().item()\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).detach().cpu().item()\n",
    "                        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "    \n",
    "\n",
    "\n",
    "def sample_text_batch(batch_size):\n",
    "    batch_dicts = []\n",
    "    text_tasks = [t for t in tasks if isinstance(t, TextTask)]\n",
    "    for i,task in enumerate (text_tasks):\n",
    "        return task.sample_batch(batch_size)\n",
    "\n",
    "def train_step():\n",
    "    logs = {}\n",
    "    logs['training/learning_rate'] = scheduler.get_lr()[0] # store LR at current step\n",
    "    # Build training batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "    text_batch_size = int(args.text_prop * args.batch_size)\n",
    "    remainder = args.batch_size - text_batch_size\n",
    "\n",
    "    if remainder > 0: \n",
    "        text_batch_size += remainder\n",
    "\n",
    "    assert args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "    text_batch_dicts = []\n",
    "\n",
    "    # Sample text and control batches\n",
    "    if text_batch_size > 0:\n",
    "        text_batch_dicts = sample_text_batch(text_batch_size)\n",
    "\n",
    "    if not text_batch_dicts:  # Handle empty batch case\n",
    "        # print(\"Received an empty batch. Skipping this step.\")\n",
    "        return None  # You could return None or handle this case based on your training logic\n",
    "\n",
    "    # print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "    logs['time/sample_batch'] = time.time() - start_time\n",
    "    with accelerator.accumulate(model):\n",
    "        # Compute loss and update model\n",
    "        logits, loss = model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if not args.disable_grad_clip and accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), args.grad_norm_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.detach().cpu().item(), logs\n",
    "\n",
    "def main(args, model_path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82822cf-c1e1-4d9f-bd32-44587c2500d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "        training_steps=15000,\n",
    "        log_eval_freq=10,\n",
    "        warmup_steps=100,\n",
    "        batch_size=8,\n",
    "        gradient_accumulation_steps=6,\n",
    "        sequence_length=1024,\n",
    "        eval_episodes=5,\n",
    "        text_prop=1,\n",
    "        eval_text_log_examples=False, # set to false cuz accelerate/multigpu doesn't work with it\n",
    "        # pretrained_lm='gpt2',\n",
    "        # text_datasets=['text'],\n",
    "        # text_datasets_paths=[\"JeanKaddour/minipile\"],\n",
    "        # text_datasets=['wikitext-2-v1'],\n",
    "        # text_datasets_paths=['wikitext'],\n",
    "        use_wandb=True,\n",
    "        device='cuda:1',\n",
    "        eval_mode='stochastic',\n",
    "        eval_text_num_examples=16,\n",
    "        # heads=8,\n",
    "        # mixed_precision='fp16',\n",
    "        cpu=False,\n",
    "        save_dir='models_minipile',\n",
    "        save_model=True\n",
    "        # disable_cosine_decay=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6ae4a9-296a-47bf-8eb8-0aa83ba8fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "gato_checkpoint = torch.load(\"/home/bhavul/bhavul/NEKO/gato/policy/models_minipile/neko-gato_24-05-12_19-35-17/checkpoint_0.pt\", map_location=args.device)\n",
    "\n",
    "tasks = []\n",
    "\n",
    "\n",
    "model = GatoPolicy(\n",
    "    device=args.device,\n",
    "    embed_dim=args.embed_dim,\n",
    "    layers=args.layers,\n",
    "    heads=args.heads,\n",
    "    dropout=args.dropout,\n",
    "    mu=args.mu,\n",
    "    M=args.M,\n",
    "    patch_size=args.patch_size,\n",
    "    resid_mid_channels=args.resid_mid_channels,\n",
    "    continuous_tokens=args.continuous_tokens,\n",
    "    discrete_tokens=args.discrete_tokens,\n",
    "    context_len=args.sequence_length,\n",
    "    use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "    use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "    activation_fn=args.activation_fn,\n",
    "    pretrained_lm=args.pretrained_lm,\n",
    "    flash=args.flash,\n",
    "    tokenizer_model_name=args.tokenizer_model_name,\n",
    "    pad_seq=args.pad_seq,\n",
    ")\n",
    "\n",
    "model.load_state_dict(gato_checkpoint)\n",
    "\n",
    "accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n",
    "model = accelerator.prepare(model)\n",
    "args.device = accelerator.device                \n",
    "\n",
    "\n",
    "model = model.to(args.device)\n",
    "model.device = args.device\n",
    "\n",
    "logs = {}\n",
    "model.eval()\n",
    "eval_start = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81b66476-9b25-4233-8490-fa1ee44bb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_datasets=['wikitext-2-v1']\n",
    "# text_datasets_paths=['wikitext']\n",
    "text_datasets='text',\n",
    "text_datasets_paths=\"JeanKaddour/minipile\",\n",
    "args.eval_text_num_examples = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a84c4567-931d-4699-bbfe-7218c6d2a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: Just remember, your graphics card only has a limited number of \"locations\" to keep track of your variables. A float takes up the \n",
      "[Output] : same number of locations (1) as a vector4. So if you end up making a complicated shader that requires lots of data to be passed around, you may want to bundle your float's into vector4's since then you will be able to pass 4 parameters around but still only take up one location. Just a tip, if you call glGetUniformLocation every time you update your variable you run the risk of stalling your pipeline which heavily degrades performance. I'd expect that drivers would store uniform locations locally rather than needing to round-trip to the GPU in order to get them. OpenGL uniform locations bear absolutely no relation to what's actually happening in hardware, after all, and drivers already need to do a job of translating them to actual GPU registers, so drivers already have a \"uniform table\" of some kind implemented. The performance hit would be needing to do a string-based lookup on \n",
      "[Prediction]:  the same size of the same size.\n",
      "\n",
      "The output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output.\n",
      "\n",
      "The output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output of the output.\n",
      "\n",
      "The output of the output of the output of the output of the output of the output of the output of the output of the output of the\n",
      "\n",
      "\n",
      "[Input]: Characteristics of seriously mentally ill adults in a public mental health system. To aid in planning mental health services, the state of Michigan collected sociodemographic and clinical information on a sample of 2,447 mental health system clientele in both hospital and community settings. Hospitalized patients were found to have poorer functioning and higher levels of disability than clients treated in community settings. Younger patients were more likely to be male, to exhibit more \n",
      "[Output] : self-destructive behaviors, and to be more aggressive and demoralized. Older patients were more likely to be women, to have a high degree of physical health care needs, and to possess poor skills in self-care and community living. The heterogeneous nature of the population and the presence of complicating problems not traditionally served by mental health systems, such as medical illness and substance abuse, point to a need for interagency planning to address the needs of seriously mentally ill adults. \n",
      "[Prediction]:  The prevalence of mental health and mental health care was also associated with the prevalence of mental health care. The prevalence of mental health care was significantly higher in the population. The prevalence of mental health care was higher in the population. The prevalence of mental health care was higher in the population. The prevalence of mental health care was higher in the population. The prevalence of mental health care was higher in the\n",
      "\n",
      "\n",
      "[Input]: Q: Unable to Install Microsoft Azure Backup Server I've been having a problem for the past two days trying install Microsoft Azure Backup Server... I'm very close. The installation always fails to install the SQL server as shown in the image. The error logs state: On a domain controller, you cannot use the built-in Windows service accounts (Local Service \n",
      "[Output] : or Network Service) as service accounts for Reporting Services A: Installation of Azure Backup Server on a domain controller is not supported - see this article. I imagine this is primarily due to the reason you encountered, that installing SQL on a domain controller is also not supported. You will need to use another machine. \n",
      "[Prediction]:                                                        \n",
      "\n",
      "\n",
      "[Input]: Prison smokes ban ruled unlawful Corrections Minister says it has been a great success and the Government will change the law if it has to. Taylor, who is in Auckland Prison, took his case to the High Court claiming the prison manager had no power to impose a total smoking ban. Photo / Paul Escourt A judge has ruled a prison smoking ban is unlawful - a victory for career criminal Arthur Taylor, who challenged it in court. But Corrections says inmates will still not be allowed to smoke and the Government says it will change the law if it has to. Taylor, who is in Auckland Prison, took his case to the High Court at Auckland claiming the prison manager had no power under the Corrections Act to impose a total smoking ban, \n",
      "[Output] : and, even if he did have the authority, had not used his discretion and implemented the smoke-free ban under direction from the chief executive. Late last week Justice Murray Gilbert ruled that the ban, which has been in place for 17 months, was \"unlawful, invalid and of no effect\". Corre \n",
      "[Prediction]:  said the court has not been in the case of the case.\n",
      "\n",
      "\"The court has not been in the case of the case, but it is not clear that the government has not been in the case of the case, but it is not\n",
      "\n",
      "\n",
      "[Input]: In CXTPTabClientWnd::OnMDIDestroy, the method always calls either ActivateNextItem or ActivateNextTopmostItem. So what happens if I have, for example, three documents opened and I hit the \"x\" on the tab of a window that is not the active MDI window? CodeJock deactivates the active window and activates another window depending on the tab order. This seems wrong since clicking the \"x\" on an inactive window does not MDI activate that window (which happens to be one of the big benefits in our system to using MDI tabs). To see this open three documents. We are using \"new tabs on the left flag\". Activate the doc that goes with the right-most tab. Now hit the \"x\" on the middle tab. The document associated with the left-most tab activates and the active document deactivates. Since the document being closed was not active, why pick another document to activate? I \"fixed\" this by getting the active window and seeing if it is the one being closed. Only if \n",
      "[Output] : it is do I activate another document. \n",
      "[Prediction]: \n",
      "\n",
      "I have a \"x\n",
      "\n",
      "\n",
      "[Input]: 965 = 148*g + 740*g for g. 45 Solve -431*v - 4429 = 4622 for v. -21 Solve 0 = -2180*g + 101*g - 1938*g + 136578 for g. 34 Solve 99*t - 662*t + 23688 = 95*t for t. 36 Solve 502*r + 1055*r - 14998 = 37940 for r. 34 Solve 250*r + 2903*r + 138732 = 0 for r. -44 Solve -33256*h = -33389*h - 5054 for h. -38 Solve -4120*o - 979 = -4209*o for o. 11 Solve 0 = 3879*d - 4083*d + 1224 for d. 6 Solve 7134 = 159*r - 819 - 73 - 1673 for r. 61 Solve 2128 = 7083*q - 14131*q + 6915*q for q. -16 Solve 42*v - 669 - 859 = 132*v + 2 for v. -17 Solve 786*u - 17020 = 12062 for u. 37 Solve 5649*z + 129210 + 117181 = -24761 for z. -48 Solve -173 - 1617 = 36*f - 98 for f. -47 Solve -37*t + 537*t - 16805 = -1973 + 6668 for t. 43 Solve 218*s - 20*s - 395 - 3763 = 0 for s. 21 Solve -114*g = 465*g + 838*g for g. 0 Solve 1319*o + 1472*o + 8396 + 2875 = 3080*o for o. 39 Solve -226*j - 104*j + 4128 \n",
      "[Output] : = -72*j for j. 16 Solve 7 = -49*s - 103 - 380 for s. -10 Solve 77*q + 536609 = 539843 for q. 42 Solve 222*d + 992 + 3126 - 18326 = 0 for d. 64 Solv \n",
      "[Prediction]: \n",
      "Solve -2*d - 4*d - 4*d - 4*d - 4*d - 4*d - 4*d = -5*d for d.\n",
      "\n",
      "\n",
      "[Input]: Tuesday, February 9, 2010 In one of the myriad of commentaries Red Pine scoured through to put together his book on the Diamond Sutra, there was this from late \n",
      "[Output] : 4th century Chinese monk Seng-chao: \"When your practice and understanding meet, you will see the Buddha.\" An interesting line, don't you think? One that immediately sparks for me thoughts of time, and steps made in one's life. Ultimately, we are told, there isn't any time in the way we humans understand time. What we regularly view as time is just a construction. So, when is this \"when\" being spoken about? Zen tends to toss out developmental approaches to practice, pointing it's students back to the now, to your breath, to your life in this moment. And yet, even to speak of a \"moment,\" or of \"now,\" is a construction. So, what now? There's not even a single square inch of earth upon which to stand, said Dogen, in a commentary on the Ten Grave precepts. This kind of talk tends to upset most of us if we actually take time (laugh, laugh) to let it sin \n",
      "[Prediction]: .\n",
      "\n",
      "The first thing is that we can do is to do is to say that we can do it.\n",
      "\n",
      "The first thing we can do is to do is to do is to say that we can do it.\n",
      "\n",
      "The first thing we can do is to do is to do is to do is to do it.\n",
      "\n",
      "The second thing we can do is to do is to do is to do is to do it.\n",
      "\n",
      "The second thing we can do is to do is to do is to do is to do it.\n",
      "\n",
      "The second thing we can do is to do is to do is to do is to do it.\n",
      "\n",
      "The second thing we can do is to do is to do is to do is to do it.\n",
      "\n",
      "\n",
      "[Input]: In order to fight obesity, the CDC argues for \n",
      "[Output] : resisting portion size inflation. Research has shown that the bigger your plate, the likelier it is you'll overeat. The same logic may apply to fast food, where according to a new infographic by the Centers for Disease Control, portion sizes for popular items have increased dramatically since the 1950s. Since the dawn of the Cold War, the volume of an order of french fries has grown 180 percent. The weight of the average burger has more than tripled. And get this -- sodas are six times larger than they were back in the days of \"I Love Lucy.\" CDC We want to hear what you think about this article. Submit a letter to the editor or write to letters@theatlantic.com. \n",
      "[Prediction]: \n",
      "\n",
      "The letter is \"I'm not sure I'm not sure if you're not going to be able to do this, but I'm not sure if you're not going to be able to do this, but I'm not sure if you're not going to be able to do this, I'm not sure if you're not going to be able to do this, but I'm not sure if you're not going to be able to do this, I'm not sure if you're not going to be able to do this, I'm not sure if you're not going to\n",
      "\n",
      "\n",
      "[Input]: set(COMPONENT_SRCS \"app_main.c\") set(COMPONENT_ADD_INCLUDEDIRS \n",
      "[Output] : \".\") register_component() \n",
      "[Prediction]: \n",
      "set\n",
      "\n",
      "\n",
      "[Input]: Q: SQL Query in suspended state causing high CPU usage Environment detail: SQL Server 2008 R2. I have applications running on JBoss that access the DB using JDBC driver. Issue: My online DB server was experiencing high CPU usage. Using the following query I was able to determine there are 2 queries (Select command) in suspended status (wait \n",
      "[Output] : type - CXPACKET) for nearly 10 minutes and going on. I believe this is the cause of high CPU usage. USE MASTER GO SELECT scheduler_id , runnable_tasks_count , pending_disk_io_count FROM sys.dm_os_schedulers WHERE scheduler_id < 255 SELECT qs.percent_complete , qs.session_id , scheduler_id , blocking_session_id , qs.status , command , wait_time , wait_type , last_wait_type , wait_resource , ST.text , host_name , program_name /* ,SUBSTRING(ST.text, (QS.statement_start_offset/2) + 1, ( (CASE statement_end_offset WHEN -1 THEN DATA \n",
      "[Prediction]: _end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_end_\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Evaluation results:\n",
      "evaluation/text/loss: 3.3587772846221924\n",
      "evaluation/text/perplexity: 28.754011154174805\n",
      "time/evaluation: 2088.8546850681305\n"
     ]
    }
   ],
   "source": [
    "tasks = []\n",
    "if len(text_datasets) > 0:\n",
    "    # add text datasets\n",
    "    tasks.append(TextTask(text_datasets, text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name)) \n",
    "\n",
    "# loop over eval for each task\n",
    "with torch.no_grad():\n",
    "    for task in tasks:\n",
    "        if isinstance(task, TextTask):\n",
    "            eval_logs = task.evaluate(model, num_examples_to_test=args.eval_text_num_examples, deterministic='stochastic')\n",
    "            for k, v in eval_logs.items():\n",
    "                logs[f'evaluation/text/{k}'] = v\n",
    "            pass\n",
    "\n",
    "\n",
    "            dataset_split = task.text_dataset['test']\n",
    "\n",
    "            sampled_indices = torch.randperm(len(dataset_split))[:10]\n",
    "            samples = dataset_split.select(sampled_indices)\n",
    "            \n",
    "            for sample in samples:\n",
    "                # GPT-2 hardcore limit of context length!\n",
    "                # If you don't set it, and you get an example > 1024 in length\n",
    "                # You face that weird error : tensor a (1024) must match dimension tensor b (1023) at singleton dimension 3 (something like this) \n",
    "                actual_text = sample['text'][:1024]\n",
    "                # roughly speaking...splitting by spaces\n",
    "                words_list = actual_text.split()\n",
    "                if len(words_list) > 1:\n",
    "                    split_index = random.randint(1, len(words_list)-1)\n",
    "                    input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])\n",
    "                    pred_tokens = model.predict_text(input_text=actual_text, max_length=len(words_list[split_index:]), deterministic='stochastic')\n",
    "                    decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                    print(f'[Input]: {input_text} \\n[Output] : {target_text} \\n[Prediction]: {decoded_target}\\n\\n')\n",
    "\n",
    "logs['time/evaluation'] = time.time() - eval_start\n",
    "\n",
    "print('=' * 80)\n",
    "print(f'Evaluation results:')\n",
    "for k, v in logs.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b8eaa30-e2b1-476b-9d76-9b661aa1a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_predict_text(input_text, max_length=20, deterministic=True, temperature=1.0, top_p=1.0, context_length=1024):\n",
    "    tokenized_outputs = model.text_tokenizer(input_text, truncation=True, padding=\"longest\", max_length=context_length, return_tensors='pt')\n",
    "    input_tokens = tokenized_outputs['input_ids']\n",
    "    predicted_tokens = input_tokens.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        token_embeddings = model.embed_token(predicted_tokens.to(args.device))\n",
    "        token_masks = torch.ones((predicted_tokens.shape[0], 1), device=args.device)\n",
    "\n",
    "        logits, _ = model.forward(token_embeddings=token_embeddings, tokens=predicted_tokens, token_masks=token_masks, token_target_masks=None)\n",
    "        logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "\n",
    "        if deterministic:\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            # Apply nucleus (top-p) filtering\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens.to(args.device), next_token.to(args.device)], dim=1)\n",
    "\n",
    "    return predicted_tokens[:, input_tokens.size(1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6c94229-558d-4f12-8868-f0a8e3e4fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_text_on_random_examples_with_nucleus(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0, split='test'):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        \n",
    "        for sample in samples:\n",
    "            actual_text = sample['text'][:1024]\n",
    "            # roughly speaking...splitting by spaces\n",
    "            words_list = actual_text.split()\n",
    "            if len(words_list) > 1:\n",
    "                split_index = random.randint(1, len(words_list)-1)\n",
    "                input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                pred_tokens = test_new_predict_text(input_text=input_text, max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                print(f'[Input]: \\n{input_text} \\n[Output] : \\n{target_text} \\n[Prediction]: \\n{decoded_target}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e07c4699-83c8-4b8e-9e40-c28759cc3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_good_ones(task, num_of_examples_to_test=10, deterministic=False, temperature=1.0, top_p=1.0):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_split = task.text_dataset['test']\n",
    "        \n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:num_of_examples_to_test]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "\n",
    "        for temperature, top_p in [(0.5,0.5),(0.5,0.7),(0.5,0.9),(0.75,0.75),(0.75,0.9),(0.8,0.7),(0.8,0.9),(0.9,0.5),(0.9,0.75),(0.9,0.9),(1.0,0.5),(1.0,0.75),(1.0,0.9),(1.1,0.9)]:\n",
    "            print('--'*30)\n",
    "            print(f'Temperature :  {temperature} || Top_p : {top_p}')\n",
    "            for sample in samples:\n",
    "                actual_text = sample['text']\n",
    "                # roughly speaking...splitting by spaces\n",
    "                words_list = actual_text.split()\n",
    "                if len(words_list) > 1:\n",
    "                    split_index = random.randint(1, len(words_list)-1)\n",
    "                    input_text, target_text = ' '.join(words_list[:split_index]), ' '.join(words_list[split_index:])  \n",
    "                    pred_tokens = test_new_predict_text(input_text='Hello how are', max_length=len(words_list[split_index:]), deterministic=deterministic, temperature=temperature, top_p=top_p)\n",
    "                    decoded_target = task.text_tokenizer.decode(pred_tokens.squeeze(), skip_special_tokens=True)\n",
    "                    print(f'[Input]: {input_text} \\n[Output]: {target_text} \\n[Prediction]: {decoded_target}\\n\\n\\n')\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a8a7c08-acb7-434a-a63f-572f5b9d8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: \n",
      "Hi, On 2014/02/26 12:28 , Guillaume Sachot wrote: > I also thought of that, but I cannot adapt a datacenter network only for one probe :) An easy way out for both sides would be for you to host two probes, one \n",
      "[Output] : \n",
      "of each VLAN. At the moment probes are not set up for multiple interfaces, and we have no clear idea what it would require to make that work. (And even if we did, it would probably take a while before it is released) Philip The RIPE NCC uses cookies. Some of these cookies may have been set already. More information about our cookies can be found in our privacypolicy. You can accept our cookies either by clicking here or by continuing to use the site. \n",
      "[Prediction]: \n",
      " that would be a good way to get the most efficient network to do it. It would be easy to use a lot of things like this, but I have to do it.\n",
      "\n",
      "I've been working on the internet, but I'm not sure what I want to do. I'm not sure how to do this in the future, but I'm not sure if I'm doing this, but I can't get\n",
      "\n",
      "\n",
      "[Input]: \n",
      "## ## Copyright (c) 2010 The WebM project authors. All Rights Reserved. ## ## Use of this source code is governed by a \n",
      "[Output] : \n",
      "BSD-style license ## that can be found in the LICENSE file in the root of the source ## tree. An additional intellectual property rights grant can be found ## in the file PATENTS. All contributing project authors may ## be found in the AUTHORS file in the root of the source tree. ## VP9_COMMON_SRCS-yes += vp9_common.mk VP9_COMMON_SRCS-yes += vp9_iface_common.h VP9_COMMON_SRCS-yes += vp9_iface_common.c VP9_COMMON_SRCS-yes += common/vp9_ppflags.h VP9_COMMON_SRCS-yes += common/vp9_alloccommon.c VP9_COMMON_SRCS-yes += common/vp9_blockd.c # VP9_COMMON_SRCS-yes += common/vp9_debugmodes.c VP9_COMMON_SRCS-yes += common/vp9_entropy.c VP9_COMMON_SRCS-yes += common/vp9_entropymode.c VP9_COMMON_SRCS-yes += common/vp9_entropymv.c VP9_COMMON_SRCS-yes += common/vp9_frame_buffers.c VP9_COMMON_SRCS-yes += common/vp9_frame_buffers.h VP9_COMMON_SRCS-yes += common/vp9_idct.c VP9_COMMON_SRCS-yes += common \n",
      "[Prediction]: \n",
      " BSD-style license that can be\n",
      "// found in the LICENSE file.\n",
      "\n",
      "#ifndef __future__\n",
      "#define __future__\n",
      "#define __future__\n",
      "\n",
      "#include \"../../../common/common/common/common/common/common/common/common/utils.h\"\n",
      "#include \"../../../../../../../../../../../../../../../../../../../../../../../../../../\n",
      "\n",
      "\n",
      "[Input]: \n",
      "Q: How to detect a page redirect? I have a page that should be displayed slightly differently if users are coming from a particular link. I.e. The normal page is example.com/foo If someone clicks on a link to example.com/foo-bar I want it redirected to example.com/foo, which can be done with the following line in a .htaccess file: Redirect permanent /foo-bar /foo I was planning to then using the following javascript to determine which page a user has come from: <script type=\"text/javascript\"> document.write(document.referrer); </script> Which does print out the referrer URL if the user has come \n",
      "[Output] : \n",
      "from a normal link on the site to /foo. But if they have clicked on a link to /foo-bar it prints out the URL of the page with the link, rather than the /foo-bar address that I need. Is there a way to redirect a URL to another URL, and detect this on the page redirected to? (Note that that the difference of content on the page isn't critical, so if the 1% of users with javascript turned off see the wrong thing w \n",
      "[Prediction]: \n",
      " to the page?\n",
      "\n",
      "A:\n",
      "\n",
      "You can create a page like this:\n",
      "$('#page').click(function(function(){\n",
      "                                                     \n",
      "\n",
      "\n",
      "[Input]: \n",
      "function testAxis() % Copyright 2010, Joseph O. Deasy, on behalf of the CERR development team. % % This file is part of The Computational Environment for Radiotherapy Research (CERR). % % CERR development has been led by: Aditya Apte, Divya Khullar, James Alaly, and Joseph O. Deasy. % % CERR has been financially supported by the US National Institutes of Health \n",
      "[Output] : \n",
      "under multiple grants. % % CERR is distributed under the terms of the Lesser GNU Public License. % % This version of CERR is free software: you can redistribute it and/or modify % it under the terms of the GNU General Public License as published by % the Free Software Foundation, either version 3 of the License, or % (at your option) any later version. % % CERR is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; % without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. % See the GNU General Public License for more details. % % You should have receive \n",
      "[Prediction]: \n",
      ", and the United States. %R has been widely used to define the CERR of the CERR in the United States.\n",
      "\n",
      "The CERR is an independent of the CERR, which is a member of the CERR project. The CERR is a member of the CERR project. The CERR is a member of the CERR project. The CERR has a CERR project which is responsible for the CERR, which is the CERR projective project. The\n",
      "\n",
      "\n",
      "[Input]: \n",
      "Q: Doing conditional operations using meshgrids in numpy and merging them? I have two variables, x,y. I want to calculate a group of secondary variables, based on conditions applied over all possible combinations of (x,y) pairs. My final goal is to generate surface plots with these variables over the x,y grid. For example, considering the conditions below: A if $(x > y*\\sqrt( (4*y) + 1)) AND (y < x*\\sqrt( (4*x)+1) $ B if $(x < y*\\sqrt( (4*y) + 1)) AND (y > x*\\sqrt( (4*x)+1) $ C if $(x < y*\\sqrt( (4*y) + 1)) AND (y < x*\\sqrt( (4*x)+1) $ D, if $x = y$ With (x,y) in [1,5], where the set of results {A,B,C,D} would represent one of my intended secondary variables that I \n",
      "[Output] : \n",
      "intend to plot. My approach was to create a meshgrid of x and y, and then run the separate conditions. I ended with four separate grids. Specific question: how would I go around merging these four meshgrids into one, so I could do the surface plot? General question: Does this approach makes sense? Is there a more streamlined or efficient appr \n",
      "[Prediction]: \n",
      " would be able to show that the number of variables can be applied to the number of points in the range. \n",
      "I would be able to use the following:\n",
      "$$\\sqrt{x}(x) = \\sqrt{x,A,A,B,B,B,\n",
      "\n",
      "\n",
      "[Input]: \n",
      "Q: ANTLR4 Parser, Visitor not created I'm new to ANTLR and trying to write grammar in ANTLR4 without any prior brush with the previous version. I'm following the book 'The Definitive ANTLR 4 Reference'. I use Eclipse and installed ANTLR4 IDE as given in here. I wrote the following grammar in Expr.g4: grammar Expr; import Common; options{ language = Java; } prog: stat+; stat: expr NEWLINE | ID '=' expr NEWLINE | NEWLINE; expr: expr ('/'|'*') expr | expr ('+'|'-') expr | INT | ID | '('expr')'; The Common.g4 contains the following: lexer grammar Common; ID: [A-Za-z]+; INT: [0-9]+; NEWLINE: '\\r'?'\\n'; WS: [\\t]+ -> skip; The lexer.java was created but not parser.java and visitor.java and other base file. Please help me fix the problem. Thanks in advance. A: In fact I had the same problem once, I used to integrate two G4 files within the \n",
      "[Output] : \n",
      "same project the first one generated Visitor but the second didn't. Then I realized that each G4 file has its own configuration for code generati \n",
      "[Prediction]: \n",
      " following code:\n",
      "\n",
      "A:\n",
      "\n",
      "I'm not sure what you want to do to do with this.  It\n",
      "\n",
      "\n",
      "[Input]: \n",
      "BUSAN - A blank newspaper in Lebanon and Burger King’s trolling campaign that took advantage of McDonald’s drive-throughs were announced on Sunday as the Grand Prix of the Year winners of the 2019 AD Stars, Korea’s only advertising festival. The two were selected among 20,645 entrees from more than 60 countries, for the Public Service Advertising and Product & Service divisions, respectively. “The Blank Edition” was released last October when Lebanon had been without a government for six months due to political infighting resulting in a deadlock. To represent the lack of responsibility over the situation, Lebanon’s second largest newspaper An Nahar printed an empty newspaper, with zero photos, articles or advertisements. Lebanese people were also called upon to write messages to politicians on the blank edition and upload them on their social media. The campaign received $5 million worth of media saturation effect and was covered in more than 100 international \n",
      "[Output] : \n",
      "publications. Months later in January 2019, L \n",
      "[Prediction]: \n",
      " media reports. The report also confirmed\n",
      "\n",
      "\n",
      "[Input]: \n",
      "Q: how to separate lists into different divs dynamically Good day, I have a list of categories in my page, but anytime it could be added or removed. Can you please help me formulate a loop to divide my lists into a maximum of 5 list per div. ex. foreach($categories as $cat) <label><input type =\"checkbox\"><?php echo $cat->name ?></label></br> endforeach the above code will generate a list of checkboxes in 1 column so I tried this loop <?php $count = 0; ?> \n",
      "[Output] : \n",
      "@foreach($categories as $cat) @if($count == 0) <div class=\"col-md-4\"> @elseif($count == 5) <?php $count = 0; ?> </div> @endif <label><input type=\"checkbox\" value=\"{{$cat->xid}}\">{{$cat->category}}</label><br> <?php $count++; ?> @endforeach and it generates like this but it only wraps the first 5 elements on the list and the next 5 is not working. Can you help me please :) suggestions are really appreciated A: The below is your code to which I modified would generate the required layout. you will need to apply \n",
      "[Prediction]: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "You need to add a column to the list of values of the list, so you can do it to do it. \n",
      "If you want to add a column to the list of the list, you can use the table like the list, and then use the list of rows in the list, you can use a list of values in the list. \n",
      "\n",
      "\n",
      "[Input]: \n",
      "Chicken alla Vodka Lasagne Thanksgiving is one of my favorite holidays, I love making food for my family and enjoying a big meal with them. I love sitting around the table and talking about the things we are grateful for. I love the tradition that surrounds Thanksgiving. Making the same meals year after year. Letting the kids help cook dinner. This year Mason wanted to know why we weren’t having Lasagna! I quickly explained–Lasagna isn’t for Thanksgiving! It’s for Christmas. Growing up I was always really interested in my heritage. My mom’s family is from \n",
      "[Output] : \n",
      "Puerto Rico. My Dad’s family is a little bit of everything. My dad’s grandmothers came from Italy and Canada. Somewhere down the line we also had family coming from Ireland and Scotland. Along the way we’ve adopted the our Italian roots more than anything. Probably because it was the most recent. So Lasagne is for Christmas. But I have a little boy who’s been begging for it for so Lasagne it is, BEFORE Christmas. But with a twist. Traditional Lasagne is \n",
      "[Prediction]: \n",
      " the same family. I’d love the kids in the kitchen. I’d love to be able to eat with a little bit of food and eat.\n",
      "\n",
      "I love to eat a small bowl and a little bit of a bit of a couple of hours. I love to eat, but I’ve never been so much about to have a few days of time. I love\n",
      "\n",
      "\n",
      "[Input]: \n",
      "The State of Bitcoin in Kenya Following bitcoin’s birth in 2009 as a decentralized peer-to-peer payment network with fast transaction speeds and low transaction fees, one of its most predicted future application uses was for international remittance purposes. Many experts envisioned that bitcoin would be a driving force in reducing the costs of international money transfers to developing regions such as Sub-Saharan Africa and, thereby, create more financial inclusion for the unbanked. While anticipated bitcoin remittances into Africa have still not taken off at a large scale, Africa has birthed several local Bitcoin economies spread across the continent. In this article, you will be introduced to the state of bitcoin in Kenya to give you an insight into East \n",
      "[Output] : \n",
      "Africa’s leading bitcoin hub. Kenya’s Bitcoin Ecosystem Kenya is among the four main Bitcoin economies in Africa next to South Africa, Nigeria, and Ghana. While bitcoin awareness and adoption are growing in many parts of Africa, only the countries me \n",
      "[Prediction]: \n",
      " Asia’s largest transaction in the country.\n",
      "\n",
      "The original cryptocurrency market in which the market is the first time of the currency is largely the world’s largest economy. The market has\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_text_on_random_examples_with_nucleus(task, num_of_examples_to_test=10, deterministic=False, temperature=0.5, top_p=0.95, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c954b-3e0c-475d-8902-3a26ef2ec961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11809bb8-f719-4253-8f8f-e3442beebc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fd79a1-9282-41d6-9103-6812d142f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ca15d5-f58a-469a-9b8d-f20f2b468cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhavul/bhavul/NEKO/dev_notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742531b1-1095-4a65-a735-5961985f4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"${PYTHONPATH}:/home/bhavul/bhavul/NEKO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23eae53d-1031-4fda-a917-4ec07aa14750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bhavul/bhavul/NEKO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9505f8-124d-430d-be93-332f02d7dac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8bfaad3-de33-4422-a36a-64bdb275b19a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60482eb1-9d86-48ca-94aa-a6da92861880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "from gato.utils.utils import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465dc659-6be2-43ac-bf1a-22ed409fde9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavul/.conda/envs/neko/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate import DataLoaderConfiguration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a2343-12b5-420e-b353-a2915f7b338d",
   "metadata": {},
   "source": [
    "# Provide Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696f02c-85ed-46c7-98d8-d6d3479f9ddb",
   "metadata": {},
   "source": [
    "# Text Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd5c9d65-ec15-4135-bd21-9fca454ac6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "# supports dataset in huggingface datasets library for now\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from gato.tasks.task import Task\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import TYPE_CHECKING, List,Dict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "befecaf9-959c-4f1c-91d0-3fc31ba8e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTask(Task): \n",
    "    def __init__(self, dataset_names:List[str], dataset_paths:List[str], context_length:int, tokenizer_model:str):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        text_datasets_list = []\n",
    "        assert len(dataset_names) == len(dataset_paths), \"The dataset names and paths parameters should have corresponding values and hence equal lengths\"\n",
    "        for i, text_dataset in enumerate(dataset_names):\n",
    "            text_datasets_list.append(load_dataset(path=dataset_paths[i], name=text_dataset))\n",
    "        if len(text_datasets_list) == 1:\n",
    "            self.text_dataset = text_datasets_list[0]\n",
    "        else:            \n",
    "            # https://huggingface.co/docs/datasets/v2.14.4/en/process#concatenate\n",
    "            # must have the same feature columns\n",
    "            self.text_dataset = concatenate_datasets(text_datasets_list)\n",
    "\n",
    "    def sample_batch(self, batch_size, is_test=False) -> List[Dict]:\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        sampled_indices = torch.randperm(len(dataset_split))[:batch_size]\n",
    "        samples = dataset_split.select(sampled_indices)\n",
    "        tokenized_outputs = self.text_tokenizer(samples['text'], truncation=True, max_length=self.context_length, return_tensors='pt')\n",
    "    \n",
    "        batch_dicts = []\n",
    "        for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "            if input_ids.numel() > 0:  # Check if non-empty\n",
    "                # Split into input and target tokens\n",
    "                input_tokens = input_ids[:-1]\n",
    "                target_tokens = input_ids[1:]\n",
    "                batch_dicts.append({\n",
    "                    'text': input_tokens,\n",
    "                    'target': target_tokens,\n",
    "                    'images': None,\n",
    "                    'continuous_obs': None,\n",
    "                    'discrete_obs': None,\n",
    "                    'continuous_actions': None,\n",
    "                    'discrete_actions': None\n",
    "                })\n",
    "    \n",
    "        return batch_dicts\n",
    "        \n",
    "    def sample_batch_old(self, batch_size, is_test=False)->List[Dict]:\n",
    "        \"\"\"Gets called during training and test both, fetch as many examples as batch_size param.\"\"\"\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        random_indices = np.random.randint(0, len(self.text_dataset[split]), size=batch_size)\n",
    "        tokenized_outputs = self.text_tokenizer(self.text_dataset[split][random_indices]['text'], truncation=True,\n",
    "            max_length=self.context_length,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_length=True)\n",
    "        \n",
    "        batch_dicts = []\n",
    "        count = 0 \n",
    "        # todo - ii. vectorize this? Also do we wanna impose any length constraint?\n",
    "        for length, input_ids in zip(tokenized_outputs[\"length\"], tokenized_outputs[\"input_ids\"]):\n",
    "            # we only pick non-empty string examples right now\n",
    "            if length > 0:\n",
    "                batch_example_dict = {\n",
    "                    'text': input_ids,  # A list of tokens\n",
    "                    'images': None,\n",
    "                    'continuous_obs': None,\n",
    "                    'discrete_obs': None,\n",
    "                    'continuous_actions': None,\n",
    "                    'discrete_actions': None\n",
    "                }\n",
    "                batch_dicts.append(batch_example_dict)\n",
    "                count += 1\n",
    "                if count == batch_size:\n",
    "                    break\n",
    "        \n",
    "        return batch_dicts\n",
    "\n",
    "    def evaluate(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        split = 'train' if not is_test else 'test'\n",
    "        dataset_split = self.text_dataset[split]\n",
    "        num_examples_to_test = min(num_examples_to_test, len(dataset_split))\n",
    "    \n",
    "        if num_examples_to_test == 0:\n",
    "            return {'loss': float('nan'), 'perplexity': float('nan')}\n",
    "    \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "    \n",
    "        for batch_dict in batch_dicts:\n",
    "            tokens = batch_dict['text']\n",
    "            ith_position = np.random.randint(1, len(tokens))\n",
    "            input_tokens = tokens[:ith_position]\n",
    "            target_tokens = tokens[ith_position:]\n",
    "    \n",
    "            pred_logits, pred_tokens = model.predict_text({'text': input_tokens}, max_length=len(target_tokens), deterministic=deterministic)\n",
    "    \n",
    "            target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long, device=model.device)\n",
    "            loss = torch.nn.functional.cross_entropy(pred_logits[:, -len(target_tokens):], target_tokens_tensor.unsqueeze(0))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += len(target_tokens)\n",
    "    \n",
    "            if log_examples_to_output:\n",
    "                print(f'Input: {self.text_tokenizer.decode(input_tokens)} | Target: {self.text_tokenizer.decode(target_tokens)} | Prediction: {self.text_tokenizer.decode(pred_tokens)}')\n",
    "    \n",
    "        avg_loss = total_loss / len(batch_dicts)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "        \n",
    "    def evaluate_old(self, model: GatoPolicy, num_examples_to_test=50, deterministic=False, log_examples_to_output=False, is_test=True):\n",
    "        tokenizer = model.module.text_tokenizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "    \n",
    "        split = 'train' if not is_test else 'test'\n",
    "        if num_examples_to_test > len(self.text_dataset[split]):\n",
    "            print(f'num_examples_to_test chosen is more than test examples, so setting it to whole test dataset.')\n",
    "            num_examples_to_test = len(self.text_dataset[split])\n",
    "    \n",
    "        if log_examples_to_output:\n",
    "            print(f'--- examples ---')\n",
    "        \n",
    "        batch_dicts = self.sample_batch(num_examples_to_test, is_test)\n",
    "        print(f'Num of examples to test : {num_examples_to_test} | Actual batch size of eval data : {len(batch_dicts)}')\n",
    "        \n",
    "        actual_examples_tested = 0\n",
    "        for idx in range(min(num_examples_to_test, len(batch_dicts))):\n",
    "            batch_dict = batch_dicts[idx]\n",
    "            \n",
    "            # Split the tokens into input and target tokens\n",
    "            tokens = batch_dict['text']\n",
    "            ith_position = np.random.randint(1, len(tokens))\n",
    "            input_tokens = tokens[:ith_position]\n",
    "            target_tokens = tokens[ith_position:]\n",
    "    \n",
    "            new_batch_dict = copy.deepcopy(batch_dict)\n",
    "            new_batch_dict['text'] = input_tokens\n",
    "    \n",
    "            # Generate prediction\n",
    "            pred_logits, pred_tokens = model.module.predict_text(new_batch_dict, max_length=len(target_tokens), deterministic=deterministic)\n",
    "            \n",
    "            if log_examples_to_output and idx%50==0:\n",
    "                print(f'Text Example : {tokenizer.decode(batch_dict[\"text\"])} \\n Input passed to model : {tokenizer.decode(new_batch_dict[\"text\"])} \\n Predicted output : {tokenizer.decode(pred_tokens)}')\n",
    "                print(\"----\")\n",
    "    \n",
    "            # Calculate loss\n",
    "            target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long, device=model.device)\n",
    "            loss = loss_fn(pred_logits[:, -len(target_tokens):, :], target_tokens_tensor.unsqueeze(0))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += len(target_tokens)\n",
    "            actual_examples_tested += 1\n",
    "            \n",
    "        if log_examples_to_output:\n",
    "            print(f'--- examples end ---')\n",
    "    \n",
    "        avg_loss = total_loss / actual_examples_tested\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "        metrics = {\n",
    "            'loss': avg_loss,\n",
    "            'perplexity': perplexity.item()\n",
    "        }\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7fc45-b598-4116-8a2f-b5c8ab7d87f7",
   "metadata": {},
   "source": [
    "## Gato Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "76a6c5d4-12b8-4740-935a-bb1e491933ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Union, TYPE_CHECKING\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# import gato\n",
    "from gato.transformers import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "16b6fbfe-09ef-410b-bca1-d857c4e940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatoPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Union[torch.device, str],\n",
    "        embed_dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "\n",
    "        activation_fn='gelu',\n",
    "\n",
    "        mu: int = 100,\n",
    "        M: int = 256,\n",
    "\n",
    "        patch_size: int = 16,\n",
    "        resid_mid_channels: int = 132,\n",
    "        num_groups: int = 32,\n",
    "        position_vocab_size: int = 128,\n",
    "        continuous_tokens: int = 1024,\n",
    "        discrete_tokens: int = 1024,\n",
    "\n",
    "        context_len=1024,\n",
    "\n",
    "        use_pos_encoding: bool = True,\n",
    "        use_patch_pos_encoding: bool = True,\n",
    "\n",
    "        pretrained_lm: Optional[str] = None, # Optional, name of pretrained language model to use\n",
    "        flash: bool = False, # TODO verify correctness\n",
    "        tokenizer_model_name: str = 'gpt2',\n",
    "        pad_seq: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # Text Tokenizer\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)        \n",
    "        # tokens\n",
    "        self.vocab_size = self.text_tokenizer.vocab_size \n",
    "        \n",
    "\n",
    "        if pretrained_lm is not None:\n",
    "            print(f'loading pretrained GPT2 weights')\n",
    "            config = transformers.GPT2Config.from_pretrained(pretrained_lm)\n",
    "            config.attn_pdrop = dropout # 0.1\n",
    "            config.resid_pdrop = dropout\n",
    "            self.transformer = GPT2Model.from_pretrained(\n",
    "                pretrained_lm,\n",
    "                config=config,\n",
    "            )\n",
    "            embed_dim = config.n_embd\n",
    "            assert self.transformer.wte.weight.shape[0] == self.text_tokens, \"pretrained token/expected mimsatch\" # potentially make text_tokens dynamic\n",
    "        else:\n",
    "            gate = False\n",
    "            if activation_fn == 'geglu':\n",
    "                gate = True\n",
    "                activation_fn = 'gelu'\n",
    "            config = transformers.GPT2Config(\n",
    "                vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "                n_embd=embed_dim,\n",
    "                n_head=heads,\n",
    "                n_layer=layers,\n",
    "                resid_pdrop=dropout,\n",
    "                attn_pdrop=dropout,\n",
    "                n_positions=context_len,\n",
    "                n_inner=embed_dim * 4,\n",
    "                activation_function=activation_fn,\n",
    "            )\n",
    "            config.n_ctx = context_len\n",
    "            config.gate = gate\n",
    "            config.flash = flash\n",
    "            self.transformer = self.transformer = GPT2Model(config)\n",
    "        \n",
    "        # embedding tokens\n",
    "        self.embed_token = nn.Embedding(self.vocab_size, embed_dim)\n",
    "        if pretrained_lm is not None:\n",
    "            self.embed_token.weight.data[:] = self.transformer.wte.weight.data\n",
    "        \n",
    "        \n",
    "        # head\n",
    "        self.predict_token = nn.Linear(embed_dim, self.vocab_size, bias=False)\n",
    "        self.separator_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_target_masks, token_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        assert token_embeddings is not None, \"token_embeddings is None\"\n",
    "        assert token_masks is not None, \"token_masks is None\"\n",
    "\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "        \n",
    "        if compute_loss and 'target' in kwargs:\n",
    "            target_tokens = kwargs['target']\n",
    "            loss_masks = (target_tokens != self.text_tokenizer.pad_token_id).float()\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.vocab_size), target_tokens.view(-1), reduction='none')\n",
    "            loss = (loss * loss_masks.view(-1)).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "        \n",
    "    \n",
    "    # predicts next token (for each input token)\n",
    "    def forward_old(self, inputs: Optional[list]=None, compute_loss=False, **kwargs):\n",
    "        # tokenize inputs\n",
    "        if inputs is not None:\n",
    "            token_embeddings, tokens, token_target_masks, token_masks = self.tokenize_input_dicts(inputs)\n",
    "        else:\n",
    "            token_embeddings = kwargs['token_embeddings']\n",
    "            tokens = kwargs['tokens']\n",
    "            token_target_masks = kwargs['token_target_masks']\n",
    "            token_masks = kwargs['token_masks']\n",
    "\n",
    "        # pass to transformer\n",
    "        #final_representations = self.transformer(x = token_embeddings, custom_mask = token_masks, batch_first=True)\n",
    "        final_representations = self.transformer(inputs_embeds=token_embeddings, attention_mask=token_masks)['last_hidden_state']\n",
    "        logits = self.predict_token(final_representations)\n",
    "\n",
    "        if compute_loss:\n",
    "            loss_logits = logits[:, :-1, :].contiguous().view(-1, self.vocab_size)\n",
    "            target_tokens = tokens[:, 1:].contiguous().view(-1)\n",
    "            loss_masks = token_target_masks[:, 1:].contiguous().view(-1)\n",
    "            loss = torch.nn.functional.cross_entropy(loss_logits, target_tokens, reduction='none')\n",
    "            loss = (loss * loss_masks).sum() / loss_masks.sum()\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def tokenize_input_dicts(self, inputs: list):\n",
    "        # if not inputs:\n",
    "        #     return None, None, None, None\n",
    "    \n",
    "        batch_len = len(inputs)\n",
    "        max_tokens = max(len(batch['text']) for batch in inputs)\n",
    "    \n",
    "        # Allocate tensors\n",
    "        token_embeddings = torch.zeros((batch_len, max_tokens, self.embed_token.embedding_dim), device=self.device)\n",
    "        tokens = torch.zeros((batch_len, max_tokens), dtype=torch.long, device=self.device)\n",
    "        token_target_masks = torch.zeros((batch_len, max_tokens), device=self.device)\n",
    "        token_masks = torch.zeros((batch_len, max_tokens), device=self.device)\n",
    "    \n",
    "        for i, batch in enumerate(inputs):\n",
    "            text_tokens = torch.tensor(batch['text'], dtype=torch.long, device=self.device)\n",
    "            n_timesteps = len(text_tokens)\n",
    "    \n",
    "            tokens[i, :n_timesteps] = text_tokens\n",
    "            token_embeddings[i, :n_timesteps] = self.embed_token(text_tokens)\n",
    "            token_target_masks[i, 1:n_timesteps] = 1  # Target mask should start from second token to avoid prediction of first token\n",
    "            token_masks[i, :n_timesteps] = 1\n",
    "\n",
    "        print(\"token_embeddings shape:\", token_embeddings.shape)  # Debug print\n",
    "        print(\"token_masks shape:\", token_masks.shape)  # Debug print\n",
    "    \n",
    "        return token_embeddings, tokens, token_target_masks, token_masks\n",
    "\n",
    "    def tokenize_input_dicts_old(self, inputs: list):\n",
    "        \"\"\"\"\n",
    "        inputs: list of dicts for a batch\n",
    "        [\n",
    "            {\n",
    "                # observations\n",
    "                text: T x L  or None\n",
    "                \n",
    "            },\n",
    "            ...\n",
    "            {\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        returns: the tokens_id, tokens_embedding for each batch respectively\n",
    "        \"\"\"\n",
    "        batch_len = len(inputs)\n",
    "\n",
    "        token_embeddings = []\n",
    "        tokens = []\n",
    "        token_target_masks = []\n",
    "        max_tokens = -1 # max number of timesteps across all batches\n",
    "\n",
    "        for batch in inputs:\n",
    "\n",
    "            text_tokens = torch.tensor(batch['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "\n",
    "            # Split the tokens into input and target tokens\n",
    "            input_tokens = text_tokens[:, :-1]\n",
    "            target_tokens = text_tokens[:, 1:]\n",
    "        \n",
    "            text_embeddings = self.embed_token(text_tokens)\n",
    "            text_targets_masks = torch.ones_like(target_tokens)\n",
    "            n_timesteps = text_tokens.shape[1]\n",
    "\n",
    "            batch_embeddings = text_embeddings\n",
    "            batch_tokens = input_tokens\n",
    "            batch_target_masks = text_targets_masks\n",
    "\n",
    "            token_embeddings.append(batch_embeddings)\n",
    "            tokens.append(batch_tokens)\n",
    "            token_target_masks.append(batch_target_masks)\n",
    "            max_tokens = max(max_tokens, batch_embeddings.shape[1])\n",
    "\n",
    "        token_masks = []\n",
    "        # (left pad) to max tokens\n",
    "        for i in range(batch_len):\n",
    "            pad_len = max_tokens - token_embeddings[i].shape[1]\n",
    "            token_masks.append(torch.cat([torch.ones_like(token_embeddings[i]), torch.zeros(1, pad_len, device=self.device)], dim=1))\n",
    "            token_embeddings[i] = torch.cat([token_embeddings[i], torch.zeros(1, pad_len, self.embed_token.embedding_dim, device=self.device)], dim=1)\n",
    "            tokens[i] = torch.cat([tokens[i], torch.zeros(1, pad_len, dtype=torch.long, device=self.device)], dim=1)\n",
    "            token_target_masks[i] = torch.cat([token_target_masks[i], torch.zeros(1, pad_len, device=self.device)], dim=1)\n",
    "\n",
    "        # Check if token_embeddings list is not empty before concatenating\n",
    "        if token_embeddings:\n",
    "            token_embeddings = torch.cat(token_embeddings, dim=0)\n",
    "            tokens = torch.cat(tokens, dim=0)\n",
    "            token_target_masks = torch.cat(token_target_masks, dim=0)\n",
    "            token_masks = torch.cat(token_masks, dim=0)\n",
    "        else:\n",
    "            token_embeddings = None\n",
    "            tokens = None\n",
    "            token_target_masks = None\n",
    "            token_masks = None\n",
    "        \n",
    "        # if self.pad_seq:\n",
    "        #     # get seq length\n",
    "        #     seq_len = token_embeddings.shape[1]\n",
    "        #     pad_len = self.context_len - seq_len\n",
    "        #     if pad_len > 0:\n",
    "        #         token_embeddings = torch.cat([token_embeddings, torch.zeros(batch_len, pad_len, self.embed_dim, device=self.device)], dim=1)\n",
    "        #         tokens = torch.cat([tokens, torch.zeros(batch_len, pad_len, dtype=torch.long, device=self.device)], dim=1)\n",
    "        #         token_target_masks = torch.cat([token_target_masks, torch.zeros(batch_len, pad_len, device=self.device)], dim=1)\n",
    "        #         token_masks = torch.cat([token_masks, torch.zeros(batch_len, pad_len, device=self.device)], dim=1)\n",
    "        return token_embeddings, tokens, token_target_masks, token_masks\n",
    "\n",
    "\n",
    "    def predict_text(self, batch_dict, max_length=20, deterministic=True, top_p=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        predicted_tokens = []\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[:, -1, :]  # focus on the last time-step logits\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                sorted_logits[sorted_indices_to_remove] = 0\n",
    "                sorted_probs = torch.nn.functional.softmax(sorted_logits, dim=-1)\n",
    "                token = torch.multinomial(sorted_probs, 1)[0]\n",
    "                token = sorted_indices[torch.where(sorted_probs == token)]\n",
    "    \n",
    "            predicted_tokens.append(token.item())\n",
    "    \n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            input_tokens = torch.cat([input_tokens, next_token.unsqueeze(0)], dim=1)  # Append the predicted token\n",
    "\n",
    "            # Ensure that sequence does not exceed context_len\n",
    "            # if input_tokens.size(1) > self.context_len:\n",
    "            #     input_tokens = input_tokens[:, -self.context_len:]\n",
    "            #     token_embeddings = token_embeddings[:, -self.context_len:, :]\n",
    "            #     token_masks = token_masks[:, -self.context_len:]\n",
    "    \n",
    "        return logits, predicted_tokens\n",
    "\n",
    "    def predict_text_old(self, batch_dict, max_length:int=20, deterministic:bool=True, top_p:float=0.9):\n",
    "        input_tokens = torch.tensor(batch_dict['text'], dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        token_embeddings = self.embed_token(input_tokens)\n",
    "        token_masks = torch.ones_like(input_tokens)\n",
    "        predicted_tokens = []\n",
    "        \n",
    "        # predict tokens, sampling or deterministically picking best token\n",
    "        for i in range(max_length):\n",
    "            logits, _ = self.forward(token_embeddings=token_embeddings, tokens=input_tokens, token_target_masks=None, token_masks=token_masks)\n",
    "            logits = logits[0, -1, :]\n",
    "    \n",
    "            if deterministic:\n",
    "                token = torch.argmax(logits, dim=-1)\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "                sorted_probs /= sorted_probs.sum()\n",
    "                token = torch.multinomial(sorted_probs, num_samples=1)[0]\n",
    "                token = sorted_indices[token]\n",
    "    \n",
    "            predicted_tokens.append(token.item())\n",
    "    \n",
    "            if token == self.text_tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            input_tokens = torch.cat([input_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            token_embeddings = self.embed_token(input_tokens)\n",
    "            token_masks = torch.ones_like(input_tokens)\n",
    "    \n",
    "            input_tokens = input_tokens[:, -self.context_len:]\n",
    "            token_embeddings = token_embeddings[:, -self.context_len:, :]\n",
    "            token_masks = token_masks[:, -self.context_len:]\n",
    "\n",
    "        return logits, predicted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07e09b-1be0-44cd-b36e-50f4fd07bef5",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ee00ab07-ecbc-4af7-97be-7706644f4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        accelerator,\n",
    "        scheduler,\n",
    "        tasks,\n",
    "        exp_name,\n",
    "        args\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.accelerator = accelerator\n",
    "        self.tasks = tasks\n",
    "        self.args = args\n",
    "        self.print_logs = True # args.print_logs\n",
    "        self.device = torch.device(args.device)\n",
    "\n",
    "        self.min_lr = self.args.learning_rate / self.args.min_factor\n",
    "        self.deterministic = self.args.eval_mode == 'deterministic'\n",
    "\n",
    "        self.exp_name = exp_name\n",
    "        self.exp_dir = os.path.join(self.args.save_dir, self.exp_name)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def train(self):\n",
    "        self.start_time = time.time()\n",
    "        iters = self.args.training_steps // self.args.log_eval_freq\n",
    "        for i in range(iters):\n",
    "            logs = self.train_iteration(self.args.log_eval_freq, i)\n",
    "            self.accelerator.log(logs)\n",
    "\n",
    "        ## Save model at end of training only if not saving checkpoints\n",
    "        if self.args.save_model and self.args.save_mode == 'last':\n",
    "            self.accelerator.wait_for_everyone()\n",
    "            if self.accelerator.is_main_process:\n",
    "                unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                save_model(unwrapped_model, self.exp_dir, f'checkpoint_{self.steps}', self.args)\n",
    "\n",
    "        self.accelerator.end_training()\n",
    "\n",
    "\n",
    "    def train_iteration(self, num_steps, iter):\n",
    "        logs = {}\n",
    "\n",
    "        train_start = time.time()\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        self.model.train()\n",
    "        for i in range(num_steps):\n",
    "            self.steps += 1\n",
    "            train_loss, step_logs = self.train_step()\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "        # add logs from last train_step as well\n",
    "        for log in step_logs:\n",
    "            logs[log] = step_logs[log]\n",
    "\n",
    "        logs['time/training'] = time.time() - train_start\n",
    "\n",
    "        eval_start = time.time()\n",
    "        self.model.eval()\n",
    "\n",
    "        # loop over eval for each env\n",
    "        with torch.no_grad():\n",
    "            for task in self.tasks:\n",
    "                eval_logs = {}\n",
    "                if isinstance(task, TextTask):\n",
    "                    eval_logs = task.evaluate(self.model, num_examples_to_test=self.args.eval_text_num_examples, deterministic=self.deterministic, log_examples_to_output=self.args.eval_text_log_examples)\n",
    "                    for k, v in eval_logs.items():\n",
    "                        logs[f'evaluation/text/{k}'] = v\n",
    "                    pass\n",
    "\n",
    "        logs['time/total'] = time.time() - self.start_time\n",
    "        logs['time/evaluation'] = time.time() - eval_start\n",
    "        logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "        logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            if self.print_logs:\n",
    "                print('=' * 80)\n",
    "                print(f'Iteration {iter}')\n",
    "                for k, v in logs.items():\n",
    "                    print(f'{k}: {v}')\n",
    "                print('=' * 80)\n",
    "\n",
    "        ## Save model\n",
    "        if self.args.save_model and self.args.save_mode == 'checkpoint':\n",
    "            self.accelerator.wait_for_everyone()\n",
    "            if self.accelerator.is_main_process:\n",
    "                unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                save_model(unwrapped_model, self.exp_dir, f'checkpoint_{self.steps}', self.args)\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def train_step(self):\n",
    "        logs = {}\n",
    "        logs['training/learning_rate'] = self.scheduler.get_lr()[0] # store LR at current step\n",
    "        # Build training batch\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Calculate batch size for each task, the following need to be revised to including more new tasks\n",
    "        text_batch_size = int(self.args.text_prop * self.args.batch_size)\n",
    "        remainder = self.args.batch_size - text_batch_size\n",
    "\n",
    "        if remainder > 0: \n",
    "            text_batch_size += remainder\n",
    "\n",
    "        assert self.args.batch_size == text_batch_size, \"Total batch size is not eqaual to the sum of each task's batch size\" \n",
    "\n",
    "        text_batch_dicts = []\n",
    "\n",
    "        # Sample text and control batches\n",
    "        if text_batch_size > 0:\n",
    "            text_batch_dicts = self.sample_text_batch(text_batch_size)\n",
    "\n",
    "        assert text_batch_dicts, \"Batch dicts is empty\"\n",
    "        print(f'text_batch_size:{text_batch_size}')\n",
    "\n",
    "\n",
    "        logs['time/sample_batch'] = time.time() - start_time\n",
    "        with self.accelerator.accumulate(self.model):\n",
    "            # Compute loss and update model\n",
    "            logits, loss = self.model.forward(inputs = text_batch_dicts, compute_loss=True)\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "            if not self.args.disable_grad_clip and self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.model.parameters(), self.args.grad_norm_clip)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach().cpu().item(), logs\n",
    "\n",
    "    def sample_text_batch(self, batch_size):\n",
    "        batch_dicts = []\n",
    "        text_tasks = [t for t in self.tasks if isinstance(t, TextTask)]\n",
    "        for i,task in enumerate (text_tasks):\n",
    "            batch_dicts.extend(task.sample_batch(batch_size))\n",
    "        return batch_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed366d-f13d-45c7-a0cf-238df84f33d7",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "140f3f38-8c26-42d2-a9e4-18d50cb14319",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs(\n",
    "    training_steps=1,\n",
    "    log_eval_freq=1,\n",
    "    warmup_steps=1,\n",
    "    batch_size=16,\n",
    "    eval_episodes=1,\n",
    "    text_prop=1,\n",
    "    eval_text_log_examples=True,\n",
    "    pretrained_lm=None,\n",
    "    text_datasets=['wikitext-2-v1'],\n",
    "    text_datasets_paths=['wikitext'],\n",
    "    learning_rate=0.00001,\n",
    "    use_wandb=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e93068-5578-41c1-8382-226e27886913",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a8c19b6c-0635-4523-baf9-50ecb7f36701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gato.training.trainer import Trainer\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0ed9cfc0-8802-45d3-9ffd-75871f69b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "if args.use_wandb:\n",
    "    log_with = 'wandb'\n",
    "else:\n",
    "    log_with = None\n",
    "\n",
    "dl_config = DataLoaderConfiguration(split_batches=True)\n",
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    dataloader_config=dl_config, \n",
    "    mixed_precision=args.mixed_precision,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    log_with=log_with\n",
    ")\n",
    "args.device = accelerator.device.type\n",
    "\n",
    "exp_date = datetime.now().strftime('%y-%m-%d_%H-%M-%S')\n",
    "exp_name = f'neko-gato_{exp_date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d8067681-5077-4a59-8cd8-2f03b1ede992",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c611cb73-d3cd-4ffc-84e3-e68876d9560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.append(TextTask(args.text_datasets, args.text_datasets_paths, args.sequence_length, tokenizer_model=args.tokenizer_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b5a4e221-75e6-4277-8435-2fbb1c0a84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GatoPolicy(\n",
    "        device=args.device,\n",
    "        embed_dim=args.embed_dim,\n",
    "        layers=args.layers,\n",
    "        heads=args.heads,\n",
    "        dropout=args.dropout,\n",
    "        mu=args.mu,\n",
    "        M=args.M,\n",
    "        patch_size=args.patch_size,\n",
    "        resid_mid_channels=args.resid_mid_channels,\n",
    "        continuous_tokens=args.continuous_tokens,\n",
    "        discrete_tokens=args.discrete_tokens,\n",
    "        context_len=args.sequence_length,\n",
    "        use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "        use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "        activation_fn=args.activation_fn,\n",
    "        pretrained_lm=args.pretrained_lm,\n",
    "        flash=args.flash,\n",
    "        tokenizer_model_name=args.tokenizer_model_name,\n",
    "        pad_seq=args.pad_seq,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "627d19c3-c1ff-4aa4-b433-2f90f1996e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "57d87cfd-63c3-46cd-b19a-ee6ee32cb26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 133.9008M\n"
     ]
    }
   ],
   "source": [
    "# print trainable parameters\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Trainable Parameters:', '{}M'.format(params / 1e6))\n",
    "args.trainable_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "29278345-3f2b-4367-a48c-bb5804f08262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device = args.device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "# Setup scheduler\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(optimizer, args.warmup_steps, args.training_steps, base_lr=args.learning_rate, init_lr=args.init_lr, min_lr=args.learning_rate / args.min_factor, cosine_decay=not args.disable_cosine_decay)\n",
    "\n",
    "# setup up Accelerate, without dataloader:\n",
    "#model, optimizer, scheduler = accelerator.prepare(model, optimizer, scheduler)\n",
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "\n",
    "if args.use_wandb:\n",
    "    accelerator.init_trackers(args.wandb_project, init_kwargs={'wandb': {'name': exp_name, 'config': args}})\n",
    "else:\n",
    "    accelerator.init_trackers('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ffb52e9b-a8ad-4ae3-bd03-fb9f9facd262",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(args\u001b[38;5;241m.\u001b[39msave_dir)\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      6\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     args\u001b[38;5;241m=\u001b[39margs\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/training/trainer.py:47\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtraining_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mlog_eval_freq\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m---> 47\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mlog(logs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m## Save model at end of training only if not saving checkpoints\u001b[39;00m\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/training/trainer.py:70\u001b[0m, in \u001b[0;36mTrainer.train_iteration\u001b[0;34m(self, num_steps, iter)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     train_loss, step_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# add logs from last train_step as well\u001b[39;00m\n",
      "File \u001b[0;32m~/bhavul/NEKO/gato/training/trainer.py:178\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/sample_batch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Compute loss and update model\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcombined_batch_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_grad_clip \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n",
      "Cell \u001b[0;32mIn[227], line 91\u001b[0m, in \u001b[0;36mGatoPolicy.forward\u001b[0;34m(self, inputs, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Optional[\u001b[38;5;28mlist\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# tokenize inputs\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m         token_embeddings, tokens, token_target_masks, token_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_input_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         token_embeddings \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[227], line 147\u001b[0m, in \u001b[0;36mGatoPolicy.tokenize_input_dicts\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_input_dicts\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if not inputs:\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m#     return None, None, None, None\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     batch_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m--> 147\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Allocate tensors\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_len, max_tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_token\u001b[38;5;241m.\u001b[39membedding_dim), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "# Create save dir if does not exist\n",
    "if args.save_model and not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    accelerator = accelerator,\n",
    "    tasks = tasks,\n",
    "    exp_name = exp_name,\n",
    "    args=args\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b24c4f-c807-46df-b8e8-06cda2f4eb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8263297-5ef8-48a3-979c-0dc9d947fb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
